{
  "2026-02-12": [
    {
      "id": "2602.10604",
      "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters",
      "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.",
      "authors": [
        {
          "_id": "698d417065c0d15a6d162026",
          "name": "Ailin Huang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162027",
          "name": "Ang Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162028",
          "name": "Aobo Kong",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162029",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16202a",
          "name": "Binxing Jiao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16202b",
          "name": "Bo Dong",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16202c",
          "name": "Bojun Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16202d",
          "name": "Boyu Chen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16202e",
          "name": "Brian Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16202f",
          "name": "Buyun Ma",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162030",
          "name": "Chang Su",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162031",
          "name": "Changxin Miao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162032",
          "name": "Changyi Wan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162033",
          "name": "Chao Lou",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162034",
          "name": "Chen Hu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162035",
          "name": "Chen Xu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162036",
          "name": "Chenfeng Yu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162037",
          "name": "Chengting Feng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162038",
          "name": "Chengyuan Yao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162039",
          "name": "Chunrui Han",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16203a",
          "name": "Dan Ma",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16203b",
          "name": "Dapeng Shi",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16203c",
          "name": "Daxin Jiang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16203d",
          "name": "Dehua Ma",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16203e",
          "name": "Deshan Sun",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16203f",
          "name": "Di Qi",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162040",
          "name": "Enle Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162041",
          "name": "Fajie Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162042",
          "name": "Fanqi Wan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162043",
          "name": "Guanzhe Huang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162044",
          "name": "Gulin Yan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162045",
          "name": "Guoliang Cao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162046",
          "name": "Guopeng Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162047",
          "name": "Han Cheng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162048",
          "name": "Hangyu Guo",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162049",
          "name": "Hanshan Zhang",
          "user": {
            "_id": "64b7874b9f5987572ca28461",
            "type": "user",
            "user": "brain-zhang",
            "isPro": false,
            "fullname": "hanshanzhang",
            "avatarUrl": "/avatars/d24ee0a6329ff93936aa7829481e2046.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:56:52.602Z"
        },
        {
          "_id": "698d417065c0d15a6d16204a",
          "name": "Hao Nie",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16204b",
          "name": "Haonan Jia",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16204c",
          "name": "Haoran Lv",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16204d",
          "name": "Hebin Zhou",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16204e",
          "name": "Hekun Lv",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16204f",
          "name": "Heng Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162050",
          "name": "Heung-Yeung Shum",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162051",
          "name": "Hongbo Huang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162052",
          "name": "Hongbo Peng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162053",
          "name": "Hongyu Zhou",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162054",
          "name": "Hongyuan Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162055",
          "name": "Houyong Chen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162056",
          "name": "Huangxi Zhu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162057",
          "name": "Huimin Wu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162058",
          "name": "Huiyong Guo",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162059",
          "name": "Jia Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16205a",
          "name": "Jian Zhou",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16205b",
          "name": "Jianjian Sun",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16205c",
          "name": "Jiaoren Wu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16205d",
          "name": "Jiaran Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16205e",
          "name": "Jiashu Lv",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16205f",
          "name": "Jiashuo Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162060",
          "name": "Jiayi Fu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162061",
          "name": "Jiayu Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162062",
          "name": "Jie Cheng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162063",
          "name": "Jie Luo",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162064",
          "name": "Jie Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162065",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162066",
          "name": "Jieyi Hou",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162067",
          "name": "Jing Bai",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162068",
          "name": "Jingcheng Hu",
          "user": {
            "_id": "625026b7d2d191ac43320c5e",
            "type": "user",
            "user": "reign12",
            "isPro": false,
            "fullname": "Jingcheng Hu",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:28:37.335Z"
        },
        {
          "_id": "698d417065c0d15a6d162069",
          "name": "Jingjing Xie",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16206a",
          "name": "Jingwei Wu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16206b",
          "name": "Jingyang Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16206c",
          "name": "Jishi Zhou",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16206d",
          "name": "Junfeng Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16206e",
          "name": "Junzhe Lin",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16206f",
          "name": "Ka Man Lo",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162070",
          "name": "Kai Liang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162071",
          "name": "Kaibo Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162072",
          "name": "Kaijun Tan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162073",
          "name": "Kaiwen Yan",
          "user": {
            "_id": "66668c591964b6188ee310c2",
            "type": "user",
            "user": "linrany",
            "isPro": false,
            "fullname": "Kaiwen Yan",
            "avatarUrl": "/avatars/8a8265073dbacbb2c7139b1c8da3e055.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:56:58.524Z"
        },
        {
          "_id": "698d417065c0d15a6d162074",
          "name": "Kaixiang Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162075",
          "name": "Kang An",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162076",
          "name": "Kangheng Lin",
          "user": {
            "_id": "658a810665df457a55ffcd04",
            "type": "user",
            "user": "Kangheng",
            "isPro": false,
            "fullname": "Linkangheng",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:56:56.339Z"
        },
        {
          "_id": "698d417065c0d15a6d162077",
          "name": "Lei Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162078",
          "name": "Liang Lv",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162079",
          "name": "Liang Zhao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16207a",
          "name": "Liangyu Chen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16207b",
          "name": "Lieyu Shi",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16207c",
          "name": "Liguo Tan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16207d",
          "name": "Lin Lin",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16207e",
          "name": "Lina Chen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16207f",
          "name": "Luck Ma",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162080",
          "name": "Mengqiang Ren",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162081",
          "name": "Michael Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162082",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162083",
          "name": "Mingliang Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162084",
          "name": "Mingming Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162085",
          "name": "Mingrui Chen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162086",
          "name": "Mitt Huang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162087",
          "name": "Na Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162088",
          "name": "Peng Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162089",
          "name": "Qi Han",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16208a",
          "name": "Qian Zhao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16208b",
          "name": "Qinglin He",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16208c",
          "name": "Qinxin Du",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16208d",
          "name": "Qiuping Wu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16208e",
          "name": "Quan Sun",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16208f",
          "name": "Rongqiu Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162090",
          "name": "Ruihang Miao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162091",
          "name": "Ruixin Han",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162092",
          "name": "Ruosi Wan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162093",
          "name": "Ruyan Guo",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162094",
          "name": "Shan Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162095",
          "name": "Shaoliang Pang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162096",
          "name": "Shaowen Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162097",
          "name": "Shengjie Fan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162098",
          "name": "Shijie Shang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d162099",
          "name": "Shiliang Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16209a",
          "name": "Shiwei Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16209b",
          "name": "Shuangshuang Tian",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16209c",
          "name": "Siqi Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16209d",
          "name": "Siye Wu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16209e",
          "name": "Siyu Chen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d16209f",
          "name": "Song Yuan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a0",
          "name": "Tiancheng Cao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a1",
          "name": "Tianchi Yue",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a2",
          "name": "Tianhao Cheng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a3",
          "name": "Tianning Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a4",
          "name": "Tingdan Luo",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a5",
          "name": "Wang You",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a6",
          "name": "Wei Ji",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a7",
          "name": "Wei Yuan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a8",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620a9",
          "name": "Weibo Wu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620aa",
          "name": "Weihao Xie",
          "user": {
            "_id": "6657620ea496f7fcb67c3871",
            "type": "user",
            "user": "chalengr",
            "isPro": false,
            "fullname": "xieweihao",
            "avatarUrl": "/avatars/54fef1c835e6f6b478652d438a140d45.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:56:48.216Z"
        },
        {
          "_id": "698d417065c0d15a6d1620ab",
          "name": "Wen Sun",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ac",
          "name": "Wenjin Deng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ad",
          "name": "Wenzhen Zheng",
          "user": {
            "_id": "650c04795510464e85b47470",
            "type": "user",
            "user": "zhengwenzhen",
            "isPro": false,
            "fullname": "wen",
            "avatarUrl": "/avatars/98c194e77826b928c49659849f466dad.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:56:45.930Z"
        },
        {
          "_id": "698d417065c0d15a6d1620ae",
          "name": "Wuxun Xie",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620af",
          "name": "Xiangfeng Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b0",
          "name": "Xiangwen Kong",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b1",
          "name": "Xiangyu Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b2",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b3",
          "name": "Xiaobo Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b4",
          "name": "Xiaojia Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b5",
          "name": "Xiaolan Yuan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b6",
          "name": "Xiaoran Jiao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b7",
          "name": "Xiaoxiao Ren",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b8",
          "name": "Xiaoyun Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620b9",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ba",
          "name": "Xin Liu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620bb",
          "name": "Xin Wu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620bc",
          "name": "Xing Chen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620bd",
          "name": "Xingping Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620be",
          "name": "Xinran Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620bf",
          "name": "Xu Zhao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620c0",
          "name": "Xuan He",
          "user": {
            "_id": "64ec5b64bfb2aa06a46ff2d6",
            "type": "user",
            "user": "tpa115k31",
            "isPro": false,
            "fullname": "xuan he",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Lgl55OtDWa0tzRI2ShpUe.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:56:36.240Z"
        },
        {
          "_id": "698d417065c0d15a6d1620c1",
          "name": "Xuanti Feng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620c2",
          "name": "Xuedan Cai",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620c3",
          "name": "Xuqiang Zhou",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620c4",
          "name": "Yanbo Yu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620c5",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620c6",
          "name": "Yang Xu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620c7",
          "name": "Yanlin Lai",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620c8",
          "name": "Yanming Xu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620c9",
          "name": "Yaoyu Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ca",
          "name": "Yeqing Shen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620cb",
          "name": "Yibo Zhu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620cc",
          "name": "Yichen Lv",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620cd",
          "name": "Yicheng Cao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ce",
          "name": "Yifeng Gong",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620cf",
          "name": "Yijing Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d0",
          "name": "Yikun Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d1",
          "name": "Yin Zhao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d2",
          "name": "Yingxiu Zhao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d3",
          "name": "Yinmin Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d4",
          "name": "Yitong Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d5",
          "name": "Yixuan Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d6",
          "name": "Yiyang Chen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d7",
          "name": "Yongchi Zhao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d8",
          "name": "Yongshen Long",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620d9",
          "name": "Yongyao Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620da",
          "name": "Yousong Guan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620db",
          "name": "Yu Zhou",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620dc",
          "name": "Yuang Peng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620dd",
          "name": "Yuanhao Ding",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620de",
          "name": "Yuantao Fan",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620df",
          "name": "Yuanzhen Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e0",
          "name": "Yuchu Luo",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e1",
          "name": "Yudi Zhao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e2",
          "name": "Yue Peng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e3",
          "name": "Yueqiang Lin",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e4",
          "name": "Yufan Lu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e5",
          "name": "Yuling Zhao",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e6",
          "name": "Yunzhou Ju",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e7",
          "name": "Yurong Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e8",
          "name": "Yusheng Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620e9",
          "name": "Yuxiang Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ea",
          "name": "Yuyang Chen",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620eb",
          "name": "Yuzhu Cai",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ec",
          "name": "Zejia Weng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ed",
          "name": "Zetao Hong",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ee",
          "name": "Zexi Li",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620ef",
          "name": "Zhe Xie",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620f0",
          "name": "Zheng Ge",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620f1",
          "name": "Zheng Gong",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620f2",
          "name": "Zheng Zeng",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620f3",
          "name": "Zhenyi Lu",
          "user": {
            "_id": "63607ace9ddc44e710e13f0f",
            "type": "user",
            "user": "lu-vae",
            "isPro": false,
            "fullname": "zy",
            "avatarUrl": "/avatars/b5f331549562aea4a5c8b681fd9da1ff.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:56:50.532Z"
        },
        {
          "_id": "698d417065c0d15a6d1620f4",
          "name": "Zhewei Huang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620f5",
          "name": "Zhichao Chang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620f6",
          "name": "Zhiguo Huang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620f7",
          "name": "Zhiheng Hu",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620f8",
          "name": "Zidong Yang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620f9",
          "name": "Zili Wang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620fa",
          "name": "Ziqi Ren",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620fb",
          "name": "Zixin Zhang",
          "hidden": false
        },
        {
          "_id": "698d417065c0d15a6d1620fc",
          "name": "Zixuan Wang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-11T07:53:51",
      "upvotes": 147,
      "date": "2026-02-12"
    },
    {
      "id": "2602.11124",
      "title": "PhyCritic: Multimodal Critic Models for Physical AI",
      "summary": "With the rapid development of large multimodal models, reliable judge and critic models have become essential for open-ended evaluation and preference alignment, providing pairwise preferences, numerical scores, and explanatory justifications for assessing model-generated responses. However, existing critics are primarily trained in general visual domains such as captioning or image question answering, leaving physical AI tasks involving perception, causal reasoning, and planning largely underexplored. We introduce PhyCritic, a multimodal critic model optimized for physical AI through a two-stage RLVR pipeline: a physical skill warmup stage that enhances physically oriented perception and reasoning, followed by self-referential critic finetuning, where the critic generates its own prediction as an internal reference before judging candidate responses, improving judgment stability and physical correctness. Across both physical and general-purpose multimodal judge benchmarks, PhyCritic achieves strong performance gains over open-source baselines and, when applied as a policy model, further improves perception and reasoning in physically grounded tasks.",
      "authors": [
        {
          "_id": "698d486865c0d15a6d162162",
          "name": "Tianyi Xiong",
          "user": {
            "_id": "6570977f87a92b76922c9950",
            "type": "user",
            "user": "txiong23",
            "isPro": false,
            "fullname": "Tianyi Xiong",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570977f87a92b76922c9950/AQGto1w6ugBvH2yCV46YU.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:28:23.141Z"
        },
        {
          "_id": "698d486865c0d15a6d162163",
          "name": "Shihao Wang",
          "hidden": false
        },
        {
          "_id": "698d486865c0d15a6d162164",
          "name": "Guilin Liu",
          "hidden": false
        },
        {
          "_id": "698d486865c0d15a6d162165",
          "name": "Yi Dong",
          "hidden": false
        },
        {
          "_id": "698d486865c0d15a6d162166",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "698d486865c0d15a6d162167",
          "name": "Heng Huang",
          "hidden": false
        },
        {
          "_id": "698d486865c0d15a6d162168",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "698d486865c0d15a6d162169",
          "name": "Zhiding Yu",
          "user": {
            "_id": "66c8037c737ba92ae3fe0322",
            "type": "user",
            "user": "Zhiding",
            "isPro": true,
            "fullname": "Zhiding Yu",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c8037c737ba92ae3fe0322/WR_Yh5DWOVVh7IFlF24NM.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:28:25.160Z"
        }
      ],
      "published_at": "2026-02-11T18:35:39",
      "upvotes": 41,
      "date": "2026-02-12"
    },
    {
      "id": "2602.11144",
      "title": "GENIUS: Generative Fluid Intelligence Evaluation Suite",
      "summary": "Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess Crystallized Intelligence, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks Generative Fluid Intelligence (GFI): the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce GENIUS (GEN Fluid Intelligence EvalUation Suite). We formalize GFI as a synthesis of three primitives. These include Inducing Implicit Patterns (e.g., inferring personalized visual preferences), Executing Ad-hoc Constraints (e.g., visualizing abstract metaphors), and Adapting to Contextual Knowledge (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, GENIUS establishes a rigorous standard for GFI, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: https://github.com/arctanxarc/GENIUS{https://github.com/arctanxarc/GENIUS}.",
      "authors": [
        {
          "_id": "698d4ad765c0d15a6d162188",
          "name": "Ruichuan An",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d162189",
          "name": "Sihan Yang",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d16218a",
          "name": "Ziyu Guo",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d16218b",
          "name": "Wei Dai",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d16218c",
          "name": "Zijun Shen",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d16218d",
          "name": "Haodong Li",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d16218e",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d16218f",
          "name": "Xinyu Wei",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d162190",
          "name": "Guopeng Li",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d162191",
          "name": "Wenshan Wu",
          "hidden": false
        },
        {
          "_id": "698d4ad765c0d15a6d162192",
          "name": "Wentao Zhang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-11T18:55:54",
      "upvotes": 40,
      "date": "2026-02-12"
    },
    {
      "id": "2602.04935",
      "title": "ASA: Training-Free Representation Engineering for Tool-Calling Agents",
      "summary": "Adapting LLM agents to domain-specific tool calling remains notably brittle under evolving interfaces. Prompt and schema engineering is easy to deploy but often fragile under distribution shift and strict parsers, while continual parameter-efficient fine-tuning improves reliability at the cost of training, maintenance, and potential forgetting. We identify a critical Lazy Agent failure mode where tool necessity is nearly perfectly decodable from mid-layer activations, yet the model remains conservative in entering tool mode, revealing a representation-behavior gap. We propose Activation Steering Adapter (ASA), a training-free, inference-time controller that performs a single-shot mid-layer intervention and targets tool domains via a router-conditioned mixture of steering vectors with a probe-guided signed gate to amplify true intent while suppressing spurious triggers. On MTU-Bench with Qwen2.5-1.5B, ASA improves strict tool-use F1 from 0.18 to 0.50 while reducing the false positive rate from 0.15 to 0.05, using only about 20KB of portable assets and no weight updates.",
      "authors": [
        {
          "_id": "698c7b53eb12ea74539168e7",
          "name": "Youjin Wang",
          "user": {
            "_id": "68e8b2658b33409d96ca711c",
            "type": "user",
            "user": "wangyoujin",
            "isPro": false,
            "fullname": "wyj",
            "avatarUrl": "/avatars/abea0be180fc111989185e3d7aeaeb88.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:57:24.472Z"
        },
        {
          "_id": "698c7b53eb12ea74539168e8",
          "name": "Run Zhou",
          "hidden": false
        },
        {
          "_id": "698c7b53eb12ea74539168e9",
          "name": "Rong Fu",
          "hidden": false
        },
        {
          "_id": "698c7b53eb12ea74539168ea",
          "name": "Shuaishuai Cao",
          "hidden": false
        },
        {
          "_id": "698c7b53eb12ea74539168eb",
          "name": "Hongwei Zeng",
          "hidden": false
        },
        {
          "_id": "698c7b53eb12ea74539168ec",
          "name": "Jiaxuan Lu",
          "hidden": false
        },
        {
          "_id": "698c7b53eb12ea74539168ed",
          "name": "Sicheng Fan",
          "hidden": false
        },
        {
          "_id": "698c7b53eb12ea74539168ee",
          "name": "Jiaqiao Zhao",
          "hidden": false
        },
        {
          "_id": "698c7b53eb12ea74539168ef",
          "name": "Liangming Pan",
          "hidden": false
        }
      ],
      "published_at": "2026-02-04T14:20:02",
      "upvotes": 38,
      "date": "2026-02-12"
    },
    {
      "id": "2602.10560",
      "title": "When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning",
      "summary": "While reasoning over long context is crucial for various real-world applications, it remains challenging for large language models (LLMs) as they suffer from performance degradation as the context length grows. Recent work MemAgent has tried to tackle this by processing context chunk-by-chunk in an RNN-like loop and updating a textual memory for final answering. However, this naive recurrent memory update faces two crucial drawbacks: (i) memory can quickly explode because it can update indiscriminately, even on evidence-free chunks; and (ii) the loop lacks an exit mechanism, leading to unnecessary computation after even sufficient evidence is collected. To address these issues, we propose GRU-Mem, which incorporates two text-controlled gates for more stable and efficient long-context reasoning. Specifically, in GRU-Mem, the memory only updates when the update gate is open and the recurrent loop will exit immediately once the exit gate is open. To endow the model with such capabilities, we introduce two reward signals r^{update} and r^{exit} within end-to-end RL, rewarding the correct updating and exiting behaviors respectively. Experiments on various long-context reasoning tasks demonstrate the effectiveness and efficiency of GRU-Mem, which generally outperforms the vanilla MemAgent with up to 400\\% times inference speed acceleration.",
      "authors": [
        {
          "_id": "698d420e65c0d15a6d162108",
          "name": "Leheng Sheng",
          "hidden": false
        },
        {
          "_id": "698d420e65c0d15a6d162109",
          "name": "Yongtao Zhang",
          "hidden": false
        },
        {
          "_id": "698d420e65c0d15a6d16210a",
          "name": "Wenchang Ma",
          "hidden": false
        },
        {
          "_id": "698d420e65c0d15a6d16210b",
          "name": "Yaorui Shi",
          "user": {
            "_id": "63edd2d1f765928ceeb49057",
            "type": "user",
            "user": "yrshi",
            "isPro": false,
            "fullname": "Yaorui SHI",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676530369930-noauth.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:28:35.189Z"
        },
        {
          "_id": "698d420e65c0d15a6d16210c",
          "name": "Ting Huang",
          "hidden": false
        },
        {
          "_id": "698d420e65c0d15a6d16210d",
          "name": "Xiang Wang",
          "hidden": false
        },
        {
          "_id": "698d420e65c0d15a6d16210e",
          "name": "An Zhang",
          "hidden": false
        },
        {
          "_id": "698d420e65c0d15a6d16210f",
          "name": "Ke Shen",
          "hidden": false
        },
        {
          "_id": "698d420e65c0d15a6d162110",
          "name": "Tat-Seng Chua",
          "hidden": false
        }
      ],
      "published_at": "2026-02-11T06:14:53",
      "upvotes": 22,
      "date": "2026-02-12"
    },
    {
      "id": "2602.10177",
      "title": "Towards Autonomous Mathematics Research",
      "summary": "Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. In this work, we introduce Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research. We demonstrate the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom's Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, we suggest codifying standard levels quantifying autonomy and novelty of AI-assisted results. We conclude with reflections on human-AI collaboration in mathematics.",
      "authors": [
        {
          "_id": "698d426265c0d15a6d162113",
          "name": "Tony Feng",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162114",
          "name": "Trieu H. Trinh",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162115",
          "name": "Garrett Bingham",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162116",
          "name": "Dawsen Hwang",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162117",
          "name": "Yuri Chervonyi",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162118",
          "name": "Junehyuk Jung",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162119",
          "name": "Joonkyung Lee",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16211a",
          "name": "Carlo Pagano",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16211b",
          "name": "Sang-hyun Kim",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16211c",
          "name": "Federico Pasqualotto",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16211d",
          "name": "Sergei Gukov",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16211e",
          "name": "Jonathan N. Lee",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16211f",
          "name": "Junsu Kim",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162120",
          "name": "Kaiying Hou",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162121",
          "name": "Golnaz Ghiasi",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162122",
          "name": "Yi Tay",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162123",
          "name": "YaGuang Li",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162124",
          "name": "Chenkai Kuang",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162125",
          "name": "Yuan Liu",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162126",
          "name": "Hanzhao",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162127",
          "name": "Lin",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162128",
          "name": "Evan Zheran Liu",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d162129",
          "name": "Nigamaa Nayakanti",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16212a",
          "name": "Xiaomeng Yang",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16212b",
          "name": "Heng-tze Cheng",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16212c",
          "name": "Demis Hassabis",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16212d",
          "name": "Koray Kavukcuoglu",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16212e",
          "name": "Quoc V. Le",
          "hidden": false
        },
        {
          "_id": "698d426265c0d15a6d16212f",
          "name": "Thang Luong",
          "hidden": false
        }
      ],
      "published_at": "2026-02-10T18:50:15",
      "upvotes": 22,
      "date": "2026-02-12"
    },
    {
      "id": "2602.08253",
      "title": "G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design",
      "summary": "While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.",
      "authors": [
        {
          "_id": "698bf9256052d3bed9630ad3",
          "name": "Baoyun Zhao",
          "user": {
            "_id": "698ae59d70be6790cd1e6a53",
            "type": "user",
            "user": "ZBoyn",
            "isPro": false,
            "fullname": "Baoyun Zhao",
            "avatarUrl": "/avatars/66125d952e7880baa4fe1635aafbc160.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:14:17.040Z"
        },
        {
          "_id": "698bf9256052d3bed9630ad4",
          "name": "He Wang",
          "user": {
            "_id": "617a39153490eedabaa5391f",
            "type": "user",
            "user": "iphysresearch",
            "isPro": false,
            "fullname": "He Wang",
            "avatarUrl": "/avatars/29ce0101b99bc96f81032ac6228ce878.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:57:46.758Z"
        },
        {
          "_id": "698bf9256052d3bed9630ad5",
          "name": "Liang Zeng",
          "hidden": false
        }
      ],
      "published_at": "2026-02-09T04:13:35",
      "upvotes": 21,
      "date": "2026-02-12"
    },
    {
      "id": "2602.10622",
      "title": "How Do Decoder-Only LLMs Perceive Users? Rethinking Attention Masking for User Representation Learning",
      "summary": "Decoder-only large language models are increasingly used as behavioral encoders for user representation learning, yet the impact of attention masking on the quality of user embeddings remains underexplored. In this work, we conduct a systematic study of causal, hybrid, and bidirectional attention masks within a unified contrastive learning framework trained on large-scale real-world Alipay data that integrates long-horizon heterogeneous user behaviors. To improve training dynamics when transitioning from causal to bidirectional attention, we propose Gradient-Guided Soft Masking, a gradient-based pre-warmup applied before a linear scheduler that gradually opens future attention during optimization. Evaluated on 9 industrial user cognition benchmarks covering prediction, preference, and marketing sensitivity tasks, our approach consistently yields more stable training and higher-quality bidirectional representations compared with causal, hybrid, and scheduler-only baselines, while remaining compatible with decoder pretraining. Overall, our findings highlight the importance of masking design and training transition in adapting decoder-only LLMs for effective user representation learning. Our code is available at https://github.com/JhCircle/Deepfind-GGSM.",
      "authors": [
        {
          "_id": "698d5d9565c0d15a6d162214",
          "name": "Jiahao Yuan",
          "user": {
            "_id": "67acb56faee29e2e5c1c41ec",
            "type": "user",
            "user": "Jhcircle",
            "isPro": false,
            "fullname": "Jiahao Yuan",
            "avatarUrl": "/avatars/b47094266fe70d10786da845ae2ade2c.svg"
          },
          "hidden": false,
          "status": "admin_assigned",
          "statusLastChangedAt": "2026-02-12T16:47:44.410Z"
        },
        {
          "_id": "698d5d9565c0d15a6d162215",
          "name": "Yike Xu",
          "hidden": false
        },
        {
          "_id": "698d5d9565c0d15a6d162216",
          "name": "Jinyong Wen",
          "hidden": false
        },
        {
          "_id": "698d5d9565c0d15a6d162217",
          "name": "Baokun Wang",
          "hidden": false
        },
        {
          "_id": "698d5d9565c0d15a6d162218",
          "name": "Yang Chen",
          "hidden": false
        },
        {
          "_id": "698d5d9565c0d15a6d162219",
          "name": "Xiaotong Lin",
          "hidden": false
        },
        {
          "_id": "698d5d9565c0d15a6d16221a",
          "name": "Wuliang Huang",
          "hidden": false
        },
        {
          "_id": "698d5d9565c0d15a6d16221b",
          "name": "Ziyi Gao",
          "hidden": false
        },
        {
          "_id": "698d5d9565c0d15a6d16221c",
          "name": "Xing Fu",
          "hidden": false
        },
        {
          "_id": "698d5d9565c0d15a6d16221d",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "698d5d9565c0d15a6d16221e",
          "name": "Weiqiang Wang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-11T08:12:43",
      "upvotes": 20,
      "date": "2026-02-12"
    },
    {
      "id": "2602.08711",
      "title": "TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions",
      "summary": "This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create \"script-like\" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.",
      "authors": [
        {
          "_id": "698ab9841b2dc6b37d61b092",
          "name": "Linli Yao",
          "hidden": false
        },
        {
          "_id": "698ab9841b2dc6b37d61b093",
          "name": "Yuancheng Wei",
          "hidden": false
        },
        {
          "_id": "698ab9841b2dc6b37d61b094",
          "name": "Yaojie Zhang",
          "hidden": false
        },
        {
          "_id": "698ab9841b2dc6b37d61b095",
          "name": "Lei Li",
          "hidden": false
        },
        {
          "_id": "698ab9841b2dc6b37d61b096",
          "name": "Xinlong Chen",
          "hidden": false
        },
        {
          "_id": "698ab9841b2dc6b37d61b097",
          "name": "Feifan Song",
          "hidden": false
        },
        {
          "_id": "698ab9841b2dc6b37d61b098",
          "name": "Ziyue Wang",
          "hidden": false
        },
        {
          "_id": "698ab9841b2dc6b37d61b099",
          "name": "Kun Ouyang",
          "hidden": false
        },
        {
          "_id": "698ab9841b2dc6b37d61b09a",
          "name": "Yuanxin Liu",
          "hidden": false
        },
        {
          "_id": "698ab9841b2dc6b37d61b09b",
          "name": "Lingpeng Kong",
          "hidden": false
        },
        {
          "_id": "698ab9841b2dc6b37d61b09c",
          "name": "Qi Liu",
          "hidden": false
        },
        {
          "_id": "698ab9841b2dc6b37d61b09d",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "698ab9841b2dc6b37d61b09e",
          "name": "Kun Gai",
          "hidden": false
        },
        {
          "_id": "698ab9841b2dc6b37d61b09f",
          "name": "Yuanxing Zhang",
          "hidden": false
        },
        {
          "_id": "698ab9841b2dc6b37d61b0a0",
          "name": "Xu Sun",
          "hidden": false
        }
      ],
      "published_at": "2026-02-09T14:21:58",
      "upvotes": 20,
      "date": "2026-02-12"
    },
    {
      "id": "2602.10975",
      "title": "FeatureBench: Benchmarking Agentic Coding for Complex Feature Development",
      "summary": "Agents powered by large language models (LLMs) are increasingly adopted in the software industry, contributing code as collaborators or even autonomous developers. As their presence grows, it becomes important to assess the current boundaries of their coding abilities. Existing agentic coding benchmarks, however, cover a limited task scope, e.g., bug fixing within a single pull request (PR), and often rely on non-executable evaluations or lack an automated approach for continually updating the evaluation coverage. To address such issues, we propose FeatureBench, a benchmark designed to evaluate agentic coding performance in end-to-end, feature-oriented software development. FeatureBench incorporates an execution-based evaluation protocol and a scalable test-driven method that automatically derives tasks from code repositories with minimal human effort. By tracing from unit tests along a dependency graph, our approach can identify feature-level coding tasks spanning multiple commits and PRs scattered across the development timeline, while ensuring the proper functioning of other features after the separation. Using this framework, we curated 200 challenging evaluation tasks and 3825 executable environments from 24 open-source repositories in the first version of our benchmark. Empirical evaluation reveals that the state-of-the-art agentic model, such as Claude 4.5 Opus, which achieves a 74.4% resolved rate on SWE-bench, succeeds on only 11.0% of tasks, opening new opportunities for advancing agentic coding. Moreover, benefiting from our automated task collection toolkit, FeatureBench can be easily scaled and updated over time to mitigate data leakage. The inherent verifiability of constructed environments also makes our method potentially valuable for agent training.",
      "authors": [
        {
          "_id": "698d4c7965c0d15a6d1621a7",
          "name": "Qixing Zhou",
          "hidden": false
        },
        {
          "_id": "698d4c7965c0d15a6d1621a8",
          "name": "Jiacheng Zhang",
          "user": {
            "_id": "6868f58a4757672a6da7c417",
            "type": "user",
            "user": "jiachengzhg",
            "isPro": false,
            "fullname": "JiaCheng Zhang",
            "avatarUrl": "/avatars/73154b7e0f1af68054b97f10a6c2e670.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:28:04.846Z"
        },
        {
          "_id": "698d4c7965c0d15a6d1621a9",
          "name": "Haiyang Wang",
          "user": {
            "_id": "65f43c3cc9940817caaf4434",
            "type": "user",
            "user": "Haiyang-W",
            "isPro": false,
            "fullname": "Haiyang Wang",
            "avatarUrl": "/avatars/ecec2856ba7a7d3421a2071a0a88800b.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:28:02.525Z"
        },
        {
          "_id": "698d4c7965c0d15a6d1621aa",
          "name": "Rui Hao",
          "hidden": false
        },
        {
          "_id": "698d4c7965c0d15a6d1621ab",
          "name": "Jiahe Wang",
          "hidden": false
        },
        {
          "_id": "698d4c7965c0d15a6d1621ac",
          "name": "Minghao Han",
          "hidden": false
        },
        {
          "_id": "698d4c7965c0d15a6d1621ad",
          "name": "Yuxue Yang",
          "hidden": false
        },
        {
          "_id": "698d4c7965c0d15a6d1621ae",
          "name": "Shuzhe Wu",
          "hidden": false
        },
        {
          "_id": "698d4c7965c0d15a6d1621af",
          "name": "Feiyang Pan",
          "hidden": false
        },
        {
          "_id": "698d4c7965c0d15a6d1621b0",
          "name": "Lue Fan",
          "hidden": false
        },
        {
          "_id": "698d4c7965c0d15a6d1621b1",
          "name": "Dandan Tu",
          "hidden": false
        },
        {
          "_id": "698d4c7965c0d15a6d1621b2",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-11T16:06:32",
      "upvotes": 15,
      "date": "2026-02-12"
    },
    {
      "id": "2602.11008",
      "title": "ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression",
      "summary": "We present ROCKET, a training-free model compression method that achieves state-of-the-art performance in comparison with factorization, structured-sparsification and dynamic compression baselines. Operating under a global compression budget, ROCKET comprises two key innovations: First, it formulates layer-wise compression allocation as a multi-choice knapsack problem, selecting the optimal compression level for each layer to minimize total reconstruction error while adhering to a target model size. Second, it introduces a single-step sparse matrix factorization inspired by dictionary learning: using only a small calibration set, it sparsifies weight coefficients based on activation-weights sensitivity and then updates the dictionary in closed form via least squares bypassing iterative optimization, sparse coding, or backpropagation entirely. ROCKET consistently outperforms existing compression approaches across different model architectures at 20-50\\% compression rates. Notably, it retains over 90\\% of the original model's performance at 30\\% compression without any fine-tuning. Moreover, when applying a light fine-tuning phase, recovery is substantially enhanced: for instance, compressing Qwen3-14B to an 8B-parameter model and healing it with just 30 million tokens yields performance nearly on par with the original Qwen3-8B. The code for ROCKET is at github.com/mts-ai/ROCKET/tree/main.",
      "authors": [
        {
          "_id": "698d707d65c0d15a6d16224f",
          "name": "Ammar Ali",
          "user": {
            "_id": "6166db59f78a267701a78c2a",
            "type": "user",
            "user": "ammarali32",
            "isPro": false,
            "fullname": "Ammar Ali",
            "avatarUrl": "/avatars/8784efc36f67719e9455b1f081340ed9.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:26:05.919Z"
        },
        {
          "_id": "698d707d65c0d15a6d162250",
          "name": "Baher Mohammad",
          "hidden": false
        },
        {
          "_id": "698d707d65c0d15a6d162251",
          "name": "Denis Makhov",
          "hidden": false
        },
        {
          "_id": "698d707d65c0d15a6d162252",
          "name": "Dmitriy Shopkhoev",
          "hidden": false
        },
        {
          "_id": "698d707d65c0d15a6d162253",
          "name": "Magauiya Zhussip",
          "hidden": false
        },
        {
          "_id": "698d707d65c0d15a6d162254",
          "name": "Stamatios Lefkimmiatis",
          "hidden": false
        }
      ],
      "published_at": "2026-02-11T16:34:52",
      "upvotes": 14,
      "date": "2026-02-12"
    },
    {
      "id": "2602.10224",
      "title": "Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience. Based on this insight, we propose Meta-Experience Learning (MEL), a novel framework that incorporates self-distilled meta-experience into the model's parametric memory. Building upon standard RLVR, we introduce an additional design that leverages the LLM's self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience. The meta-experience is further internalized into the LLM's parametric memory by minimizing the negative log-likelihood, which induces a language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92%--4.73% Pass@1 gains across varying model sizes.",
      "authors": [
        {
          "_id": "698d401f65c0d15a6d162015",
          "name": "Shiting Huang",
          "user": {
            "_id": "66ae3fbf491b555fef3bac0c",
            "type": "user",
            "user": "chocckaka",
            "isPro": false,
            "fullname": "Shiting Huang",
            "avatarUrl": "/avatars/47353470d46097ce108d32792dbbf2a2.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:57:00.511Z"
        },
        {
          "_id": "698d401f65c0d15a6d162016",
          "name": "Zecheng Li",
          "hidden": false
        },
        {
          "_id": "698d401f65c0d15a6d162017",
          "name": "Yu Zeng",
          "user": {
            "_id": "665d652e0f35c005de892108",
            "type": "user",
            "user": "YuZeng260",
            "isPro": false,
            "fullname": "Yu Zeng",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:57:02.566Z"
        },
        {
          "_id": "698d401f65c0d15a6d162018",
          "name": "Qingnan Ren",
          "hidden": false
        },
        {
          "_id": "698d401f65c0d15a6d162019",
          "name": "Zhen Fang",
          "hidden": false
        },
        {
          "_id": "698d401f65c0d15a6d16201a",
          "name": "Qisheng Su",
          "hidden": false
        },
        {
          "_id": "698d401f65c0d15a6d16201b",
          "name": "Kou Shi",
          "hidden": false
        },
        {
          "_id": "698d401f65c0d15a6d16201c",
          "name": "Lin Chen",
          "hidden": false
        },
        {
          "_id": "698d401f65c0d15a6d16201d",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "698d401f65c0d15a6d16201e",
          "name": "Feng Zhao",
          "hidden": false
        }
      ],
      "published_at": "2026-02-10T19:16:09",
      "upvotes": 13,
      "date": "2026-02-12"
    },
    {
      "id": "2602.11089",
      "title": "DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning",
      "summary": "In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the data recipe, which comprises a data processing pipeline to transform raw sources into training corpora. Despite the growing use of LLMs to automate individual data processing steps, such as data synthesis and filtering, the overall design of data recipes remains largely manual and labor-intensive, requiring substantial human expertise and iteration. To bridge this gap, we formulate end-to-end data recipe generation for LLM adaptation. Given a target benchmark and a pool of available data sources, a model is required to output a complete data recipe that adapts a base LLM to the target task. We present DataChef-32B, which performs online reinforcement learning using a proxy reward that predicts downstream performance for candidate recipes. Across six held-out tasks, DataChef-32B produces practical recipes that reach comparable downstream performance to those curated by human experts. Notably, the recipe from DataChef-32B adapts Qwen3-1.7B-Base to the math domain, achieving 66.7 on AIME'25 and surpassing Qwen3-1.7B. This work sheds new light on automating LLM training and developing self-evolving AI systems.",
      "authors": [
        {
          "_id": "698d4b1d65c0d15a6d1621a0",
          "name": "Yicheng Chen",
          "hidden": false
        },
        {
          "_id": "698d4b1d65c0d15a6d1621a1",
          "name": "Zerun Ma",
          "hidden": false
        },
        {
          "_id": "698d4b1d65c0d15a6d1621a2",
          "name": "Xinchen Xie",
          "hidden": false
        },
        {
          "_id": "698d4b1d65c0d15a6d1621a3",
          "name": "Yining Li",
          "hidden": false
        },
        {
          "_id": "698d4b1d65c0d15a6d1621a4",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "published_at": "2026-02-11T17:56:15",
      "upvotes": 12,
      "date": "2026-02-12"
    },
    {
      "id": "2602.11149",
      "title": "Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning",
      "summary": "Supervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models. Standard machine learning intuition suggests that training with more unique training samples yields better generalization. Counterintuitively, we show that SFT benefits from repetition: under a fixed update budget, training for more epochs on smaller datasets outperforms single-epoch training on larger datasets. On AIME'24/25 and GPQA benchmarks, Olmo3-7B trained for 128 epochs on 400 samples outperforms the equivalent 1 epoch on 51200 samples by 12-26 percentage points, with no additional catastrophic forgetting. We find that training token accuracy reliably signals when repetition has saturated; improvements from additional epochs plateau at full memorization, a pattern consistent across all settings. These findings provide a practical approach for reasoning SFT, where scaling epochs with token accuracy as a stopping criterion can replace expensive undirected data scaling. We pose the repetition advantage, where full memorization coincides with improved generalization, as a new open problem for the community in understanding the training dynamics of large language models.",
      "authors": [
        {
          "_id": "698d88a365c0d15a6d1622cb",
          "name": "Dawid J. Kopiczko",
          "user": {
            "_id": "637de0fa1342ba1762422495",
            "type": "user",
            "user": "dakopi",
            "isPro": false,
            "fullname": "Dawid",
            "avatarUrl": "/avatars/b1cce386a8b33007fc381fcbfc5cdc9a.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:26:00.517Z"
        },
        {
          "_id": "698d88a365c0d15a6d1622cc",
          "name": "Sagar Vaze",
          "hidden": false
        },
        {
          "_id": "698d88a365c0d15a6d1622cd",
          "name": "Tijmen Blankevoort",
          "hidden": false
        },
        {
          "_id": "698d88a365c0d15a6d1622ce",
          "name": "Yuki M. Asano",
          "hidden": false
        }
      ],
      "published_at": "2026-02-11T18:58:54",
      "upvotes": 10,
      "date": "2026-02-12"
    },
    {
      "id": "2602.07106",
      "title": "Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models",
      "summary": "Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.",
      "authors": [
        {
          "_id": "698d49ec65c0d15a6d162170",
          "name": "Haoyu Zhang",
          "user": {
            "_id": "665d72007bef1cfc313a92dd",
            "type": "user",
            "user": "lemonade666",
            "isPro": false,
            "fullname": "Haoyu Zhang",
            "avatarUrl": "/avatars/6d56671153bbf1ffff072472678819da.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:28:21.307Z"
        },
        {
          "_id": "698d49ec65c0d15a6d162171",
          "name": "Zhipeng Li",
          "hidden": false
        },
        {
          "_id": "698d49ec65c0d15a6d162172",
          "name": "Yiwen Guo",
          "hidden": false
        },
        {
          "_id": "698d49ec65c0d15a6d162173",
          "name": "Tianshu Yu",
          "hidden": false
        }
      ],
      "published_at": "2026-02-06T18:03:30",
      "upvotes": 10,
      "date": "2026-02-12"
    },
    {
      "id": "2602.10609",
      "title": "Online Causal Kalman Filtering for Stable and Effective Policy Optimization",
      "summary": "Reinforcement learning for large language models suffers from high-variance token-level importance sampling (IS) ratios, which would destabilize policy optimization at scale. To improve stability, recent methods typically use a fixed sequence-level IS ratio for all tokens in a sequence or adjust each token's IS ratio separately, thereby neglecting temporal off-policy derivation across tokens in a sequence. In this paper, we first empirically identify that local off-policy deviation is structurally inconsistent at the token level, which may distort policy-gradient updates across adjacent tokens and lead to training collapse. To address the issue, we propose Online Causal Kalman Filtering for stable and effective Policy Optimization (KPO). Concretely, we model the desired IS ratio as a latent state that evolves across tokens and apply a Kalman filter to update this state online and autoregressively based on the states of past tokens, regardless of future tokens. The resulting filtered IS ratios preserve token-wise local structure-aware variation while strongly smoothing noise spikes, yielding more stable and effective policy updates. Experimentally, KPO achieves superior results on challenging math reasoning datasets compared with state-of-the-art counterparts.",
      "authors": [
        {
          "_id": "698d37a66c5152984e4f3e7b",
          "name": "Shuo He",
          "hidden": false
        },
        {
          "_id": "698d37a66c5152984e4f3e7c",
          "name": "Lang Feng",
          "user": {
            "_id": "66ba29dd59e8e7a957154c5f",
            "type": "user",
            "user": "langfeng01",
            "isPro": false,
            "fullname": "Lang Feng",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T20:23:25.095Z"
        },
        {
          "_id": "698d37a66c5152984e4f3e7d",
          "name": "Xin Cheng",
          "hidden": false
        },
        {
          "_id": "698d37a66c5152984e4f3e7e",
          "name": "Lei Feng",
          "user": {
            "_id": "698dc46341d49f1062061c64",
            "type": "user",
            "user": "CharlesFeng1995",
            "isPro": false,
            "fullname": "Lei Feng",
            "avatarUrl": "/avatars/34d7f69e61aeb37e226da8488f8b8c98.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:57:10.067Z"
        },
        {
          "_id": "698d37a66c5152984e4f3e7f",
          "name": "Bo An",
          "hidden": false
        }
      ],
      "published_at": "2026-02-11T07:57:43",
      "upvotes": 10,
      "date": "2026-02-12"
    },
    {
      "id": "2602.11103",
      "title": "GameDevBench: Evaluating Agentic Capabilities Through Game Development",
      "summary": "Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5's performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development.",
      "authors": [
        {
          "_id": "698d4fd565c0d15a6d1621cb",
          "name": "Wayne Chi",
          "hidden": false
        },
        {
          "_id": "698d4fd565c0d15a6d1621cc",
          "name": "Yixiong Fang",
          "hidden": false
        },
        {
          "_id": "698d4fd565c0d15a6d1621cd",
          "name": "Arnav Yayavaram",
          "hidden": false
        },
        {
          "_id": "698d4fd565c0d15a6d1621ce",
          "name": "Siddharth Yayavaram",
          "hidden": false
        },
        {
          "_id": "698d4fd565c0d15a6d1621cf",
          "name": "Seth Karten",
          "hidden": false
        },
        {
          "_id": "698d4fd565c0d15a6d1621d0",
          "name": "Qiuhong Anna Wei",
          "hidden": false
        },
        {
          "_id": "698d4fd565c0d15a6d1621d1",
          "name": "Runkun Chen",
          "hidden": false
        },
        {
          "_id": "698d4fd565c0d15a6d1621d2",
          "name": "Alexander Wang",
          "hidden": false
        },
        {
          "_id": "698d4fd565c0d15a6d1621d3",
          "name": "Valerie Chen",
          "hidden": false
        },
        {
          "_id": "698d4fd565c0d15a6d1621d4",
          "name": "Ameet Talwalkar",
          "hidden": false
        },
        {
          "_id": "698d4fd565c0d15a6d1621d5",
          "name": "Chris Donahue",
          "hidden": false
        }
      ],
      "published_at": "2026-02-11T18:15:11",
      "upvotes": 9,
      "date": "2026-02-12"
    },
    {
      "id": "2602.09514",
      "title": "EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies",
      "summary": "Long-horizon planning is widely recognized as a core capability of autonomous LLM-based agents; however, current evaluation frameworks suffer from being largely episodic, domain-specific, or insufficiently grounded in persistent economic dynamics. We introduce EcoGym, a generalizable benchmark for continuous plan-and-execute decision making in interactive economies. EcoGym comprises three diverse environments: Vending, Freelance, and Operation, implemented in a unified decision-making process with standardized interfaces, and budgeted actions over an effectively unbounded horizon (1000+ steps if 365 day-loops for evaluation). The evaluation of EcoGym is based on business-relevant outcomes (e.g., net worth, income, and DAU), targeting long-term strategic coherence and robustness under partial observability and stochasticity. Experiments across eleven leading LLMs expose a systematic tension: no single model dominates across all three scenarios. Critically, we find that models exhibit significant suboptimality in either high-level strategies or efficient actions executions. EcoGym is released as an open, extensible testbed for transparent long-horizon agent evaluation and for studying controllability-utility trade-offs in realistic economic settings.",
      "authors": [
        {
          "_id": "698d873b65c0d15a6d1622b9",
          "name": "Xavier Hu",
          "hidden": false
        },
        {
          "_id": "698d873b65c0d15a6d1622ba",
          "name": "Jinxiang Xia",
          "hidden": false
        },
        {
          "_id": "698d873b65c0d15a6d1622bb",
          "name": "Shengze Xu",
          "hidden": false
        },
        {
          "_id": "698d873b65c0d15a6d1622bc",
          "name": "Kangqi Song",
          "hidden": false
        },
        {
          "_id": "698d873b65c0d15a6d1622bd",
          "name": "Yishuo Yuan",
          "hidden": false
        },
        {
          "_id": "698d873b65c0d15a6d1622be",
          "name": "Guibin Zhang",
          "hidden": false
        },
        {
          "_id": "698d873b65c0d15a6d1622bf",
          "name": "JinCheng Ren",
          "hidden": false
        },
        {
          "_id": "698d873b65c0d15a6d1622c0",
          "name": "Boyu Feng",
          "hidden": false
        },
        {
          "_id": "698d873b65c0d15a6d1622c1",
          "name": "Li Lu",
          "hidden": false
        },
        {
          "_id": "698d873b65c0d15a6d1622c2",
          "name": "Tieyong Zeng",
          "hidden": false
        },
        {
          "_id": "698d873b65c0d15a6d1622c3",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "698d873b65c0d15a6d1622c4",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "698d873b65c0d15a6d1622c5",
          "name": "He Zhu",
          "hidden": false
        },
        {
          "_id": "698d873b65c0d15a6d1622c6",
          "name": "Yuchen Eleanor Jiang",
          "hidden": false
        },
        {
          "_id": "698d873b65c0d15a6d1622c7",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "698d873b65c0d15a6d1622c8",
          "name": "Wangchunshu Zhou",
          "hidden": false
        }
      ],
      "published_at": "2026-02-10T08:12:23",
      "upvotes": 9,
      "date": "2026-02-12"
    },
    {
      "id": "2602.08099",
      "title": "VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval",
      "summary": "Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.",
      "authors": [
        {
          "_id": "698b10fb1b2dc6b37d61b3f7",
          "name": "Issar Tzachor",
          "user": {
            "_id": "66c704d5c797952bc2360ecb",
            "type": "user",
            "user": "issart12345",
            "isPro": false,
            "fullname": "issart",
            "avatarUrl": "/avatars/11e999a17c043c100571d8df0d966fdf.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T20:23:27.323Z"
        },
        {
          "_id": "698b10fb1b2dc6b37d61b3f8",
          "name": "Dvir Samuel",
          "hidden": false
        },
        {
          "_id": "698b10fb1b2dc6b37d61b3f9",
          "name": "Rami Ben-Ari",
          "hidden": false
        }
      ],
      "published_at": "2026-02-08T19:39:32",
      "upvotes": 9,
      "date": "2026-02-12"
    },
    {
      "id": "2602.10999",
      "title": "CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion",
      "summary": "Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks.",
      "authors": [
        {
          "_id": "698d437e65c0d15a6d162132",
          "name": "Yusong Lin",
          "hidden": false
        },
        {
          "_id": "698d437e65c0d15a6d162133",
          "name": "Haiyang Wang",
          "user": {
            "_id": "65f43c3cc9940817caaf4434",
            "type": "user",
            "user": "Haiyang-W",
            "isPro": false,
            "fullname": "Haiyang Wang",
            "avatarUrl": "/avatars/ecec2856ba7a7d3421a2071a0a88800b.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:28:33.349Z"
        },
        {
          "_id": "698d437e65c0d15a6d162134",
          "name": "Shuzhe Wu",
          "hidden": false
        },
        {
          "_id": "698d437e65c0d15a6d162135",
          "name": "Lue Fan",
          "hidden": false
        },
        {
          "_id": "698d437e65c0d15a6d162136",
          "name": "Feiyang Pan",
          "hidden": false
        },
        {
          "_id": "698d437e65c0d15a6d162137",
          "name": "Sanyuan Zhao",
          "hidden": false
        },
        {
          "_id": "698d437e65c0d15a6d162138",
          "name": "Dandan Tu",
          "hidden": false
        }
      ],
      "published_at": "2026-02-11T16:22:18",
      "upvotes": 8,
      "date": "2026-02-12"
    }
  ],
  "2026-02-11": [
    {
      "id": "2602.05400",
      "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration",
      "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.",
      "authors": [
        {
          "_id": "698b396b1b2dc6b37d61b4be",
          "name": "Shaobo Wang",
          "user": {
            "_id": "66968099c952e09a4cb29f78",
            "type": "user",
            "user": "Steven-Shaobo",
            "isPro": false,
            "fullname": "Wang",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66968099c952e09a4cb29f78/n90NI2R3E9_RqCyMjDCQF.webp"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:16:57.815Z"
        },
        {
          "_id": "698b396b1b2dc6b37d61b4bf",
          "name": "Xuan Ouyang",
          "user": {
            "_id": "67e617d4470f96a302734e16",
            "type": "user",
            "user": "YoungXuan",
            "isPro": false,
            "fullname": "Xuan Ouyang",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:16:55.631Z"
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c0",
          "name": "Tianyi Xu",
          "user": {
            "_id": "6518a144a28f86d3e9e67c34",
            "type": "user",
            "user": "tianyi0216",
            "isPro": false,
            "fullname": "Tianyi Xu",
            "avatarUrl": "/avatars/f2aed39e971cffe6c9d0b9c2f7a0df70.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:16:53.605Z"
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c1",
          "name": "Yuzheng Hu",
          "hidden": false
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c2",
          "name": "Jialin Liu",
          "hidden": false
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c3",
          "name": "Guo Chen",
          "hidden": false
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c4",
          "name": "Tianyu Zhang",
          "hidden": false
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c5",
          "name": "Junhao Zheng",
          "hidden": false
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c6",
          "name": "Kexin Yang",
          "hidden": false
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c7",
          "name": "Xingzhang Ren",
          "hidden": false
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c8",
          "name": "Dayiheng Liu",
          "hidden": false
        },
        {
          "_id": "698b396b1b2dc6b37d61b4c9",
          "name": "Linfeng Zhang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-05T07:34:23",
      "upvotes": 294,
      "date": "2026-02-11"
    },
    {
      "id": "2602.09856",
      "title": "Code2World: A GUI World Model via Renderable Code Generation",
      "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.",
      "authors": [
        {
          "_id": "698bf5b66052d3bed9630aa7",
          "name": "Yuhao Zheng",
          "user": {
            "_id": "64107c7df52d7eb22e062956",
            "type": "user",
            "user": "yhzheng1031",
            "isPro": false,
            "fullname": "Yuhao Zheng",
            "avatarUrl": "/avatars/7b1cee9a2b8454fedfbd4c3d1df9865c.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:14:28.241Z"
        },
        {
          "_id": "698bf5b66052d3bed9630aa8",
          "name": "Li'an Zhong",
          "hidden": false
        },
        {
          "_id": "698bf5b66052d3bed9630aa9",
          "name": "Yi Wang",
          "user": {
            "_id": "6773bcaa675a971ddf1e81dd",
            "type": "user",
            "user": "CokeWang",
            "isPro": false,
            "fullname": "CokeWang",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a8VUwZYXd7O_mq_zFvXMh.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:14:30.778Z"
        },
        {
          "_id": "698bf5b66052d3bed9630aaa",
          "name": "Rui Dai",
          "user": {
            "_id": "661de9defdbc9c247f159d15",
            "type": "user",
            "user": "DerryD",
            "isPro": false,
            "fullname": "Rui Dai",
            "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:14:25.982Z"
        },
        {
          "_id": "698bf5b66052d3bed9630aab",
          "name": "Kaikui Liu",
          "hidden": false
        },
        {
          "_id": "698bf5b66052d3bed9630aac",
          "name": "Xiangxiang Chu",
          "hidden": false
        },
        {
          "_id": "698bf5b66052d3bed9630aad",
          "name": "Linyuan Lv",
          "hidden": false
        },
        {
          "_id": "698bf5b66052d3bed9630aae",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "698bf5b66052d3bed9630aaf",
          "name": "Kevin Qinghong Lin",
          "user": {
            "_id": "64440be5af034cdfd69ca3a7",
            "type": "user",
            "user": "KevinQHLin",
            "isPro": false,
            "fullname": "Qinghong (Kevin) Lin",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:14:23.397Z"
        }
      ],
      "published_at": "2026-02-10T14:56:19",
      "upvotes": 182,
      "date": "2026-02-11"
    },
    {
      "id": "2602.09082",
      "title": "UI-Venus-1.5 Technical Report",
      "summary": "GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus",
      "authors": [
        {
          "_id": "698bea506052d3bed96309cb",
          "name": "Veuns-Team",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309cd",
          "name": "Changlong Gao",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309ce",
          "name": "Zhangxuan Gu",
          "user": {
            "_id": "60d2a2984956988b63753371",
            "type": "user",
            "user": "zhangxgu",
            "isPro": false,
            "fullname": "Zhangxuan Gu",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:15:14.456Z"
        },
        {
          "_id": "698bea506052d3bed96309cf",
          "name": "Yulin Liu",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309d0",
          "name": "Xinyu Qiu",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309d1",
          "name": "Shuheng Shen",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309d2",
          "name": "Yue Wen",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309d3",
          "name": "Tianyu Xia",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309d4",
          "name": "Zhenyu Xu",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309d5",
          "name": "Zhengwen Zeng",
          "user": {
            "_id": "64cb238576200ec80fe988f8",
            "type": "user",
            "user": "zengw",
            "isPro": false,
            "fullname": "zeus",
            "avatarUrl": "/avatars/42c48710c7881c9dfbcc075fec3cb600.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:24:43.235Z"
        },
        {
          "_id": "698bea506052d3bed96309d6",
          "name": "Beitong Zhou",
          "user": {
            "_id": "654c9dac09dd7ef524a0be1e",
            "type": "user",
            "user": "syorami",
            "isPro": false,
            "fullname": "beitongzhou",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654c9dac09dd7ef524a0be1e/T4glmZthS0mJydhvGZGKH.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:15:11.859Z"
        },
        {
          "_id": "698bea506052d3bed96309d7",
          "name": "Xingran Zhou",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309d8",
          "name": "Weizhi Chen",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309d9",
          "name": "Sunhao Dai",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309da",
          "name": "Jingya Dou",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309db",
          "name": "Yichen Gong",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309dc",
          "name": "Yuan Guo",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309dd",
          "name": "Zhenlin Guo",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309de",
          "name": "Feng Li",
          "user": {
            "_id": "65e0763a9299e96ee674876e",
            "type": "user",
            "user": "fengrudian",
            "isPro": false,
            "fullname": "fengdian",
            "avatarUrl": "/avatars/0ea342c9f72fa3b8a8f634559d094907.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:16:04.463Z"
        },
        {
          "_id": "698bea506052d3bed96309df",
          "name": "Qian Li",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309e0",
          "name": "Jinzhen Lin",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309e1",
          "name": "Yuqi Zhou",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309e2",
          "name": "Linchao Zhu",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309e3",
          "name": "Liang Chen",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309e4",
          "name": "Zhenyu Guo",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309e5",
          "name": "Changhua Meng",
          "hidden": false
        },
        {
          "_id": "698bea506052d3bed96309e6",
          "name": "Weiqiang Wang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-09T18:43:40",
      "upvotes": 147,
      "date": "2026-02-11"
    },
    {
      "id": "2602.10063",
      "title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes",
      "summary": "Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\\% and 4.72\\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at https://github.com/QuantaAlpha/chain-of-mindset{https://github.com/QuantaAlpha/chain-of-mindset}.",
      "authors": [
        {
          "_id": "698bf4ef6052d3bed9630a96",
          "name": "Tianyi Jiang",
          "user": {
            "_id": "6895e7f146763431aea25ca4",
            "type": "user",
            "user": "LumosJiang",
            "isPro": false,
            "fullname": "Tianyi Jiang",
            "avatarUrl": "/avatars/52e550c3f7e8da2e31b63413e2e71e6c.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:14:33.352Z"
        },
        {
          "_id": "698bf4ef6052d3bed9630a97",
          "name": "Arctanx An",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630a98",
          "name": "Hengyi Feng",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630a99",
          "name": "Naixin Zhai",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630a9a",
          "name": "Haodong Li",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630a9b",
          "name": "Xiaomin Yu",
          "user": {
            "_id": "64084fa192033c150738e4f2",
            "type": "user",
            "user": "Yu2020",
            "isPro": false,
            "fullname": "Yu_xm",
            "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T12:34:26.745Z"
        },
        {
          "_id": "698bf4ef6052d3bed9630a9c",
          "name": "Jiahui Liu",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630a9d",
          "name": "Hanwen Du",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630a9e",
          "name": "Shuo Zhang",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630a9f",
          "name": "Zhi Yang",
          "user": {
            "_id": "64aa645404e7b379feccc490",
            "type": "user",
            "user": "yangzhi1",
            "isPro": false,
            "fullname": "Zhi Yang",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:14:35.722Z"
        },
        {
          "_id": "698bf4ef6052d3bed9630aa0",
          "name": "Jie Huang",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630aa1",
          "name": "Yuhua Li",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630aa2",
          "name": "Yongxin Ni",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630aa3",
          "name": "Huacan Wang",
          "hidden": false
        },
        {
          "_id": "698bf4ef6052d3bed9630aa4",
          "name": "Ronghao Chen",
          "hidden": false
        }
      ],
      "published_at": "2026-02-10T18:31:47",
      "upvotes": 69,
      "date": "2026-02-11"
    },
    {
      "id": "2602.08234",
      "title": "SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning",
      "summary": "Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.",
      "authors": [
        {
          "_id": "698aba731b2dc6b37d61b0e4",
          "name": "Peng Xia",
          "user": {
            "_id": "643e9ee6f6bb3c31a26e7bc4",
            "type": "user",
            "user": "richardxp888",
            "isPro": false,
            "fullname": "Peng Xia",
            "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:18:58.896Z"
        },
        {
          "_id": "698aba731b2dc6b37d61b0e5",
          "name": "Jianwen Chen",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0e6",
          "name": "Hanyang Wang",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0e7",
          "name": "Jiaqi Liu",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0e8",
          "name": "Kaide Zeng",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0e9",
          "name": "Yu Wang",
          "user": {
            "_id": "63234809155b0e2c44f354d6",
            "type": "user",
            "user": "YuWangX",
            "isPro": false,
            "fullname": "Yu Wang",
            "avatarUrl": "/avatars/60d38f8f0e12363f3f5e0388e635d7b6.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:18:56.703Z"
        },
        {
          "_id": "698aba731b2dc6b37d61b0ea",
          "name": "Siwei Han",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0eb",
          "name": "Yiyang Zhou",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0ec",
          "name": "Xujiang Zhao",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0ed",
          "name": "Haifeng Chen",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0ee",
          "name": "Zeyu Zheng",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0ef",
          "name": "Cihang Xie",
          "hidden": false
        },
        {
          "_id": "698aba731b2dc6b37d61b0f0",
          "name": "Huaxiu Yao",
          "hidden": false
        }
      ],
      "published_at": "2026-02-09T03:17:17",
      "upvotes": 63,
      "date": "2026-02-11"
    },
    {
      "id": "2602.09443",
      "title": "P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads",
      "summary": "The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.",
      "authors": [
        {
          "_id": "698c019f6052d3bed9630b1c",
          "name": "Yun Luo",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b1d",
          "name": "Futing Wang",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b1e",
          "name": "Qianjia Cheng",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b1f",
          "name": "Fangchen Yu",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b20",
          "name": "Haodi Lei",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b21",
          "name": "Jianhao Yan",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b22",
          "name": "Chenxi Li",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b23",
          "name": "Jiacheng Chen",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b24",
          "name": "Yufeng Zhao",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b25",
          "name": "Haiyuan Wan",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b26",
          "name": "Yuchen Zhang",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b27",
          "name": "Shenghe Zheng",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b28",
          "name": "Junchi Yao",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b29",
          "name": "Qingyang Zhang",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b2a",
          "name": "Haonan He",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b2b",
          "name": "Wenxuan Zeng",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b2c",
          "name": "Li Sheng",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b2d",
          "name": "Chengxing Xie",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b2e",
          "name": "Yuxin Zuo",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b2f",
          "name": "Yizhuo Li",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b30",
          "name": "Yulun Wu",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b31",
          "name": "Rui Huang",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b32",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b33",
          "name": "Kai Chen",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b34",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b35",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b36",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b37",
          "name": "Ning Ding",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b38",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b39",
          "name": "Peng Ye",
          "hidden": false
        },
        {
          "_id": "698c019f6052d3bed9630b3a",
          "name": "Ganqu Cui",
          "hidden": false
        }
      ],
      "published_at": "2026-02-10T06:28:08",
      "upvotes": 54,
      "date": "2026-02-11"
    },
    {
      "id": "2602.10090",
      "title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning",
      "summary": "Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.",
      "authors": [
        {
          "_id": "698bf6fe6052d3bed9630ac0",
          "name": "Zhaoyang Wang",
          "hidden": false
        },
        {
          "_id": "698bf6fe6052d3bed9630ac1",
          "name": "Canwen Xu",
          "hidden": false
        },
        {
          "_id": "698bf6fe6052d3bed9630ac2",
          "name": "Boyi Liu",
          "hidden": false
        },
        {
          "_id": "698bf6fe6052d3bed9630ac3",
          "name": "Yite Wang",
          "hidden": false
        },
        {
          "_id": "698bf6fe6052d3bed9630ac4",
          "name": "Siwei Han",
          "hidden": false
        },
        {
          "_id": "698bf6fe6052d3bed9630ac5",
          "name": "Zhewei Yao",
          "hidden": false
        },
        {
          "_id": "698bf6fe6052d3bed9630ac6",
          "name": "Huaxiu Yao",
          "hidden": false
        },
        {
          "_id": "698bf6fe6052d3bed9630ac7",
          "name": "Yuxiong He",
          "hidden": false
        }
      ],
      "published_at": "2026-02-10T18:55:41",
      "upvotes": 46,
      "date": "2026-02-11"
    },
    {
      "id": "2602.08426",
      "title": "Prism: Spectral-Aware Block-Sparse Attention",
      "summary": "Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a \"blind spot\" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to 5.1times speedup.",
      "authors": [
        {
          "_id": "698bedd06052d3bed9630a13",
          "name": "Xinghao Wang",
          "user": {
            "_id": "6191f22d08a57f265f7f5266",
            "type": "user",
            "user": "Singhoo",
            "isPro": false,
            "fullname": "XinghaoWang",
            "avatarUrl": "/avatars/215164e7b6d025a3c32555ff541cdd62.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:15:09.483Z"
        },
        {
          "_id": "698bedd06052d3bed9630a14",
          "name": "Pengyu Wang",
          "hidden": false
        },
        {
          "_id": "698bedd06052d3bed9630a15",
          "name": "Xiaoran Liu",
          "user": {
            "_id": "64f033ef82c6eea604c4da8b",
            "type": "user",
            "user": "SII-xrliu",
            "isPro": false,
            "fullname": "Xiaoran Liu (SII)",
            "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:15:02.192Z"
        },
        {
          "_id": "698bedd06052d3bed9630a16",
          "name": "Fangxu Liu",
          "hidden": false
        },
        {
          "_id": "698bedd06052d3bed9630a17",
          "name": "Jason Chu",
          "hidden": false
        },
        {
          "_id": "698bedd06052d3bed9630a18",
          "name": "Kai Song",
          "hidden": false
        },
        {
          "_id": "698bedd06052d3bed9630a19",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "published_at": "2026-02-09T09:31:06",
      "upvotes": 33,
      "date": "2026-02-11"
    },
    {
      "id": "2602.07035",
      "title": "DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents",
      "summary": "Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C",
      "authors": [
        {
          "_id": "698b43df6052d3bed963079f",
          "name": "Jiahao Zhao",
          "user": {
            "_id": "698b419584704fee74958520",
            "type": "user",
            "user": "bubble65",
            "isPro": false,
            "fullname": "Zhao Jiahao",
            "avatarUrl": "/avatars/7e94be7a4396174e3546114c3e3af598.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:16:42.263Z"
        },
        {
          "_id": "698b43df6052d3bed96307a0",
          "name": "Shaoxuan Xu",
          "hidden": false
        },
        {
          "_id": "698b43df6052d3bed96307a1",
          "name": "Zhongxiang Sun",
          "user": {
            "_id": "6309bfdab8d7b3889319b588",
            "type": "user",
            "user": "Jeryi",
            "isPro": false,
            "fullname": "SunZX",
            "avatarUrl": "/avatars/572acdad470f765ef2e058ead3741e24.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:16:40.143Z"
        },
        {
          "_id": "698b43df6052d3bed96307a2",
          "name": "Fengqi Zhu",
          "hidden": false
        },
        {
          "_id": "698b43df6052d3bed96307a3",
          "name": "Jingyang Ou",
          "hidden": false
        },
        {
          "_id": "698b43df6052d3bed96307a4",
          "name": "Yuling Shi",
          "user": {
            "_id": "645b0c3ec35da9c7afd95421",
            "type": "user",
            "user": "YerbaPage",
            "isPro": false,
            "fullname": "Yuling",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:16:37.162Z"
        },
        {
          "_id": "698b43df6052d3bed96307a5",
          "name": "Chongxuan Li",
          "hidden": false
        },
        {
          "_id": "698b43df6052d3bed96307a6",
          "name": "Xiao Zhang",
          "hidden": false
        },
        {
          "_id": "698b43df6052d3bed96307a7",
          "name": "Jun Xu",
          "hidden": false
        }
      ],
      "published_at": "2026-02-03T09:12:08",
      "upvotes": 27,
      "date": "2026-02-11"
    },
    {
      "id": "2602.10104",
      "title": "Olaf-World: Orienting Latent Actions for Video World Modeling",
      "summary": "Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq\u0394-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.",
      "authors": [
        {
          "_id": "698bfbeb6052d3bed9630ae1",
          "name": "Yuxin Jiang",
          "user": {
            "_id": "64e84d40d50f3979be9afcbb",
            "type": "user",
            "user": "YuxinJ",
            "isPro": false,
            "fullname": "Jiang Yuxin",
            "avatarUrl": "/avatars/6a706a4916132c1f1cda63d11dc46b87.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:14:13.438Z"
        },
        {
          "_id": "698bfbeb6052d3bed9630ae2",
          "name": "Yuchao Gu",
          "hidden": false
        },
        {
          "_id": "698bfbeb6052d3bed9630ae3",
          "name": "Ivor W. Tsang",
          "hidden": false
        },
        {
          "_id": "698bfbeb6052d3bed9630ae4",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "published_at": "2026-02-10T18:58:41",
      "upvotes": 25,
      "date": "2026-02-11"
    },
    {
      "id": "2602.09084",
      "title": "Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling",
      "summary": "We study instruction-based image editing under professional workflows and identify three persistent challenges: (i) editors often over-edit, modifying content beyond the user's intent; (ii) existing models are largely single-turn, while multi-turn edits can alter object faithfulness; and (iii) evaluation at around 1K resolution is misaligned with real workflows that often operate on ultra high-definition images (e.g., 4K). We propose Agent Banana, a hierarchical agentic planner-executor framework for high-fidelity, object-aware, deliberative editing. Agent Banana introduces two key mechanisms: (1) Context Folding, which compresses long interaction histories into structured memory for stable long-horizon control; and (2) Image Layer Decomposition, which performs localized layer-based edits to preserve non-target regions while enabling native-resolution outputs. To support rigorous evaluation, we build HDD-Bench, a high-definition, dialogue-based benchmark featuring verifiable stepwise targets and native 4K images (11.8M pixels) for diagnosing long-horizon failures. On HDD-Bench, Agent Banana achieves the best multi-turn consistency and background fidelity (e.g., IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12) while remaining competitive on instruction following, and also attains strong performance on standard single-turn editing benchmarks. We hope this work advances reliable, professional-grade agentic image editing and its integration into real workflows.",
      "authors": [
        {
          "_id": "698be8996052d3bed96309ac",
          "name": "Ruijie Ye",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309ad",
          "name": "Jiayi Zhang",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309ae",
          "name": "Zhuoxin Liu",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309af",
          "name": "Zihao Zhu",
          "user": {
            "_id": "66dd321b41074b6a3df723d4",
            "type": "user",
            "user": "SingleBicycle",
            "isPro": false,
            "fullname": "Zihao Zhu",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yeFbjcbmTT0U3c7i0eTdZ.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:15:16.736Z"
        },
        {
          "_id": "698be8996052d3bed96309b0",
          "name": "Siyuan Yang",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309b1",
          "name": "Li Li",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309b2",
          "name": "Tianfu Fu",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309b3",
          "name": "Franck Dernoncourt",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309b4",
          "name": "Yue Zhao",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309b5",
          "name": "Jiacheng Zhu",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309b6",
          "name": "Ryan Rossi",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309b7",
          "name": "Wenhao Chai",
          "hidden": false
        },
        {
          "_id": "698be8996052d3bed96309b8",
          "name": "Zhengzhong Tu",
          "hidden": false
        }
      ],
      "published_at": "2026-02-09T18:59:18",
      "upvotes": 22,
      "date": "2026-02-11"
    },
    {
      "id": "2602.00268",
      "title": "TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation",
      "summary": "Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.",
      "authors": [
        {
          "_id": "698c6b52eb12ea7453916823",
          "name": "Ariel Shaulov",
          "user": {
            "_id": "65d4985d4e358ce02a949f8c",
            "type": "user",
            "user": "shaulov",
            "isPro": false,
            "fullname": "Ariel Shaulov",
            "avatarUrl": "/avatars/3eda6f50d17802b1ce94349c89637e3c.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T12:34:28.330Z"
        },
        {
          "_id": "698c6b52eb12ea7453916824",
          "name": "Eitan Shaar",
          "hidden": false
        },
        {
          "_id": "698c6b52eb12ea7453916825",
          "name": "Amit Edenzon",
          "hidden": false
        },
        {
          "_id": "698c6b52eb12ea7453916826",
          "name": "Lior Wolf",
          "hidden": false
        }
      ],
      "published_at": "2026-01-30T19:44:16",
      "upvotes": 20,
      "date": "2026-02-11"
    },
    {
      "id": "2602.07022",
      "title": "Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss",
      "summary": "Recent studies have explored autoregressive models for image generation, with promising results, and have combined diffusion models with autoregressive frameworks to optimize image generation via diffusion losses. In this study, we present a theoretical analysis of diffusion and autoregressive models with diffusion loss, highlighting the latter's advantages. We present a theoretical comparison of conditional diffusion and autoregressive diffusion with diffusion loss, demonstrating that patch denoising optimization in autoregressive models effectively mitigates condition errors and leads to a stable condition distribution. Our analysis also reveals that autoregressive condition generation refines the condition, causing the condition error influence to decay exponentially. In addition, we introduce a novel condition refinement approach based on Optimal Transport (OT) theory to address ``condition inconsistency''. We theoretically demonstrate that formulating condition refinement as a Wasserstein Gradient Flow ensures convergence toward the ideal condition distribution, effectively mitigating condition inconsistency. Experiments demonstrate the superiority of our method over diffusion and autoregressive models with diffusion loss methods.",
      "authors": [
        {
          "_id": "698c2ee36052d3bed9630c8e",
          "name": "Yucheng Zhou",
          "user": {
            "_id": "636f37fa93d9a0c987e092fa",
            "type": "user",
            "user": "YCZhou",
            "isPro": false,
            "fullname": "Yucheng Zhou",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636f37fa93d9a0c987e092fa/vdZgFPobSIUbBTC3jlfH5.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:13:43.761Z"
        },
        {
          "_id": "698c2ee36052d3bed9630c8f",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "698c2ee36052d3bed9630c90",
          "name": "Jianbing Shen",
          "hidden": false
        }
      ],
      "published_at": "2026-02-02T07:48:04",
      "upvotes": 19,
      "date": "2026-02-11"
    },
    {
      "id": "2602.04208",
      "title": "SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models",
      "summary": "Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic control, with test-time scaling (TTS) gaining attention to enhance robustness beyond training. However, existing TTS methods for VLAs require additional training, verifiers, and multiple forward passes, making them impractical for deployment. Moreover, they intervene only at action decoding while keeping visual representations fixed-insufficient under perceptual ambiguity, where reconsidering how to perceive is as important as deciding what to do. To address these limitations, we propose SCALE, a simple inference strategy that jointly modulates visual perception and action based on 'self-uncertainty', inspired by uncertainty-driven exploration in Active Inference theory-requiring no additional training, no verifier, and only a single forward pass. SCALE broadens exploration in both perception and action under high uncertainty, while focusing on exploitation when confident-enabling adaptive execution across varying conditions. Experiments on simulated and real-world benchmarks demonstrate that SCALE improves state-of-the-art VLAs and outperforms existing TTS methods while maintaining single-pass efficiency.",
      "authors": [
        {
          "_id": "69881f9dbeecc443208d245f",
          "name": "Hyeonbeom Choi",
          "user": {
            "_id": "675288db99b478caa10a95e7",
            "type": "user",
            "user": "violet-blue",
            "isPro": false,
            "fullname": "Hyeonbeom Choi",
            "avatarUrl": "/avatars/e3c63a61b324a21b1853e1def2915910.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:19:31.704Z"
        },
        {
          "_id": "69881f9dbeecc443208d2460",
          "name": "Daechul Ahn",
          "user": {
            "_id": "6384ac60b2906edaf8342ca5",
            "type": "user",
            "user": "Daechul",
            "isPro": false,
            "fullname": "Daechul Ahn",
            "avatarUrl": "/avatars/033d27d1ef43444300b8a5c97447000a.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:30:40.330Z"
        },
        {
          "_id": "69881f9dbeecc443208d2461",
          "name": "Youhan Lee",
          "hidden": false
        },
        {
          "_id": "69881f9dbeecc443208d2462",
          "name": "Taewook Kang",
          "hidden": false
        },
        {
          "_id": "69881f9dbeecc443208d2463",
          "name": "Seongwon Cho",
          "hidden": false
        },
        {
          "_id": "69881f9dbeecc443208d2464",
          "name": "Jonghyun Choi",
          "hidden": false
        }
      ],
      "published_at": "2026-02-04T04:48:16",
      "upvotes": 18,
      "date": "2026-02-11"
    },
    {
      "id": "2602.08847",
      "title": "Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems",
      "summary": "Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\\% avg@16 and +4.6\\% pass@16 on math, and +15.2\\% avg@16 and +13.1\\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.",
      "authors": [
        {
          "_id": "698c133d6052d3bed9630bf2",
          "name": "Lang Feng",
          "user": {
            "_id": "66ba29dd59e8e7a957154c5f",
            "type": "user",
            "user": "langfeng01",
            "isPro": false,
            "fullname": "Lang Feng",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:13:38.406Z"
        },
        {
          "_id": "698c133d6052d3bed9630bf3",
          "name": "Longtao Zheng",
          "user": {
            "_id": "63db5dc49f2687298a1547bf",
            "type": "user",
            "user": "ltzheng",
            "isPro": false,
            "fullname": "Longtao Zheng",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63db5dc49f2687298a1547bf/xVFi0kRkYud191cQgma16.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:13:41.494Z"
        },
        {
          "_id": "698c133d6052d3bed9630bf4",
          "name": "Shuo He",
          "user": {
            "_id": "641d6099f9a3a9c532bd3954",
            "type": "user",
            "user": "heshuo",
            "isPro": false,
            "fullname": "Shuo He",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641d6099f9a3a9c532bd3954/NBrTueSRKkxhdLvCdZ0lt.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:57:44.843Z"
        },
        {
          "_id": "698c133d6052d3bed9630bf5",
          "name": "Fuxiang Zhang",
          "user": {
            "_id": "64054d8a3d49e1e066bfa32b",
            "type": "user",
            "user": "sicer",
            "isPro": false,
            "fullname": "Fuxiang Zhang",
            "avatarUrl": "/avatars/9044f937145cc5aa4bc3a5ffa751f724.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:13:54.465Z"
        },
        {
          "_id": "698c133d6052d3bed9630bf6",
          "name": "Bo An",
          "hidden": false
        }
      ],
      "published_at": "2026-02-09T16:13:39",
      "upvotes": 18,
      "date": "2026-02-11"
    },
    {
      "id": "2602.00462",
      "title": "LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs",
      "summary": "Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.",
      "authors": [
        {
          "_id": "698b928c6052d3bed963087f",
          "name": "Benno Krojer",
          "user": {
            "_id": "6270c58780d5f35f8dbe42be",
            "type": "user",
            "user": "BennoKrojer",
            "isPro": true,
            "fullname": "Benno Krojer",
            "avatarUrl": "/avatars/d4d6e5eadfe9b1f47bce1c66728b24fc.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:15:40.382Z"
        },
        {
          "_id": "698b928c6052d3bed9630880",
          "name": "Shravan Nayak",
          "hidden": false
        },
        {
          "_id": "698b928c6052d3bed9630881",
          "name": "Oscar Ma\u00f1as",
          "hidden": false
        },
        {
          "_id": "698b928c6052d3bed9630882",
          "name": "Vaibhav Adlakha",
          "hidden": false
        },
        {
          "_id": "698b928c6052d3bed9630883",
          "name": "Desmond Elliott",
          "hidden": false
        },
        {
          "_id": "698b928c6052d3bed9630884",
          "name": "Siva Reddy",
          "hidden": false
        },
        {
          "_id": "698b928c6052d3bed9630885",
          "name": "Marius Mosbach",
          "hidden": false
        }
      ],
      "published_at": "2026-01-31T02:33:07",
      "upvotes": 15,
      "date": "2026-02-11"
    },
    {
      "id": "2602.09849",
      "title": "BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation",
      "summary": "Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning.",
      "authors": [
        {
          "_id": "698bf64c6052d3bed9630ab2",
          "name": "Yucheng Hu",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630ab3",
          "name": "Jianke Zhang",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630ab4",
          "name": "Yuanfei Luo",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630ab5",
          "name": "Yanjiang Guo",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630ab6",
          "name": "Xiaoyu Chen",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630ab7",
          "name": "Xinshu Sun",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630ab8",
          "name": "Kun Feng",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630ab9",
          "name": "Qingzhou Lu",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630aba",
          "name": "Sheng Chen",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630abb",
          "name": "Yangang Zhang",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630abc",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "698bf64c6052d3bed9630abd",
          "name": "Jianyu Chen",
          "hidden": false
        }
      ],
      "published_at": "2026-02-10T14:54:01",
      "upvotes": 15,
      "date": "2026-02-11"
    },
    {
      "id": "2602.01244",
      "title": "Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments",
      "summary": "Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \\emph{Executability}, since each instance requires a suitable and often distinct Docker environment; and \\emph{Verifiability}, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose TerminalTraj, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\\% on TB~1.0 and 10\\% on TB~2.0 over their respective backbones. Notably, TerminalTraj-32B achieves strong performance among models with fewer than 100B parameters, reaching 35.30\\% on TB~1.0 and 22.00\\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj.",
      "authors": [
        {
          "_id": "698c7937eb12ea7453916853",
          "name": "Siwei Wu",
          "hidden": false
        },
        {
          "_id": "698c7937eb12ea7453916854",
          "name": "Yizhi Li",
          "hidden": false
        },
        {
          "_id": "698c7937eb12ea7453916855",
          "name": "Yuyang Song",
          "hidden": false
        },
        {
          "_id": "698c7937eb12ea7453916856",
          "name": "Wei Zhang",
          "hidden": false
        },
        {
          "_id": "698c7937eb12ea7453916857",
          "name": "Yang Wang",
          "hidden": false
        },
        {
          "_id": "698c7937eb12ea7453916858",
          "name": "Riza Batista-Navarro",
          "hidden": false
        },
        {
          "_id": "698c7937eb12ea7453916859",
          "name": "Xian Yang",
          "hidden": false
        },
        {
          "_id": "698c7937eb12ea745391685a",
          "name": "Mingjie Tang",
          "hidden": false
        },
        {
          "_id": "698c7937eb12ea745391685b",
          "name": "Bryan Dai",
          "hidden": false
        },
        {
          "_id": "698c7937eb12ea745391685c",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "698c7937eb12ea745391685d",
          "name": "Chenghua Lin",
          "hidden": false
        }
      ],
      "published_at": "2026-02-01T14:09:23",
      "upvotes": 14,
      "date": "2026-02-11"
    },
    {
      "id": "2602.10098",
      "title": "VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model",
      "summary": "Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is leakage-free state prediction: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.",
      "authors": [
        {
          "_id": "698bf4886052d3bed9630a8b",
          "name": "Jingwen Sun",
          "hidden": false
        },
        {
          "_id": "698bf4886052d3bed9630a8c",
          "name": "Wenyao Zhang",
          "hidden": false
        },
        {
          "_id": "698bf4886052d3bed9630a8d",
          "name": "Zekun Qi",
          "hidden": false
        },
        {
          "_id": "698bf4886052d3bed9630a8e",
          "name": "Shaojie Ren",
          "hidden": false
        },
        {
          "_id": "698bf4886052d3bed9630a8f",
          "name": "Zezhi Liu",
          "hidden": false
        },
        {
          "_id": "698bf4886052d3bed9630a90",
          "name": "Hanxin Zhu",
          "hidden": false
        },
        {
          "_id": "698bf4886052d3bed9630a91",
          "name": "Guangzhong Sun",
          "hidden": false
        },
        {
          "_id": "698bf4886052d3bed9630a92",
          "name": "Xin Jin",
          "hidden": false
        },
        {
          "_id": "698bf4886052d3bed9630a93",
          "name": "Zhibo Chen",
          "hidden": false
        }
      ],
      "published_at": "2026-02-10T18:58:01",
      "upvotes": 14,
      "date": "2026-02-11"
    },
    {
      "id": "2602.09000",
      "title": "iGRPO: Self-Feedback-Driven LLM Reasoning",
      "summary": "Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\\% and 79.64\\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.",
      "authors": [
        {
          "_id": "698c7b50eb12ea74539168dd",
          "name": "Ali Hatamizadeh",
          "hidden": false
        },
        {
          "_id": "698c7b50eb12ea74539168de",
          "name": "Shrimai Prabhumoye",
          "hidden": false
        },
        {
          "_id": "698c7b50eb12ea74539168df",
          "name": "Igor Gitman",
          "hidden": false
        },
        {
          "_id": "698c7b50eb12ea74539168e0",
          "name": "Ximing Lu",
          "hidden": false
        },
        {
          "_id": "698c7b50eb12ea74539168e1",
          "name": "Seungju Han",
          "hidden": false
        },
        {
          "_id": "698c7b50eb12ea74539168e2",
          "name": "Wei Ping",
          "hidden": false
        },
        {
          "_id": "698c7b50eb12ea74539168e3",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "698c7b50eb12ea74539168e4",
          "name": "Jan Kautz",
          "hidden": false
        }
      ],
      "published_at": "2026-02-09T18:45:11",
      "upvotes": 13,
      "date": "2026-02-11"
    }
  ],
  "2026-02-10": [
    {
      "id": "2602.08222",
      "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger",
      "summary": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.",
      "authors": [
        {
          "_id": "698ad20e1b2dc6b37d61b227",
          "name": "Zehao Chen",
          "user": {
            "_id": "64afe1653361f887816da303",
            "type": "user",
            "user": "chhao",
            "isPro": false,
            "fullname": "chenzehao",
            "avatarUrl": "/avatars/320d71adacfad9dd5db064b4ed3dec2b.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:04:42.021Z"
        },
        {
          "_id": "698ad20e1b2dc6b37d61b228",
          "name": "Gongxun Li",
          "user": {
            "_id": "691570a75f0ada3ef3844ae4",
            "type": "user",
            "user": "AlexGeek",
            "isPro": false,
            "fullname": "Gongxun Li",
            "avatarUrl": "/avatars/83d17ae85b92dbb5b7cb734bfafa1caf.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:18:07.950Z"
        },
        {
          "_id": "698ad20e1b2dc6b37d61b229",
          "name": "Tianxiang Ai",
          "hidden": false
        },
        {
          "_id": "698ad20e1b2dc6b37d61b22a",
          "name": "Yifei Li",
          "hidden": false
        },
        {
          "_id": "698ad20e1b2dc6b37d61b22b",
          "name": "Zixuan Huang",
          "hidden": false
        },
        {
          "_id": "698ad20e1b2dc6b37d61b22c",
          "name": "Wang Zhou",
          "hidden": false
        },
        {
          "_id": "698ad20e1b2dc6b37d61b22d",
          "name": "Fuzhen Zhuang",
          "hidden": false
        },
        {
          "_id": "698ad20e1b2dc6b37d61b22e",
          "name": "Xianglong Liu",
          "hidden": false
        },
        {
          "_id": "698ad20e1b2dc6b37d61b22f",
          "name": "Jianxin Li",
          "hidden": false
        },
        {
          "_id": "698ad20e1b2dc6b37d61b230",
          "name": "Deqing Wang",
          "hidden": false
        },
        {
          "_id": "698ad20e1b2dc6b37d61b231",
          "name": "Yikun Ban",
          "user": {
            "_id": "68345345f4bbf856e2d708e2",
            "type": "user",
            "user": "Yikunb",
            "isPro": false,
            "fullname": "Yikun B",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:04:44.617Z"
        }
      ],
      "published_at": "2026-02-09T02:50:40",
      "upvotes": 237,
      "date": "2026-02-10"
    },
    {
      "id": "2602.07085",
      "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining",
      "summary": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.",
      "authors": [
        {
          "_id": "698ab6f91b2dc6b37d61b031",
          "name": "Jun Han",
          "user": {
            "_id": "69674549666228b695202137",
            "type": "user",
            "user": "Junqwef",
            "isPro": false,
            "fullname": "Han Jun",
            "avatarUrl": "/avatars/47ac3b87d395856b3dd1a9f24af82c25.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:19:06.698Z"
        },
        {
          "_id": "698ab6f91b2dc6b37d61b032",
          "name": "Shuo Zhang",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b033",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b034",
          "name": "Zhi Yang",
          "user": {
            "_id": "64aa645404e7b379feccc490",
            "type": "user",
            "user": "yangzhi1",
            "isPro": false,
            "fullname": "Zhi Yang",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:05:58.707Z"
        },
        {
          "_id": "698ab6f91b2dc6b37d61b035",
          "name": "Yifan Dong",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b036",
          "name": "Tu Hu",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b037",
          "name": "Jialuo Yuan",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b038",
          "name": "Xiaomin Yu",
          "user": {
            "_id": "64084fa192033c150738e4f2",
            "type": "user",
            "user": "Yu2020",
            "isPro": false,
            "fullname": "Yu_xm",
            "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:06:00.954Z"
        },
        {
          "_id": "698ab6f91b2dc6b37d61b039",
          "name": "Yumo Zhu",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b03a",
          "name": "Fangqi Lou",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b03b",
          "name": "Xin Guo",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b03c",
          "name": "Zhaowei Liu",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b03d",
          "name": "Tianyi Jiang",
          "user": {
            "_id": "6895e7f146763431aea25ca4",
            "type": "user",
            "user": "LumosJiang",
            "isPro": false,
            "fullname": "Tianyi Jiang",
            "avatarUrl": "/avatars/52e550c3f7e8da2e31b63413e2e71e6c.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:19:04.702Z"
        },
        {
          "_id": "698ab6f91b2dc6b37d61b03e",
          "name": "Ruichuan An",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b03f",
          "name": "Jingping Liu",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b040",
          "name": "Biao Wu",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b041",
          "name": "Rongze Chen",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b042",
          "name": "Kunyi Wang",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b043",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b044",
          "name": "Sen Hu",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b045",
          "name": "Xinbing Kong",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b046",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b047",
          "name": "Ronghao Chen",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b048",
          "name": "Huacan Wang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-06T08:08:04",
      "upvotes": 178,
      "date": "2026-02-10"
    },
    {
      "id": "2602.08794",
      "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation",
      "summary": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.",
      "authors": [
        {
          "_id": "698ac65d1b2dc6b37d61b1c2",
          "name": "SII-OpenMOSS Team",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1c4",
          "name": "Donghua Yu",
          "user": {
            "_id": "630501ee34c824b17250dea3",
            "type": "user",
            "user": "yhzx233",
            "isPro": false,
            "fullname": "Donghua Yu",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630501ee34c824b17250dea3/1muf-A-SvXYzr9yjXi1Ev.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:18:12.256Z"
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1c5",
          "name": "Mingshu Chen",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1c6",
          "name": "Qi Chen",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1c7",
          "name": "Qi Luo",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1c8",
          "name": "Qianyi Wu",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1c9",
          "name": "Qinyuan Cheng",
          "user": {
            "_id": "63ec4715c81b6a52391c46b8",
            "type": "user",
            "user": "Cqy2019",
            "isPro": false,
            "fullname": "QinyuanCheng",
            "avatarUrl": "/avatars/496819b5075a1a834a2b9edeb068c80e.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:05:07.400Z"
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1ca",
          "name": "Ruixiao Li",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1cb",
          "name": "Tianyi Liang",
          "user": {
            "_id": "62c14609ac1b639c2d87192c",
            "type": "user",
            "user": "tianyilt",
            "isPro": false,
            "fullname": "SII-liangtianyi",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:05:10.522Z"
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1cc",
          "name": "Wenbo Zhang",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1cd",
          "name": "Wenming Tu",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1ce",
          "name": "Xiangyu Peng",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1cf",
          "name": "Yang Gao",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d0",
          "name": "Yanru Huo",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d1",
          "name": "Ying Zhu",
          "user": {
            "_id": "69158ffc0153b85a677dcc46",
            "type": "user",
            "user": "Auraithm",
            "isPro": false,
            "fullname": "Ying Zhu",
            "avatarUrl": "/avatars/c9c5f60522f2a8f370d790ea9938b090.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:27:41.440Z"
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d2",
          "name": "Yinze Luo",
          "user": {
            "_id": "6809a215d1b1e0758d74142d",
            "type": "user",
            "user": "0-693",
            "isPro": false,
            "fullname": "Luo Yinze",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/HQaINIuC0nd4Xa5_3_ma9.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T13:57:54.543Z"
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d3",
          "name": "Yiyang Zhang",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d4",
          "name": "Yuerong Song",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d5",
          "name": "Zhe Xu",
          "user": {
            "_id": "6443f7bf1bc692d87b25e234",
            "type": "user",
            "user": "Phospheneser",
            "isPro": false,
            "fullname": "Xu Zhe",
            "avatarUrl": "/avatars/fa9e62d96d0691a9a48e3db499a61557.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:18:14.300Z"
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d6",
          "name": "Zhiyu Zhang",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d7",
          "name": "Chenchen Yang",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d8",
          "name": "Cheng Chang",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d9",
          "name": "Chushu Zhou",
          "user": {
            "_id": "6576b137a90ae2daae171245",
            "type": "user",
            "user": "zhouchushu",
            "isPro": false,
            "fullname": "zhouchushu(SII)",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6576b137a90ae2daae171245/WQK4UyNDX1XxIK83GpOZB.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:18:18.938Z"
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1da",
          "name": "Hanfu Chen",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1db",
          "name": "Hongnan Ma",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1dc",
          "name": "Jiaxi Li",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1dd",
          "name": "Jingqi Tong",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1de",
          "name": "Junxi Liu",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1df",
          "name": "Ke Chen",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e0",
          "name": "Shimin Li",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e1",
          "name": "Songlin Wang",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e2",
          "name": "Wei Jiang",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e3",
          "name": "Zhaoye Fei",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e4",
          "name": "Zhiyuan Ning",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e5",
          "name": "Chunguo Li",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e6",
          "name": "Chenhui Li",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e7",
          "name": "Ziwei He",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e8",
          "name": "Zengfeng Huang",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e9",
          "name": "Xie Chen",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1ea",
          "name": "Xipeng Qiu",
          "user": {
            "_id": "61457b8deff2c9fdb4de4988",
            "type": "user",
            "user": "xpqiu",
            "isPro": false,
            "fullname": "Xipeng Qiu",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:18:10.009Z"
        }
      ],
      "published_at": "2026-02-09T15:31:54",
      "upvotes": 145,
      "date": "2026-02-10"
    },
    {
      "id": "2602.07026",
      "title": "Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models",
      "summary": "Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.",
      "authors": [
        {
          "_id": "698a98541b2dc6b37d61af09",
          "name": "Xiaomin Yu",
          "user": {
            "_id": "64084fa192033c150738e4f2",
            "type": "user",
            "user": "Yu2020",
            "isPro": false,
            "fullname": "Yu_xm",
            "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:07:26.323Z"
        },
        {
          "_id": "698a98541b2dc6b37d61af0a",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af0b",
          "name": "Wenjie Zhang",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af0c",
          "name": "Chonghan Liu",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af0d",
          "name": "Hanzhen Zhao",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af0e",
          "name": "Xiaoxing Hu",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af0f",
          "name": "Xinlei Yu",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af10",
          "name": "Ziyue Qiao",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af11",
          "name": "Hao Tang",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af12",
          "name": "Xue Yang",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af13",
          "name": "Xiaobin Hu",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af14",
          "name": "Chengwei Qin",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af15",
          "name": "Hui Xiong",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af16",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af17",
          "name": "Shuicheng Yan",
          "hidden": false
        }
      ],
      "published_at": "2026-02-02T13:59:39",
      "upvotes": 129,
      "date": "2026-02-10"
    },
    {
      "id": "2602.06855",
      "title": "AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents",
      "summary": "LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.",
      "authors": [
        {
          "_id": "698b0ed21b2dc6b37d61b3d0",
          "name": "Alisia Lupidi",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3d1",
          "name": "Bhavul Gauri",
          "user": {
            "_id": "60720704227ff331937110f4",
            "type": "user",
            "user": "bhavul",
            "isPro": false,
            "fullname": "Bhavul Gauri",
            "avatarUrl": "/avatars/8010bfb98256c138049aa3d237737b37.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:17:30.748Z"
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3d2",
          "name": "Thomas Simon Foster",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3d3",
          "name": "Bassel Al Omari",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3d4",
          "name": "Despoina Magka",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3d5",
          "name": "Alberto Pepe",
          "user": {
            "_id": "6932cd2194556b358ec2c640",
            "type": "user",
            "user": "albertompepe",
            "isPro": false,
            "fullname": "Alberto Pepe",
            "avatarUrl": "/avatars/da9703be57b9c62c4737b591b7695bd1.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T12:34:25.082Z"
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3d6",
          "name": "Alexis Audran-Reiss",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3d7",
          "name": "Muna Aghamelu",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3d8",
          "name": "Nicolas Baldwin",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3d9",
          "name": "Lucia Cipolina-Kun",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3da",
          "name": "Jean-Christophe Gagnon-Audet",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3db",
          "name": "Chee Hau Leow",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3dc",
          "name": "Sandra Lefdal",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3dd",
          "name": "Hossam Mossalam",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3de",
          "name": "Abhinav Moudgil",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3df",
          "name": "Saba Nazir",
          "user": {
            "_id": "697748f611a42e0efd3bddcc",
            "type": "user",
            "user": "snazir",
            "isPro": false,
            "fullname": "Saba Nazir",
            "avatarUrl": "/avatars/d591182b0e55f125691f6031ba395241.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:17:23.767Z"
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3e0",
          "name": "Emanuel Tewolde",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3e1",
          "name": "Isabel Urrego",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3e2",
          "name": "Jordi Armengol Estape",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3e3",
          "name": "Amar Budhiraja",
          "user": {
            "_id": "6687ee79eee600e418404cc9",
            "type": "user",
            "user": "ambud26",
            "isPro": false,
            "fullname": "Amar Budhiraja",
            "avatarUrl": "/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T22:16:57.977Z"
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3e4",
          "name": "Gaurav Chaurasia",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3e5",
          "name": "Abhishek Charnalia",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3e6",
          "name": "Derek Dunfield",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3e7",
          "name": "Karen Hambardzumyan",
          "user": {
            "_id": "63a2fb1cdca41424cb8956bd",
            "type": "user",
            "user": "mahnerak",
            "isPro": false,
            "fullname": "Karen Hambardzumyan",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a2fb1cdca41424cb8956bd/9BDkkHXPifN-_czXBAecF.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:17:21.100Z"
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3e8",
          "name": "Daniel Izcovich",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3e9",
          "name": "Martin Josifoski",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3ea",
          "name": "Ishita Mediratta",
          "user": {
            "_id": "67f2839f0d449a5c10d4b67a",
            "type": "user",
            "user": "ishmed",
            "isPro": false,
            "fullname": "Ish Med",
            "avatarUrl": "/avatars/9fd9b7abb9da1ef68eec461d703cac77.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:17:28.660Z"
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3eb",
          "name": "Kelvin Niu",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3ec",
          "name": "Parth Pathak",
          "user": {
            "_id": "68d66f1a49f157c15451b3c1",
            "type": "user",
            "user": "papatha",
            "isPro": false,
            "fullname": "Pathak",
            "avatarUrl": "/avatars/4ea87b124e07646941b55c6a338561d7.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:17:26.106Z"
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3ed",
          "name": "Michael Shvartsman",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3ee",
          "name": "Edan Toledo",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3ef",
          "name": "Anton Protopopov",
          "user": {
            "_id": "67f00bf17530c3fccbb26c79",
            "type": "user",
            "user": "aprotopopov",
            "isPro": false,
            "fullname": "Anton Protopopov",
            "avatarUrl": "/avatars/f0d56f04b1def33dce872a8de71f560d.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-12T20:23:30.082Z"
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3f0",
          "name": "Roberta Raileanu",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3f1",
          "name": "Alexander Miller",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3f2",
          "name": "Tatiana Shavrina",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3f3",
          "name": "Jakob Foerster",
          "hidden": false
        },
        {
          "_id": "698b0ed21b2dc6b37d61b3f4",
          "name": "Yoram Bachrach",
          "hidden": false
        }
      ],
      "published_at": "2026-02-06T16:45:02",
      "upvotes": 67,
      "date": "2026-02-10"
    },
    {
      "id": "2602.07845",
      "title": "Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning",
      "summary": "Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/",
      "authors": [
        {
          "_id": "698ab2ef1b2dc6b37d61af7b",
          "name": "Yalcin Tur",
          "hidden": false
        },
        {
          "_id": "698ab2ef1b2dc6b37d61af7c",
          "name": "Jalal Naghiyev",
          "hidden": false
        },
        {
          "_id": "698ab2ef1b2dc6b37d61af7d",
          "name": "Haoquan Fang",
          "hidden": false
        },
        {
          "_id": "698ab2ef1b2dc6b37d61af7e",
          "name": "Wei-Chuan Tsai",
          "hidden": false
        },
        {
          "_id": "698ab2ef1b2dc6b37d61af7f",
          "name": "Jiafei Duan",
          "hidden": false
        },
        {
          "_id": "698ab2ef1b2dc6b37d61af80",
          "name": "Dieter Fox",
          "hidden": false
        },
        {
          "_id": "698ab2ef1b2dc6b37d61af81",
          "name": "Ranjay Krishna",
          "hidden": false
        }
      ],
      "published_at": "2026-02-08T07:21:01",
      "upvotes": 65,
      "date": "2026-02-10"
    },
    {
      "id": "2602.08990",
      "title": "InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery",
      "summary": "We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.",
      "authors": [
        {
          "_id": "698abe481b2dc6b37d61b12c",
          "name": "Shiyang Feng",
          "user": {
            "_id": "667cf204268f6622dac71961",
            "type": "user",
            "user": "sY713",
            "isPro": false,
            "fullname": "shiyang",
            "avatarUrl": "/avatars/90e1928beb2a685e82e19758e4a6b7ae.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:05:21.332Z"
        },
        {
          "_id": "698abe481b2dc6b37d61b12d",
          "name": "Runmin Ma",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b12e",
          "name": "Xiangchao Yan",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b12f",
          "name": "Yue Fan",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b130",
          "name": "Yusong Hu",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b131",
          "name": "Songtao Huang",
          "user": {
            "_id": "67cfd5f4d8cb8688d7e2df22",
            "type": "user",
            "user": "huangst",
            "isPro": false,
            "fullname": "Songtao Huang",
            "avatarUrl": "/avatars/099139aac6d803fa47579a1152da39ef.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:05:17.495Z"
        },
        {
          "_id": "698abe481b2dc6b37d61b132",
          "name": "Shuaiyu Zhang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b133",
          "name": "Zongsheng Cao",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b134",
          "name": "Tianshuo Peng",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b135",
          "name": "Jiakang Yuan",
          "user": {
            "_id": "64a3d1ddb3239f3e3892b24b",
            "type": "user",
            "user": "JiakangYuan",
            "isPro": false,
            "fullname": "Jiakang Yuan",
            "avatarUrl": "/avatars/7ce585f5fc1d077fb1d70cc18c4da2c1.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:05:19.387Z"
        },
        {
          "_id": "698abe481b2dc6b37d61b136",
          "name": "Zijie Guo",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b137",
          "name": "Zhijie Zhong",
          "user": {
            "_id": "68ce4bf9063c34f5765ecee8",
            "type": "user",
            "user": "WAboutme",
            "isPro": false,
            "fullname": "SII-Xiaofei Wei",
            "avatarUrl": "/avatars/514a0914473bc7f8812e9c30ea27f0d5.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:18:20.999Z"
        },
        {
          "_id": "698abe481b2dc6b37d61b138",
          "name": "Shangheng Du",
          "user": {
            "_id": "6862469307eb6d925b99dedc",
            "type": "user",
            "user": "shanghengdu",
            "isPro": false,
            "fullname": "Shangheng Du",
            "avatarUrl": "/avatars/5ecc7fe31e899c93e6491e1a527d5d2d.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:18:25.159Z"
        },
        {
          "_id": "698abe481b2dc6b37d61b139",
          "name": "Weida Wang",
          "user": {
            "_id": "661b9d96c153e4a0a25adc3e",
            "type": "user",
            "user": "weidawang",
            "isPro": false,
            "fullname": "Weida Wang",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661b9d96c153e4a0a25adc3e/VRt7kCQ0KdJp-lhPLOajO.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:18:27.134Z"
        },
        {
          "_id": "698abe481b2dc6b37d61b13a",
          "name": "Jinxin Shi",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b13b",
          "name": "Yuhao Zhou",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b13c",
          "name": "Xiaohan He",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b13d",
          "name": "Zhiyin Yu",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b13e",
          "name": "Fangchen Yu",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b13f",
          "name": "Qihao Zheng",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b140",
          "name": "Jiamin Wu",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b141",
          "name": "Mianxin Liu",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b142",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b143",
          "name": "Shaowei Hou",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b144",
          "name": "Shuya Li",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b145",
          "name": "Yankai Jiang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b146",
          "name": "Wenjie Lou",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b147",
          "name": "Lilong Wang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b148",
          "name": "Zifu Wang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b149",
          "name": "Jiong Wang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b14a",
          "name": "Wanghan Xu",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b14b",
          "name": "Yue Deng",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b14c",
          "name": "Dongrui Liu",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b14d",
          "name": "Yiheng Wang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b14e",
          "name": "Wenlong Zhang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b14f",
          "name": "Fenghua Ling",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b150",
          "name": "Shufei Zhang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b151",
          "name": "Xiaosong Wang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b152",
          "name": "Shuangjia Zheng",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b153",
          "name": "Xun Huang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b154",
          "name": "Siqi Sun",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b155",
          "name": "Shuyue Hu",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b156",
          "name": "Peng Ye",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b157",
          "name": "Chunfeng Song",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b158",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b159",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b15a",
          "name": "Yihao Liu",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b15b",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b15c",
          "name": "Qibin Hou",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b15d",
          "name": "Tao Chen",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b15e",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b15f",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b160",
          "name": "Liang He",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b161",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b162",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b163",
          "name": "Bo Zhang",
          "user": {
            "_id": "643dfd235aafbdca3a5792c0",
            "type": "user",
            "user": "BoZhang",
            "isPro": false,
            "fullname": "Bo Zhang",
            "avatarUrl": "/avatars/ce8553cf5936012c692e08054ee27937.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:18:23.044Z"
        },
        {
          "_id": "698abe481b2dc6b37d61b164",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "published_at": "2026-02-09T18:36:06",
      "upvotes": 62,
      "date": "2026-02-10"
    },
    {
      "id": "2602.08676",
      "title": "LLaDA2.1: Speeding Up Text Diffusion via Token Editing",
      "summary": "While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.",
      "authors": [
        {
          "_id": "698ab6fd1b2dc6b37d61b04b",
          "name": "Tiwei Bie",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b04c",
          "name": "Maosong Cao",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b04d",
          "name": "Xiang Cao",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b04e",
          "name": "Bingsen Chen",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b04f",
          "name": "Fuyuan Chen",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b050",
          "name": "Kun Chen",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b051",
          "name": "Lun Du",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b052",
          "name": "Daozhuo Feng",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b053",
          "name": "Haibo Feng",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b054",
          "name": "Mingliang Gong",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b055",
          "name": "Zhuocheng Gong",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b056",
          "name": "Yanmei Gu",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b057",
          "name": "Jian Guan",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b058",
          "name": "Kaiyuan Guan",
          "user": {
            "_id": "6494f6dc32f2c0d7ef28315c",
            "type": "user",
            "user": "WilfredG",
            "isPro": false,
            "fullname": "Kaiyuan Guan",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6494f6dc32f2c0d7ef28315c/y4TwcrT6UQ9Zfy6BfUMUl.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:05:52.604Z"
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b059",
          "name": "Hongliang He",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b05a",
          "name": "Zenan Huang",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b05b",
          "name": "Juyong Jiang",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b05c",
          "name": "Zhonghui Jiang",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b05d",
          "name": "Zhenzhong Lan",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b05e",
          "name": "Chengxi Li",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b05f",
          "name": "Jianguo Li",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b060",
          "name": "Zehuan Li",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b061",
          "name": "Huabin Liu",
          "user": {
            "_id": "6470600790482b0e0f65a393",
            "type": "user",
            "user": "Rookie-Liu",
            "isPro": false,
            "fullname": "Huabin",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fOUWWhf9jjsC9fgIiFlPc.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:05:42.985Z"
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b062",
          "name": "Lin Liu",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b063",
          "name": "Guoshan Lu",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b064",
          "name": "Yuan Lu",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b065",
          "name": "Yuxin Ma",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b066",
          "name": "Xingyu Mou",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b067",
          "name": "Zhenxuan Pan",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b068",
          "name": "Kaida Qiu",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b069",
          "name": "Yuji Ren",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b06a",
          "name": "Jianfeng Tan",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b06b",
          "name": "Yiding Tian",
          "user": {
            "_id": "5f94dea4cf95e81b6854e223",
            "type": "user",
            "user": "killa1218",
            "isPro": false,
            "fullname": "Dean Tian",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f94dea4cf95e81b6854e223/r3VW8rb0wlJ7t_Lcpqi5f.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:05:47.064Z"
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b06c",
          "name": "Zian Wang",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b06d",
          "name": "Lanning Wei",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b06e",
          "name": "Tao Wu",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b06f",
          "name": "Yipeng Xing",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b070",
          "name": "Wentao Ye",
          "user": {
            "_id": "65a97147cb5b4fb08e716afd",
            "type": "user",
            "user": "darklight03",
            "isPro": false,
            "fullname": "Wentao Ye",
            "avatarUrl": "/avatars/7ddd4eb85d472cf42f71a3c34d659582.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:19:02.750Z"
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b071",
          "name": "Liangyu Zha",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b072",
          "name": "Tianze Zhang",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b073",
          "name": "Xiaolu Zhang",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b074",
          "name": "Junbo Zhao",
          "user": {
            "_id": "6725f5a7f05f62659e3615f3",
            "type": "user",
            "user": "jakezhao2024",
            "isPro": false,
            "fullname": "Junbo Zhao",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/iiXdzhaSnExw2dwQQtLZ8.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:05:45.032Z"
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b075",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b076",
          "name": "Hao Zhong",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b077",
          "name": "Wanli Zhong",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b078",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b079",
          "name": "Junlin Zhou",
          "user": {
            "_id": "63eb008e5c837d9968f1eb71",
            "type": "user",
            "user": "jlzhou",
            "isPro": false,
            "fullname": "Junlin Zhou",
            "avatarUrl": "/avatars/ae43c3f5ab87b82f4bad25c65ac55d01.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:05:49.495Z"
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b07a",
          "name": "Liwang Zhu",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b07b",
          "name": "Muzhi Zhu",
          "user": {
            "_id": "632179745fc60c44fd91fc33",
            "type": "user",
            "user": "Z-MU-Z",
            "isPro": false,
            "fullname": "zhumuzhi",
            "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:05:33.817Z"
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b07c",
          "name": "Yihong Zhuang",
          "user": {
            "_id": "673b5f24e863f1d28b402efc",
            "type": "user",
            "user": "utdawn",
            "isPro": false,
            "fullname": "yihongzhuang",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/19gUgtPEY3-FtY0sNlI_-.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:05:55.143Z"
        }
      ],
      "published_at": "2026-02-09T14:00:07",
      "upvotes": 58,
      "date": "2026-02-10"
    },
    {
      "id": "2602.07837",
      "title": "RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI",
      "summary": "Online policy learning directly in the physical world is a promising yet challenging direction for embodied intelligence. Unlike simulation, real-world systems cannot be arbitrarily accelerated, cheaply reset, or massively replicated, which makes scalable data collection, heterogeneous deployment, and long-horizon effective training difficult. These challenges suggest that real-world policy learning is not only an algorithmic issue but fundamentally a systems problem. We present USER, a Unified and extensible SystEm for Real-world online policy learning. USER treats physical robots as first-class hardware resources alongside GPUs through a unified hardware abstraction layer, enabling automatic discovery, management, and scheduling of heterogeneous robots. To address cloud-edge communication, USER introduces an adaptive communication plane with tunneling-based networking, distributed data channels for traffic localization, and streaming-multiprocessor-aware weight synchronization to regulate GPU-side overhead. On top of this infrastructure, USER organizes learning as a fully asynchronous framework with a persistent, cache-aware buffer, enabling efficient long-horizon experiments with robust crash recovery and reuse of historical data. In addition, USER provides extensible abstractions for rewards, algorithms, and policies, supporting online imitation or reinforcement learning of CNN/MLP, generative policies, and large vision-language-action (VLA) models within a unified pipeline. Results in both simulation and the real world show that USER enables multi-robot coordination, heterogeneous manipulators, edge-cloud collaboration with large models, and long-running asynchronous training, offering a unified and extensible systems foundation for real-world online policy learning.",
      "authors": [
        {
          "_id": "698ab6bc1b2dc6b37d61b010",
          "name": "Hongzhi Zang",
          "hidden": false
        },
        {
          "_id": "698ab6bc1b2dc6b37d61b011",
          "name": "Shu'ang Yu",
          "user": {
            "_id": "683fe1d7f28c4d5a999d36f1",
            "type": "user",
            "user": "ysa2001",
            "isPro": false,
            "fullname": "Shu'ang Yu",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/c6RPM9spqBI1FBRU6xQVG.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:19:08.606Z"
        },
        {
          "_id": "698ab6bc1b2dc6b37d61b012",
          "name": "Hao Lin",
          "hidden": false
        },
        {
          "_id": "698ab6bc1b2dc6b37d61b013",
          "name": "Tianxing Zhou",
          "hidden": false
        },
        {
          "_id": "698ab6bc1b2dc6b37d61b014",
          "name": "Zefang Huang",
          "hidden": false
        },
        {
          "_id": "698ab6bc1b2dc6b37d61b015",
          "name": "Zhen Guo",
          "hidden": false
        },
        {
          "_id": "698ab6bc1b2dc6b37d61b016",
          "name": "Xin Xu",
          "hidden": false
        },
        {
          "_id": "698ab6bc1b2dc6b37d61b017",
          "name": "Jiakai Zhou",
          "hidden": false
        },
        {
          "_id": "698ab6bc1b2dc6b37d61b018",
          "name": "Yuze Sheng",
          "hidden": false
        },
        {
          "_id": "698ab6bc1b2dc6b37d61b019",
          "name": "Shizhe Zhang",
          "hidden": false
        },
        {
          "_id": "698ab6bc1b2dc6b37d61b01a",
          "name": "Feng Gao",
          "hidden": false
        },
        {
          "_id": "698ab6bc1b2dc6b37d61b01b",
          "name": "Wenhao Tang",
          "hidden": false
        },
        {
          "_id": "698ab6bc1b2dc6b37d61b01c",
          "name": "Yufeng Yue",
          "hidden": false
        },
        {
          "_id": "698ab6bc1b2dc6b37d61b01d",
          "name": "Quanlu Zhang",
          "hidden": false
        },
        {
          "_id": "698ab6bc1b2dc6b37d61b01e",
          "name": "Xinlei Chen",
          "hidden": false
        },
        {
          "_id": "698ab6bc1b2dc6b37d61b01f",
          "name": "Chao Yu",
          "hidden": false
        },
        {
          "_id": "698ab6bc1b2dc6b37d61b020",
          "name": "Yu Wang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-08T06:23:43",
      "upvotes": 50,
      "date": "2026-02-10"
    },
    {
      "id": "2602.00169",
      "title": "Towards Agentic Intelligence for Materials Science",
      "summary": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment.\n  To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials.",
      "authors": [
        {
          "_id": "698b5df76052d3bed9630816",
          "name": "Huan Zhang",
          "hidden": false
        },
        {
          "_id": "698b5df76052d3bed9630817",
          "name": "Yizhan Li",
          "hidden": false
        },
        {
          "_id": "698b5df76052d3bed9630818",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "698b5df76052d3bed9630819",
          "name": "Ziyu Hou",
          "hidden": false
        },
        {
          "_id": "698b5df76052d3bed963081a",
          "name": "Yu Song",
          "hidden": false
        },
        {
          "_id": "698b5df76052d3bed963081b",
          "name": "Xuye Liu",
          "user": {
            "_id": "60845b6ca5da133ac6c38681",
            "type": "user",
            "user": "xuyeliu123",
            "isPro": false,
            "fullname": "Xuye Liu",
            "avatarUrl": "/avatars/01dfcf615a57c37ff19276d79f423cf1.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:15:56.294Z"
        },
        {
          "_id": "698b5df76052d3bed963081c",
          "name": "Farshid Effaty",
          "hidden": false
        },
        {
          "_id": "698b5df76052d3bed963081d",
          "name": "Jinya Jiang",
          "user": {
            "_id": "645605624cfac492278dcd9c",
            "type": "user",
            "user": "j9jiang",
            "isPro": false,
            "fullname": "Yaya Jiang",
            "avatarUrl": "/avatars/0c8c19fa3c58f05288bea47757548db5.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:15:58.893Z"
        },
        {
          "_id": "698b5df76052d3bed963081e",
          "name": "Sifan Wu",
          "hidden": false
        },
        {
          "_id": "698b5df76052d3bed963081f",
          "name": "Qianggang Ding",
          "hidden": false
        },
        {
          "_id": "698b5df76052d3bed9630820",
          "name": "Izumi Takahara",
          "hidden": false
        },
        {
          "_id": "698b5df76052d3bed9630821",
          "name": "Leonard R. MacGillivray",
          "hidden": false
        },
        {
          "_id": "698b5df76052d3bed9630822",
          "name": "Teruyasu Mizoguchi",
          "hidden": false
        },
        {
          "_id": "698b5df76052d3bed9630823",
          "name": "Tianshu Yu",
          "hidden": false
        },
        {
          "_id": "698b5df76052d3bed9630824",
          "name": "Lizi Liao",
          "hidden": false
        },
        {
          "_id": "698b5df76052d3bed9630825",
          "name": "Yuyu Luo",
          "hidden": false
        },
        {
          "_id": "698b5df76052d3bed9630826",
          "name": "Yu Rong",
          "hidden": false
        },
        {
          "_id": "698b5df76052d3bed9630827",
          "name": "Jia Li",
          "hidden": false
        },
        {
          "_id": "698b5df76052d3bed9630828",
          "name": "Ying Diao",
          "hidden": false
        },
        {
          "_id": "698b5df76052d3bed9630829",
          "name": "Heng Ji",
          "hidden": false
        },
        {
          "_id": "698b5df76052d3bed963082a",
          "name": "Bang Liu",
          "hidden": false
        }
      ],
      "published_at": "2026-01-29T23:48:43",
      "upvotes": 45,
      "date": "2026-02-10"
    },
    {
      "id": "2602.06422",
      "title": "Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO",
      "summary": "Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's \"pure\" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.",
      "authors": [
        {
          "_id": "698a9c501b2dc6b37d61af2f",
          "name": "Yunze Tong",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af30",
          "name": "Mushui Liu",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af31",
          "name": "Canyu Zhao",
          "user": {
            "_id": "646efd223dd912a539e0bd46",
            "type": "user",
            "user": "Canyu",
            "isPro": false,
            "fullname": "Canyu Zhao",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:06:46.484Z"
        },
        {
          "_id": "698a9c501b2dc6b37d61af32",
          "name": "Wanggui He",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af33",
          "name": "Shiyi Zhang",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af34",
          "name": "Hongwei Zhang",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af35",
          "name": "Peng Zhang",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af36",
          "name": "Jinlong Liu",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af37",
          "name": "Ju Huang",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af38",
          "name": "Jiamang Wang",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af39",
          "name": "Hao Jiang",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af3a",
          "name": "Pipei Huang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-06T06:37:10",
      "upvotes": 41,
      "date": "2026-02-10"
    },
    {
      "id": "2602.08321",
      "title": "Improving Data and Reward Design for Scientific Reasoning in Large Language Models",
      "summary": "Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr. SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.",
      "authors": [
        {
          "_id": "698be5636052d3bed963099b",
          "name": "Zijie Chen",
          "hidden": false
        },
        {
          "_id": "698be5636052d3bed963099c",
          "name": "Zhenghao Lin",
          "hidden": false
        },
        {
          "_id": "698be5636052d3bed963099d",
          "name": "Xiao Liu",
          "user": {
            "_id": "63fb6e281b4b1bd4e7ffc5be",
            "type": "user",
            "user": "lx865712528",
            "isPro": false,
            "fullname": "Xiao Liu",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fb6e281b4b1bd4e7ffc5be/aiRu_bulgnxvEMrjipGoQ.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:15:18.917Z"
        },
        {
          "_id": "698be5636052d3bed963099e",
          "name": "Zhenzhong Lan",
          "hidden": false
        },
        {
          "_id": "698be5636052d3bed963099f",
          "name": "Yeyun Gong",
          "hidden": false
        },
        {
          "_id": "698be5636052d3bed96309a0",
          "name": "Peng Cheng",
          "hidden": false
        }
      ],
      "published_at": "2026-02-09T06:52:03",
      "upvotes": 38,
      "date": "2026-02-10"
    },
    {
      "id": "2602.09007",
      "title": "GEBench: Benchmarking Image Generation Models as GUI Environments",
      "summary": "Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.",
      "authors": [
        {
          "_id": "698ad8ac1b2dc6b37d61b275",
          "name": "Haodong Li",
          "user": {
            "_id": "65ddea8b2d26e59a5a33330f",
            "type": "user",
            "user": "mickyhimself",
            "isPro": false,
            "fullname": "li haodong",
            "avatarUrl": "/avatars/3104ddafd6dda3c05ea9a771dbf2deeb.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:28:30.775Z"
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b276",
          "name": "Jingwei Wu",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b277",
          "name": "Quan Sun",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b278",
          "name": "Guopeng Li",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b279",
          "name": "Juanxi Tian",
          "user": {
            "_id": "670880950e79a8b46f7ff9dd",
            "type": "user",
            "user": "Juanxi",
            "isPro": false,
            "fullname": "Juanxi Tian",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:04:13.048Z"
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b27a",
          "name": "Huanyu Zhang",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b27b",
          "name": "Yanlin Lai",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b27c",
          "name": "Ruichuan An",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b27d",
          "name": "Hongbo Peng",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b27e",
          "name": "Yuhong Dai",
          "user": {
            "_id": "65d70e775e971572da16c05b",
            "type": "user",
            "user": "BroAlanTaps",
            "isPro": false,
            "fullname": "YuHong Dai",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d70e775e971572da16c05b/8Cv71Clfk_C7k6U4yI6ln.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:04:30.092Z"
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b27f",
          "name": "Chenxi Li",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b280",
          "name": "Chunmei Qing",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b281",
          "name": "Jia Wang",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b282",
          "name": "Ziyang Meng",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b283",
          "name": "Zheng Ge",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b284",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b285",
          "name": "Daxin Jiang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-09T18:52:02",
      "upvotes": 37,
      "date": "2026-02-10"
    },
    {
      "id": "2602.08439",
      "title": "Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition",
      "summary": "Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.",
      "authors": [
        {
          "_id": "698ab3e51b2dc6b37d61afaa",
          "name": "Yuhao Dong",
          "user": {
            "_id": "652965773a416e1f2173443b",
            "type": "user",
            "user": "THUdyh",
            "isPro": true,
            "fullname": "Yuhao Dong",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:06:16.048Z"
        },
        {
          "_id": "698ab3e51b2dc6b37d61afab",
          "name": "Shulin Tian",
          "hidden": false
        },
        {
          "_id": "698ab3e51b2dc6b37d61afac",
          "name": "Shuai Liu",
          "hidden": false
        },
        {
          "_id": "698ab3e51b2dc6b37d61afad",
          "name": "Shuangrui Ding",
          "hidden": false
        },
        {
          "_id": "698ab3e51b2dc6b37d61afae",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "698ab3e51b2dc6b37d61afaf",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "698ab3e51b2dc6b37d61afb0",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "698ab3e51b2dc6b37d61afb1",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "698ab3e51b2dc6b37d61afb2",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "published_at": "2026-02-09T09:51:29",
      "upvotes": 29,
      "date": "2026-02-10"
    },
    {
      "id": "2602.06025",
      "title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory",
      "summary": "Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present BudgetMem, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., Low/Mid/High). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.",
      "authors": [
        {
          "_id": "698608f09c78be977c104b8c",
          "name": "Haozhen Zhang",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b8d",
          "name": "Haodong Yue",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b8e",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b8f",
          "name": "Quanyu Long",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b90",
          "name": "Jianzhu Bao",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b91",
          "name": "Bowen Jin",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b92",
          "name": "Weizhi Zhang",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b93",
          "name": "Xiao Li",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b94",
          "name": "Jiaxuan You",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b95",
          "name": "Chengwei Qin",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b96",
          "name": "Wenya Wang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-05T18:57:09",
      "upvotes": 27,
      "date": "2026-02-10"
    },
    {
      "id": "2602.07962",
      "title": "LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth",
      "summary": "Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as \"context rot\". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench",
      "authors": [
        {
          "_id": "698a9f631b2dc6b37d61af47",
          "name": "Weihao Zeng",
          "user": {
            "_id": "62751082b43ccfeef483424f",
            "type": "user",
            "user": "AndrewZeng",
            "isPro": false,
            "fullname": "WeihaoZeng",
            "avatarUrl": "/avatars/fec83e4478e7d1731ba6033328131852.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:19:10.951Z"
        },
        {
          "_id": "698a9f631b2dc6b37d61af48",
          "name": "Yuzhen Huang",
          "user": {
            "_id": "6462def82a83863b97c0611e",
            "type": "user",
            "user": "yuzhen17",
            "isPro": false,
            "fullname": "Yuzhen Huang",
            "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:06:27.902Z"
        },
        {
          "_id": "698a9f631b2dc6b37d61af49",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "published_at": "2026-02-08T13:20:39",
      "upvotes": 24,
      "date": "2026-02-10"
    },
    {
      "id": "2602.08543",
      "title": "GISA: A Benchmark for General Information-Seeking Assistant",
      "summary": "The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.",
      "authors": [
        {
          "_id": "698abf921b2dc6b37d61b16a",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b16b",
          "name": "Xingshuo Zhang",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b16c",
          "name": "Maosen Zhang",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b16d",
          "name": "Jiajie Jin",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b16e",
          "name": "Liancheng Zhang",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b16f",
          "name": "Xiaoshuai Song",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b170",
          "name": "Kangzhi Zhao",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b171",
          "name": "Wencong Zeng",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b172",
          "name": "Ruiming Tang",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b173",
          "name": "Han Li",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b174",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b175",
          "name": "Zhicheng Dou",
          "user": {
            "_id": "66f0bf59e9d50ec57febf751",
            "type": "user",
            "user": "douzc",
            "isPro": false,
            "fullname": "Zhicheng Dou",
            "avatarUrl": "/avatars/be97941e60064e5dd806c6fe9db3c537.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:05:14.771Z"
        }
      ],
      "published_at": "2026-02-09T11:44:15",
      "upvotes": 23,
      "date": "2026-02-10"
    },
    {
      "id": "2602.07274",
      "title": "TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents",
      "summary": "Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch, leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories. Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop. Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles. Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench. This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/terminal-bench-env.",
      "authors": [
        {
          "_id": "698abbae1b2dc6b37d61b0f3",
          "name": "Kaijie Zhu",
          "hidden": false
        },
        {
          "_id": "698abbae1b2dc6b37d61b0f4",
          "name": "Yuzhou Nie",
          "hidden": false
        },
        {
          "_id": "698abbae1b2dc6b37d61b0f5",
          "name": "Yijiang Li",
          "user": {
            "_id": "6419309f22270b3ccf177c77",
            "type": "user",
            "user": "williamium",
            "isPro": false,
            "fullname": "William Li",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6419309f22270b3ccf177c77/KQa1586iBBKqucUlfpuPp.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:18:54.946Z"
        },
        {
          "_id": "698abbae1b2dc6b37d61b0f6",
          "name": "Yiming Huang",
          "hidden": false
        },
        {
          "_id": "698abbae1b2dc6b37d61b0f7",
          "name": "Jialian Wu",
          "hidden": false
        },
        {
          "_id": "698abbae1b2dc6b37d61b0f8",
          "name": "Jiang Liu",
          "hidden": false
        },
        {
          "_id": "698abbae1b2dc6b37d61b0f9",
          "name": "Ximeng Sun",
          "hidden": false
        },
        {
          "_id": "698abbae1b2dc6b37d61b0fa",
          "name": "Zhenfei Yin",
          "hidden": false
        },
        {
          "_id": "698abbae1b2dc6b37d61b0fb",
          "name": "Lun Wang",
          "hidden": false
        },
        {
          "_id": "698abbae1b2dc6b37d61b0fc",
          "name": "Zicheng Liu",
          "hidden": false
        },
        {
          "_id": "698abbae1b2dc6b37d61b0fd",
          "name": "Emad Barsoum",
          "hidden": false
        },
        {
          "_id": "698abbae1b2dc6b37d61b0fe",
          "name": "William Yang Wang",
          "hidden": false
        },
        {
          "_id": "698abbae1b2dc6b37d61b0ff",
          "name": "Wenbo Guo",
          "hidden": false
        }
      ],
      "published_at": "2026-02-06T23:56:50",
      "upvotes": 23,
      "date": "2026-02-10"
    },
    {
      "id": "2602.07055",
      "title": "Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?",
      "summary": "Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.",
      "authors": [
        {
          "_id": "698a94fc1b2dc6b37d61aecd",
          "name": "Pingyue Zhang",
          "user": {
            "_id": "66d32d853c8270397e6c2f0a",
            "type": "user",
            "user": "williamzhangNU",
            "isPro": false,
            "fullname": "pingyue zhang",
            "avatarUrl": "/avatars/3b99da95769673f773dff9ccbdb6887d.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:07:28.765Z"
        },
        {
          "_id": "698a94fc1b2dc6b37d61aece",
          "name": "Zihan Huang",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aecf",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed0",
          "name": "Jieyu Zhang",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed1",
          "name": "Letian Xue",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed2",
          "name": "Zihan Wang",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed3",
          "name": "Qineng Wang",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed4",
          "name": "Keshigeyan Chandrasegaran",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed5",
          "name": "Ruohan Zhang",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed6",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed7",
          "name": "Ranjay Krishna",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed8",
          "name": "Jiajun Wu",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed9",
          "name": "Li Fei-Fei",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aeda",
          "name": "Manling Li",
          "hidden": false
        }
      ],
      "published_at": "2026-02-04T19:06:40",
      "upvotes": 20,
      "date": "2026-02-10"
    },
    {
      "id": "2602.09022",
      "title": "WorldCompass: Reinforcement Learning for Long-Horizon World Models",
      "summary": "This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively \"steer\" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.",
      "authors": [
        {
          "_id": "698ab6ec1b2dc6b37d61b023",
          "name": "Zehan Wang",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b024",
          "name": "Tengfei Wang",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b025",
          "name": "Haiyu Zhang",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b026",
          "name": "Xuhui Zuo",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b027",
          "name": "Junta Wu",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b028",
          "name": "Haoyuan Wang",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b029",
          "name": "Wenqiang Sun",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b02a",
          "name": "Zhenwei Wang",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b02b",
          "name": "Chenjie Cao",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b02c",
          "name": "Hengshuang Zhao",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b02d",
          "name": "Chunchao Guo",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b02e",
          "name": "Zhou Zhao",
          "hidden": false
        }
      ],
      "published_at": "2026-02-09T18:59:47",
      "upvotes": 20,
      "date": "2026-02-10"
    }
  ],
  "2026-02-09": [
    {
      "id": "2602.06717",
      "title": "F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 rightarrow 70.3 (GRPO), 69.3 rightarrow 72.5 (DAPO), and 73.2 rightarrow 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.",
      "authors": [
        {
          "_id": "69898989beecc443208d2741",
          "name": "Daniil Plyusov",
          "hidden": false
        },
        {
          "_id": "69898989beecc443208d2742",
          "name": "Alexey Gorbatovski",
          "user": {
            "_id": "62897fce5d9e25c10e4f319d",
            "type": "user",
            "user": "Myashka",
            "isPro": false,
            "fullname": "Alexey Gorbatovski",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:27:53.815Z"
        },
        {
          "_id": "69898989beecc443208d2743",
          "name": "Boris Shaposhnikov",
          "hidden": false
        },
        {
          "_id": "69898989beecc443208d2744",
          "name": "Viacheslav Sinii",
          "hidden": false
        },
        {
          "_id": "69898989beecc443208d2745",
          "name": "Alexey Malakhov",
          "user": {
            "_id": "636e71b2b0ebc04888157b71",
            "type": "user",
            "user": "ZeL1k7",
            "isPro": false,
            "fullname": "Alexey Malakhov",
            "avatarUrl": "/avatars/957ba705d470e3a01792741d7f0ff038.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T21:06:45.653Z"
        },
        {
          "_id": "69898989beecc443208d2746",
          "name": "Daniil Gavrilov",
          "user": {
            "_id": "62a9c8edc19f92ae443ab37f",
            "type": "user",
            "user": "kefirski",
            "isPro": false,
            "fullname": "Daniil Gavrilov",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a9c8edc19f92ae443ab37f/yczqpBOntLco_2Jn4hnT7.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:27:57.059Z"
        }
      ],
      "published_at": "2026-02-06T14:07:30",
      "upvotes": 68,
      "date": "2026-02-09"
    },
    {
      "id": "2602.06570",
      "title": "Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making",
      "summary": "We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.",
      "authors": [
        {
          "_id": "69895518beecc443208d2680",
          "name": "Baichuan-M3 Team",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2682",
          "name": "Chengfeng Dou",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2683",
          "name": "Fan Yang",
          "user": {
            "_id": "641c45c921964f8f6d451d16",
            "type": "user",
            "user": "fairyang",
            "isPro": false,
            "fullname": "FanYang",
            "avatarUrl": "/avatars/da06cc603f8f9ee46ddb7dc72aae5bec.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T14:32:20.075Z"
        },
        {
          "_id": "69895518beecc443208d2684",
          "name": "Fei Li",
          "user": {
            "_id": "6464dd5234acce85aea186c7",
            "type": "user",
            "user": "lifei926926",
            "isPro": false,
            "fullname": "lifei",
            "avatarUrl": "/avatars/3428029b0ae5f12885092c7aea588065.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T21:07:14.055Z"
        },
        {
          "_id": "69895518beecc443208d2685",
          "name": "Jiyuan Jia",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2686",
          "name": "Qiang Ju",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2687",
          "name": "Shuai Wang",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2688",
          "name": "Tianpeng Li",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2689",
          "name": "Xiangrong Zeng",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d268a",
          "name": "Yijie Zhou",
          "user": {
            "_id": "658670184f349f95cf7d2252",
            "type": "user",
            "user": "Jayok6",
            "isPro": false,
            "fullname": "Jie",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658670184f349f95cf7d2252/MfYwxDS1w2kIvav2GvE_U.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T11:20:27.499Z"
        },
        {
          "_id": "69895518beecc443208d268b",
          "name": "Hongda Zhang",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d268c",
          "name": "Jinyang Tai",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d268d",
          "name": "Linzhuang Sun",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d268e",
          "name": "Peidong Guo",
          "user": {
            "_id": "6487e2e1eec01aee99cf4c10",
            "type": "user",
            "user": "GuoPD",
            "isPro": false,
            "fullname": "Peidong Guo",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6487e2e1eec01aee99cf4c10/1U6zZ2OaUOrR1ueD5yraR.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:36:44.605Z"
        },
        {
          "_id": "69895518beecc443208d268f",
          "name": "Yichuan Mo",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2690",
          "name": "Xiaochuan Wang",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2691",
          "name": "Hengfu Cui",
          "hidden": false
        },
        {
          "_id": "69895518beecc443208d2692",
          "name": "Zhishou Zhang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-06T10:08:59",
      "upvotes": 58,
      "date": "2026-02-09"
    },
    {
      "id": "2602.05027",
      "title": "AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders",
      "summary": "Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.",
      "authors": [
        {
          "_id": "69871a872d626112378ad69f",
          "name": "Georgii Aparin",
          "user": {
            "_id": "660fd34df03515e4ff3f2b64",
            "type": "user",
            "user": "Egorgij21",
            "isPro": false,
            "fullname": "Georgii Aparin",
            "avatarUrl": "/avatars/0c2a29b1081ece881234acdd8ef9371a.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:34:22.635Z"
        },
        {
          "_id": "69871a872d626112378ad6a0",
          "name": "Tasnima Sadekova",
          "hidden": false
        },
        {
          "_id": "69871a872d626112378ad6a1",
          "name": "Alexey Rukhovich",
          "hidden": false
        },
        {
          "_id": "69871a872d626112378ad6a2",
          "name": "Assel Yermekova",
          "hidden": false
        },
        {
          "_id": "69871a872d626112378ad6a3",
          "name": "Laida Kushnareva",
          "hidden": false
        },
        {
          "_id": "69871a872d626112378ad6a4",
          "name": "Vadim Popov",
          "hidden": false
        },
        {
          "_id": "69871a872d626112378ad6a5",
          "name": "Kristian Kuznetsov",
          "hidden": false
        },
        {
          "_id": "69871a872d626112378ad6a6",
          "name": "Irina Piontkovskaya",
          "hidden": false
        }
      ],
      "published_at": "2026-02-04T20:29:16",
      "upvotes": 57,
      "date": "2026-02-09"
    },
    {
      "id": "2602.05843",
      "title": "OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions",
      "summary": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena",
      "authors": [
        {
          "_id": "698567834ad556f294b7ec03",
          "name": "Fangzhi Xu",
          "user": {
            "_id": "64e6cf78ecce34cb442dc889",
            "type": "user",
            "user": "xufangzhi",
            "isPro": false,
            "fullname": "Fangzhi Xu",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:51:15.682Z"
        },
        {
          "_id": "698567834ad556f294b7ec04",
          "name": "Hang Yan",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec05",
          "name": "Qiushi Sun",
          "user": {
            "_id": "6064a0eeb1703ddba0d458b9",
            "type": "user",
            "user": "QiushiSun",
            "isPro": false,
            "fullname": "Qiushi",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:35:17.132Z"
        },
        {
          "_id": "698567834ad556f294b7ec06",
          "name": "Jinyang Wu",
          "user": {
            "_id": "6747de57f8cab58c22ec94a2",
            "type": "user",
            "user": "Jinyang23",
            "isPro": false,
            "fullname": "Jinyang Wu",
            "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:35:19.089Z"
        },
        {
          "_id": "698567834ad556f294b7ec07",
          "name": "Zixian Huang",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec08",
          "name": "Muye Huang",
          "user": {
            "_id": "6628859f1a5c7e6b445868c1",
            "type": "user",
            "user": "MuyeHuang",
            "isPro": false,
            "fullname": "Muye Huang",
            "avatarUrl": "/avatars/a7684d2bd0fd60824c5e810356953243.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:35:25.705Z"
        },
        {
          "_id": "698567834ad556f294b7ec09",
          "name": "Jingyang Gong",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec0a",
          "name": "Zichen Ding",
          "user": {
            "_id": "642b9861bb77f8456634b048",
            "type": "user",
            "user": "heroding77",
            "isPro": false,
            "fullname": "Zichen Ding",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/VrNmmcdgX7FufQmdP5YaG.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:35:15.144Z"
        },
        {
          "_id": "698567834ad556f294b7ec0b",
          "name": "Kanzhi Cheng",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec0c",
          "name": "Yian Wang",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec0d",
          "name": "Xinyu Che",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec0e",
          "name": "Zeyi Sun",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec0f",
          "name": "Jian Zhang",
          "user": {
            "_id": "658be7fe135580745c510323",
            "type": "user",
            "user": "VentureZJ",
            "isPro": false,
            "fullname": "Jian Zhang",
            "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:51:08.819Z"
        },
        {
          "_id": "698567834ad556f294b7ec10",
          "name": "Zhangyue Yin",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec11",
          "name": "Haoran Luo",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec12",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec13",
          "name": "Ben Kao",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec14",
          "name": "Jun Liu",
          "hidden": false
        },
        {
          "_id": "698567834ad556f294b7ec15",
          "name": "Qika Lin",
          "user": {
            "_id": "66ac77011cfb12c087605acb",
            "type": "user",
            "user": "Qika",
            "isPro": false,
            "fullname": "Lin",
            "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:35:23.699Z"
        }
      ],
      "published_at": "2026-02-05T16:31:43",
      "upvotes": 55,
      "date": "2026-02-09"
    },
    {
      "id": "2602.03392",
      "title": "On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models",
      "summary": "Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.",
      "authors": [
        {
          "_id": "69858ae34ad556f294b7ec93",
          "name": "Shumin Wang",
          "user": {
            "_id": "652f7bf41ad13fee8c407247",
            "type": "user",
            "user": "Mystery",
            "isPro": false,
            "fullname": "Shumin",
            "avatarUrl": "/avatars/5c7a74a9edf748025bffeeba97a61505.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:50:45.887Z"
        },
        {
          "_id": "69858ae34ad556f294b7ec94",
          "name": "Yuexiang Xie",
          "hidden": false
        },
        {
          "_id": "69858ae34ad556f294b7ec95",
          "name": "Wenhao Zhang",
          "user": {
            "_id": "63f46a0fa096536aeab6ee75",
            "type": "user",
            "user": "xiaoniqiu",
            "isPro": false,
            "fullname": "garyzhang",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f46a0fa096536aeab6ee75/Bte71XHp05z_vXvt8POev.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:07:49.396Z"
        },
        {
          "_id": "69858ae34ad556f294b7ec96",
          "name": "Yuchang Sun",
          "user": {
            "_id": "6541b3d54f939214d3abbfbc",
            "type": "user",
            "user": "hiyuchang",
            "isPro": false,
            "fullname": "yuchang",
            "avatarUrl": "/avatars/37aa9cc51fd98198805456ad04b90023.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:35:09.981Z"
        },
        {
          "_id": "69858ae34ad556f294b7ec97",
          "name": "Yanxi Chen",
          "user": {
            "_id": "6576f9f4654561a1b345610b",
            "type": "user",
            "user": "yanxi-chen",
            "isPro": false,
            "fullname": "Yanxi Chen",
            "avatarUrl": "/avatars/f801f551640caa70368fcc26a0f51d27.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:07:51.201Z"
        },
        {
          "_id": "69858ae34ad556f294b7ec98",
          "name": "Yaliang Li",
          "hidden": false
        },
        {
          "_id": "69858ae34ad556f294b7ec99",
          "name": "Yanyong Zhang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-03T11:14:58",
      "upvotes": 52,
      "date": "2026-02-09"
    },
    {
      "id": "2602.01734",
      "title": "MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration",
      "summary": "Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via \u03bcP, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.",
      "authors": [
        {
          "_id": "6987609dbeecc443208d2375",
          "name": "Lianhai Ren",
          "hidden": false
        },
        {
          "_id": "6987609dbeecc443208d2376",
          "name": "Yucheng Ding",
          "hidden": false
        },
        {
          "_id": "6987609dbeecc443208d2377",
          "name": "Xiao Liu",
          "user": {
            "_id": "63fb6e281b4b1bd4e7ffc5be",
            "type": "user",
            "user": "lx865712528",
            "isPro": false,
            "fullname": "Xiao Liu",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fb6e281b4b1bd4e7ffc5be/aiRu_bulgnxvEMrjipGoQ.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:31:02.000Z"
        },
        {
          "_id": "6987609dbeecc443208d2378",
          "name": "Qianxiao Li",
          "hidden": false
        },
        {
          "_id": "6987609dbeecc443208d2379",
          "name": "Peng Cheng",
          "hidden": false
        },
        {
          "_id": "6987609dbeecc443208d237a",
          "name": "Yeyun Gong",
          "hidden": false
        }
      ],
      "published_at": "2026-02-02T07:18:45",
      "upvotes": 32,
      "date": "2026-02-09"
    },
    {
      "id": "2601.18415",
      "title": "Pisets: A Robust Speech Recognition System for Lectures and Interviews",
      "summary": "This work presents a speech-to-text system \"Pisets\" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of \"Pisets\" system is publicly available at GitHub: https://github.com/bond005/pisets.",
      "authors": [
        {
          "_id": "698061616676f93322706708",
          "name": "Ivan Bondarenko",
          "user": {
            "_id": "62b1e0f76a5435fd9a60a8dc",
            "type": "user",
            "user": "bond005",
            "isPro": false,
            "fullname": "Ivan Bondarenko",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655824626110-noauth.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-02T15:43:40.523Z"
        },
        {
          "_id": "698061616676f93322706709",
          "name": "Daniil Grebenkin",
          "user": {
            "_id": "63cb976d80ba2ca4151b67a2",
            "type": "user",
            "user": "dangrebenkin",
            "isPro": false,
            "fullname": "Daniel Grebenkin",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675713278440-63cb976d80ba2ca4151b67a2.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-02T15:43:46.074Z"
        },
        {
          "_id": "698061616676f9332270670a",
          "name": "Oleg Sedukhin",
          "user": {
            "_id": "61dd9daedb45389905634d3f",
            "type": "user",
            "user": "greyzyablik",
            "isPro": false,
            "fullname": "Oleg Sedukhin",
            "avatarUrl": "/avatars/a0482317b6118d37da294b9dc2bf5a39.svg"
          },
          "hidden": true,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-02T15:43:42.792Z"
        },
        {
          "_id": "698061616676f9332270670b",
          "name": "Mikhail Klementev",
          "user": {
            "_id": "662282de25b14cbf4b0c3f7b",
            "type": "user",
            "user": "Klemaaaaa",
            "isPro": false,
            "fullname": "Klementev Mikhail",
            "avatarUrl": "/avatars/78231051b4f29538c83ff9935e54d974.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-02T15:43:38.368Z"
        },
        {
          "_id": "698061616676f9332270670c",
          "name": "Roman Derunets",
          "user": {
            "_id": "6415cb01486c7c9a5d1560f3",
            "type": "user",
            "user": "rmndrnts",
            "isPro": false,
            "fullname": "Roman Derunets",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6415cb01486c7c9a5d1560f3/tKQPhr1o-1SLSvKg3un6J.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-02T16:51:53.503Z"
        },
        {
          "_id": "698061616676f9332270670d",
          "name": "Lyudmila Budneva",
          "hidden": false
        }
      ],
      "published_at": "2026-01-26T12:14:51",
      "upvotes": 31,
      "date": "2026-02-09"
    },
    {
      "id": "2602.06949",
      "title": "DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos",
      "summary": "Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.",
      "authors": [
        {
          "_id": "69894c74beecc443208d25db",
          "name": "Shenyuan Gao",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25dc",
          "name": "William Liang",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25dd",
          "name": "Kaiyuan Zheng",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25de",
          "name": "Ayaan Malik",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25df",
          "name": "Seonghyeon Ye",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e0",
          "name": "Sihyun Yu",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e1",
          "name": "Wei-Cheng Tseng",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e2",
          "name": "Yuzhu Dong",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e3",
          "name": "Kaichun Mo",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e4",
          "name": "Chen-Hsuan Lin",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e5",
          "name": "Qianli Ma",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e6",
          "name": "Seungjun Nah",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e7",
          "name": "Loic Magne",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e8",
          "name": "Jiannan Xiang",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25e9",
          "name": "Yuqi Xie",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25ea",
          "name": "Ruijie Zheng",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25eb",
          "name": "Dantong Niu",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25ec",
          "name": "You Liang Tan",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25ed",
          "name": "K. R. Zentner",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25ee",
          "name": "George Kurian",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25ef",
          "name": "Suneel Indupuru",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f0",
          "name": "Pooya Jannaty",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f1",
          "name": "Jinwei Gu",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f2",
          "name": "Jun Zhang",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f3",
          "name": "Jitendra Malik",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f4",
          "name": "Pieter Abbeel",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f5",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f6",
          "name": "Yuke Zhu",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f7",
          "name": "Joel Jang",
          "hidden": false
        },
        {
          "_id": "69894c74beecc443208d25f8",
          "name": "Linxi \"Jim\" Fan",
          "hidden": false
        }
      ],
      "published_at": "2026-02-06T18:49:43",
      "upvotes": 28,
      "date": "2026-02-09"
    },
    {
      "id": "2602.06130",
      "title": "Self-Improving World Modelling with Latent Actions",
      "summary": "Internal modelling of the world -- predicting transitions between previous states X and next states Y under actions Z -- is essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly action-labelled trajectories. We propose SWIRL, a self-improvement framework that learns from state-only sequences by treating actions as a latent variable and alternating between Forward World Modelling (FWM) P_\u03b8(Y|X,Z) and an Inverse Dynamics Modelling (IDM) Q_\u03c6(Z|X,Y). SWIRL iterates two phases: (1) Variational Information Maximisation, which updates the FWM to generate next states that maximise conditional mutual information with latent actions given prior states, encouraging identifiable consistency; and (2) ELBO Maximisation, which updates the IDM to explain observed transitions, effectively performing coordinate ascent. Both models are trained with reinforcement learning (specifically, GRPO) with the opposite frozen model's log-probability as a reward signal. We provide theoretical learnability guarantees for both updates, and evaluate SWIRL on LLMs and VLMs across multiple environments: single-turn and multi-turn open-world visual dynamics and synthetic textual environments for physics, web, and tool calling. SWIRL achieves gains of 16% on AURORABench, 28% on ByteMorph, 16% on WorldPredictionBench, and 14% on StableToolBench.",
      "authors": [
        {
          "_id": "6989cb0bbeecc443208d2864",
          "name": "Yifu Qiu",
          "hidden": false
        },
        {
          "_id": "6989cb0bbeecc443208d2865",
          "name": "Zheng Zhao",
          "user": {
            "_id": "64ba8e9d5299e0f164491e45",
            "type": "user",
            "user": "zsquaredz",
            "isPro": false,
            "fullname": "Zheng Zhao",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ba8e9d5299e0f164491e45/ZK4bUJZuUO8xsj0RZmzMn.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T21:06:19.908Z"
        },
        {
          "_id": "6989cb0bbeecc443208d2866",
          "name": "Waylon Li",
          "user": {
            "_id": "63dcfaaaf37111482522fbd6",
            "type": "user",
            "user": "waylonli",
            "isPro": false,
            "fullname": "Waylon Li",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63dcfaaaf37111482522fbd6/-NkJznxAJum4fMPy8zoUT.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:27:43.273Z"
        },
        {
          "_id": "6989cb0bbeecc443208d2867",
          "name": "Yftah Ziser",
          "hidden": false
        },
        {
          "_id": "6989cb0bbeecc443208d2868",
          "name": "Anna Korhonen",
          "hidden": false
        },
        {
          "_id": "6989cb0bbeecc443208d2869",
          "name": "Shay B. Cohen",
          "hidden": false
        },
        {
          "_id": "6989cb0bbeecc443208d286a",
          "name": "Edoardo M. Ponti",
          "hidden": false
        }
      ],
      "published_at": "2026-02-05T19:04:41",
      "upvotes": 26,
      "date": "2026-02-09"
    },
    {
      "id": "2602.06291",
      "title": "Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math",
      "summary": "Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose Consequence-Based Utility, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.",
      "authors": [
        {
          "_id": "698964b8beecc443208d26d8",
          "name": "Guijin Son",
          "hidden": false
        },
        {
          "_id": "698964b8beecc443208d26d9",
          "name": "Donghun Yang",
          "hidden": false
        },
        {
          "_id": "698964b8beecc443208d26da",
          "name": "Hitesh Laxmichand Patel",
          "hidden": false
        },
        {
          "_id": "698964b8beecc443208d26db",
          "name": "Hyunwoo Ko",
          "user": {
            "_id": "63e087b6a98d931aa90c1b9c",
            "type": "user",
            "user": "Cartinoe5930",
            "isPro": false,
            "fullname": "Hyunwoo Ko",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e087b6a98d931aa90c1b9c/96c6IT3f1pWGLbRdRDB2U.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-10T09:07:36.536Z"
        },
        {
          "_id": "698964b8beecc443208d26dc",
          "name": "Amit Agarwal",
          "hidden": false
        },
        {
          "_id": "698964b8beecc443208d26dd",
          "name": "Sunghee Ahn",
          "hidden": false
        },
        {
          "_id": "698964b8beecc443208d26de",
          "name": "Kyong-Ha Lee",
          "hidden": false
        },
        {
          "_id": "698964b8beecc443208d26df",
          "name": "Youngjae Yu",
          "hidden": false
        }
      ],
      "published_at": "2026-02-06T01:10:28",
      "upvotes": 22,
      "date": "2026-02-09"
    },
    {
      "id": "2602.06079",
      "title": "Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers",
      "summary": "The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.",
      "authors": [
        {
          "_id": "69895559beecc443208d26a2",
          "name": "Liangyu Wang",
          "user": {
            "_id": "66224a84afbc88c1e4881ad7",
            "type": "user",
            "user": "ly4096",
            "isPro": false,
            "fullname": "Liangyu Wang",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66224a84afbc88c1e4881ad7/fGDiIiqhTBQri3khSqNcU.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T22:17:13.834Z"
        },
        {
          "_id": "69895559beecc443208d26a3",
          "name": "Siqi Zhang",
          "hidden": false
        },
        {
          "_id": "69895559beecc443208d26a4",
          "name": "Junjie Wang",
          "hidden": false
        },
        {
          "_id": "69895559beecc443208d26a5",
          "name": "Yiming Dong",
          "hidden": false
        },
        {
          "_id": "69895559beecc443208d26a6",
          "name": "Bo Zheng",
          "hidden": false
        },
        {
          "_id": "69895559beecc443208d26a7",
          "name": "Zihan Qiu",
          "hidden": false
        },
        {
          "_id": "69895559beecc443208d26a8",
          "name": "Shengkun Tang",
          "hidden": false
        },
        {
          "_id": "69895559beecc443208d26a9",
          "name": "Di Wang",
          "hidden": false
        },
        {
          "_id": "69895559beecc443208d26aa",
          "name": "Rui Men",
          "hidden": false
        },
        {
          "_id": "69895559beecc443208d26ab",
          "name": "Dayiheng Liu",
          "hidden": false
        }
      ],
      "published_at": "2026-02-04T07:38:24",
      "upvotes": 18,
      "date": "2026-02-09"
    },
    {
      "id": "2602.05940",
      "title": "Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training",
      "summary": "Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.",
      "authors": [
        {
          "_id": "6985662b4ad556f294b7ebf8",
          "name": "Junxiao Liu",
          "user": {
            "_id": "68356f5db243fb809813a715",
            "type": "user",
            "user": "master-lan",
            "isPro": false,
            "fullname": "LiuJunxiao",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68356f5db243fb809813a715/grhHvANfDRp75rMJxWlQo.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:51:11.681Z"
        },
        {
          "_id": "6985662b4ad556f294b7ebf9",
          "name": "Zhijun Wang",
          "hidden": false
        },
        {
          "_id": "6985662b4ad556f294b7ebfa",
          "name": "Yixiao Li",
          "hidden": false
        },
        {
          "_id": "6985662b4ad556f294b7ebfb",
          "name": "Zhejian Lai",
          "user": {
            "_id": "643525ea0b30bd434ea15363",
            "type": "user",
            "user": "DreamW1ngs",
            "isPro": false,
            "fullname": "Jackie Lai",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643525ea0b30bd434ea15363/7sAzllfWUPtt68NY1gDLj.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:35:21.194Z"
        },
        {
          "_id": "6985662b4ad556f294b7ebfc",
          "name": "Liqian Huang",
          "hidden": false
        },
        {
          "_id": "6985662b4ad556f294b7ebfd",
          "name": "Xin Huang",
          "hidden": false
        },
        {
          "_id": "6985662b4ad556f294b7ebfe",
          "name": "Xue Han",
          "hidden": false
        },
        {
          "_id": "6985662b4ad556f294b7ebff",
          "name": "Junlan Feng",
          "hidden": false
        },
        {
          "_id": "6985662b4ad556f294b7ec00",
          "name": "Shujian Huang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-05T17:55:09",
      "upvotes": 17,
      "date": "2026-02-09"
    },
    {
      "id": "2602.06391",
      "title": "POINTS-GUI-G: GUI-Grounding Journey",
      "summary": "The rapid advancement of vision-language models has catalyzed the emergence of GUI agents, which hold immense potential for automating complex tasks, from online shopping to flight booking, thereby alleviating the burden of repetitive digital workflows. As a foundational capability, GUI grounding is typically established as a prerequisite for end-to-end task execution. It enables models to precisely locate interface elements, such as text and icons, to perform accurate operations like clicking and typing. Unlike prior works that fine-tune models already possessing strong spatial awareness (e.g., Qwen3-VL), we aim to master the full technical pipeline by starting from a base model with minimal grounding ability, such as POINTS-1.5. We introduce POINTS-GUI-G-8B, which achieves state-of-the-art performance with scores of 59.9 on ScreenSpot-Pro, 66.0 on OSWorld-G, 95.7 on ScreenSpot-v2, and 49.9 on UI-Vision. Our model's success is driven by three key factors: (1) Refined Data Engineering, involving the unification of diverse open-source datasets format alongside sophisticated strategies for augmentation, filtering, and difficulty grading; (2) Improved Training Strategies, including continuous fine-tuning of the vision encoder to enhance perceptual accuracy and maintaining resolution consistency between training and inference; and (3) Reinforcement Learning (RL) with Verifiable Rewards. While RL is traditionally used to bolster reasoning, we demonstrate that it significantly improves precision in the perception-intensive GUI grounding task. Furthermore, GUI grounding provides a natural advantage for RL, as rewards are easily verifiable and highly accurate.",
      "authors": [
        {
          "_id": "69894bc6beecc443208d25c0",
          "name": "Zhongyin Zhao",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c1",
          "name": "Yuan Liu",
          "user": {
            "_id": "64d47a7a508a6313e33faedd",
            "type": "user",
            "user": "YuanLiuuuuuu",
            "isPro": false,
            "fullname": "Yuan Liu",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d47a7a508a6313e33faedd/8DrbXdxA-sKZb4jgNhvY1.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:29:55.119Z"
        },
        {
          "_id": "69894bc6beecc443208d25c2",
          "name": "Yikun Liu",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c3",
          "name": "Haicheng Wang",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c4",
          "name": "Le Tian",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c5",
          "name": "Xiao Zhou",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c6",
          "name": "Yangxiu You",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c7",
          "name": "Zilin Yu",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c8",
          "name": "Yang Yu",
          "hidden": false
        },
        {
          "_id": "69894bc6beecc443208d25c9",
          "name": "Jie Zhou",
          "hidden": false
        }
      ],
      "published_at": "2026-02-06T05:14:11",
      "upvotes": 16,
      "date": "2026-02-09"
    },
    {
      "id": "2602.05281",
      "title": "Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.",
      "authors": [
        {
          "_id": "69889d19beecc443208d24b2",
          "name": "Pengyi Li",
          "hidden": false
        },
        {
          "_id": "69889d19beecc443208d24b3",
          "name": "Elizaveta Goncharova",
          "hidden": false
        },
        {
          "_id": "69889d19beecc443208d24b4",
          "name": "Andrey Kuznetsov",
          "user": {
            "_id": "643984dceb7c5616ef3f5d54",
            "type": "user",
            "user": "kuznetsoffandrey",
            "isPro": false,
            "fullname": "Andrey Kuznetsov",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:30:30.309Z"
        },
        {
          "_id": "69889d19beecc443208d24b5",
          "name": "Ivan Oseledets",
          "hidden": false
        }
      ],
      "published_at": "2026-02-05T04:06:55",
      "upvotes": 14,
      "date": "2026-02-09"
    },
    {
      "id": "2602.06075",
      "title": "MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments",
      "summary": "Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MemGUI-Bench, a comprehensive memory-centric benchmark with pass@k and staged LLM-as-judge evaluation. Our contributions include: (1) a systematic memory taxonomy analyzing 11 agents across 5 architectures; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention; (3) MemGUI-Eval, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics; and (4) RQ-driven assessment of 11 state-of-the-art agents. Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes, and synthesize 5 actionable design implications. All resources including code, benchmark, and evaluation results will be \\textit{fully open-sourced and continuously maintained} at https://lgy0404.github.io/MemGUI-Bench/.",
      "authors": [
        {
          "_id": "698949ccbeecc443208d25a3",
          "name": "Guangyi Liu",
          "user": {
            "_id": "64d761b98ebc40443831f82a",
            "type": "user",
            "user": "lgy0404",
            "isPro": false,
            "fullname": "Guangyi Liu",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T14:32:13.554Z"
        },
        {
          "_id": "698949ccbeecc443208d25a4",
          "name": "Pengxiang Zhao",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25a5",
          "name": "Yaozhen Liang",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25a6",
          "name": "Qinyi Luo",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25a7",
          "name": "Shunye Tang",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25a8",
          "name": "Yuxiang Chai",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25a9",
          "name": "Weifeng Lin",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25aa",
          "name": "Han Xiao",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25ab",
          "name": "WenHao Wang",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25ac",
          "name": "Siheng Chen",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25ad",
          "name": "Zhengxi Lu",
          "user": {
            "_id": "676127cf11b19ea602bb202a",
            "type": "user",
            "user": "LZXzju",
            "isPro": false,
            "fullname": "Zhengxi Lu",
            "avatarUrl": "/avatars/dfd802a24bd63e509728159ebb1769f6.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:29:59.917Z"
        },
        {
          "_id": "698949ccbeecc443208d25ae",
          "name": "Gao Wu",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25af",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25b0",
          "name": "Liang Liu",
          "hidden": false
        },
        {
          "_id": "698949ccbeecc443208d25b1",
          "name": "Yong Liu",
          "hidden": false
        }
      ],
      "published_at": "2026-02-03T17:01:59",
      "upvotes": 13,
      "date": "2026-02-09"
    },
    {
      "id": "2602.05847",
      "title": "OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention",
      "summary": "While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to \"think with omnimodal cues\" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.",
      "authors": [
        {
          "_id": "6989c105beecc443208d2838",
          "name": "Zhangquan Chen",
          "hidden": false
        },
        {
          "_id": "6989c105beecc443208d2839",
          "name": "Jiale Tao",
          "hidden": false
        },
        {
          "_id": "6989c105beecc443208d283a",
          "name": "Ruihuang Li",
          "hidden": false
        },
        {
          "_id": "6989c105beecc443208d283b",
          "name": "Yihao Hu",
          "hidden": false
        },
        {
          "_id": "6989c105beecc443208d283c",
          "name": "Ruitao Chen",
          "hidden": false
        },
        {
          "_id": "6989c105beecc443208d283d",
          "name": "Zhantao Yang",
          "hidden": false
        },
        {
          "_id": "6989c105beecc443208d283e",
          "name": "Xinlei Yu",
          "hidden": false
        },
        {
          "_id": "6989c105beecc443208d283f",
          "name": "Haodong Jing",
          "hidden": false
        },
        {
          "_id": "6989c105beecc443208d2840",
          "name": "Manyuan Zhang",
          "hidden": false
        },
        {
          "_id": "6989c105beecc443208d2841",
          "name": "Shuai Shao",
          "hidden": false
        },
        {
          "_id": "6989c105beecc443208d2842",
          "name": "Biao Wang",
          "hidden": false
        },
        {
          "_id": "6989c105beecc443208d2843",
          "name": "Qinglin Lu",
          "hidden": false
        },
        {
          "_id": "6989c105beecc443208d2844",
          "name": "Ruqi Huang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-05T16:35:19",
      "upvotes": 12,
      "date": "2026-02-09"
    },
    {
      "id": "2602.06139",
      "title": "EgoAVU: Egocentric Audio-Visual Understanding",
      "summary": "Understanding egocentric videos plays a vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, a scalable data engine to automatically generate egocentric audio-visual narrations, questions, and answers. EgoAVU enriches human narrations with multimodal context and generates audio-visual narrations through cross-modal correlation modeling. Token-based video filtering and modular, graph-based curation ensure both data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct, a large-scale training dataset of 3M samples, and EgoAVU-Bench, a manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitations of existing MLLMs: they bias heavily toward visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively addresses this issue, enabling up to 113% performance improvement on EgoAVU-Bench. Such benefits also transfer to other benchmarks such as EgoTempo and EgoIllusion, achieving up to 28% relative performance gain. Code will be released to the community.",
      "authors": [
        {
          "_id": "69894d1dbeecc443208d25fb",
          "name": "Ashish Seth",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d25fc",
          "name": "Xinhao Mei",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d25fd",
          "name": "Changsheng Zhao",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d25fe",
          "name": "Varun Nagaraja",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d25ff",
          "name": "Ernie Chang",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2600",
          "name": "Gregory P. Meyer",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2601",
          "name": "Gael Le Lan",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2602",
          "name": "Yunyang Xiong",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2603",
          "name": "Vikas Chandra",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2604",
          "name": "Yangyang Shi",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2605",
          "name": "Dinesh Manocha",
          "hidden": false
        },
        {
          "_id": "69894d1dbeecc443208d2606",
          "name": "Zhipeng Cai",
          "hidden": false
        }
      ],
      "published_at": "2026-02-05T19:16:55",
      "upvotes": 11,
      "date": "2026-02-09"
    },
    {
      "id": "2602.06176",
      "title": "Large Language Model Reasoning Failures",
      "summary": "Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.",
      "authors": [
        {
          "_id": "698a479d1b2dc6b37d61ae76",
          "name": "Peiyang Song",
          "user": {
            "_id": "649c5cf5c1ae48cf4d7dda34",
            "type": "user",
            "user": "p-song1",
            "isPro": false,
            "fullname": "Peiyang Song",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649c5cf5c1ae48cf4d7dda34/bSJXATqkqBn8ypUy0OezY.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T21:07:00.512Z"
        },
        {
          "_id": "698a479d1b2dc6b37d61ae77",
          "name": "Pengrui Han",
          "user": {
            "_id": "664263ea8b2d38e53f04079c",
            "type": "user",
            "user": "barryhpr",
            "isPro": false,
            "fullname": "Pengrui Han",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664263ea8b2d38e53f04079c/_7og0ggbPpEHcH6g__DF0.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-11T22:17:03.428Z"
        },
        {
          "_id": "698a479d1b2dc6b37d61ae78",
          "name": "Noah Goodman",
          "hidden": false
        }
      ],
      "published_at": "2026-02-05T20:29:26",
      "upvotes": 11,
      "date": "2026-02-09"
    },
    {
      "id": "2602.04649",
      "title": "Outcome Accuracy is Not Enough: Aligning the Reasoning Process of Reward Models",
      "summary": "Generative Reward Models (GenRMs) and LLM-as-a-Judge exhibit deceptive alignment by producing correct judgments for incorrect reasons, as they are trained and evaluated to prioritize Outcome Accuracy, which undermines their ability to generalize during RLHF. We introduce Rationale Consistency, a fine-grained metric that quantifies the alignment between the model's reasoning process and human judgment. Our evaluation of frontier models reveals that rationale consistency effectively discriminates among state-of-the-art models and detects deceptive alignment, while outcome accuracy falls short in both respects. To mitigate this gap, we introduce a hybrid signal that combines rationale consistency with outcome accuracy for GenRM training. Our training method achieves state-of-the-art performance on RM-Bench (87.1%) and JudgeBench (82%), surpassing outcome-only baselines by an average of 5%. Using RM during RLHF, our method effectively improves performance as demonstrated on Arena Hard v2, notably yielding a 7% improvement in creative writing tasks. Further analysis confirms that our method escapes the deceptive alignment trap, effectively reversing the decline in rationale consistency observed in outcome-only training.",
      "authors": [
        {
          "_id": "69844fe4e34659da7e1f50e6",
          "name": "Binghai Wang",
          "hidden": false
        },
        {
          "_id": "69844fe4e34659da7e1f50e7",
          "name": "Yantao Liu",
          "hidden": false
        },
        {
          "_id": "69844fe4e34659da7e1f50e8",
          "name": "Yuxuan Liu",
          "hidden": false
        },
        {
          "_id": "69844fe4e34659da7e1f50e9",
          "name": "Tianyi Tang",
          "hidden": false
        },
        {
          "_id": "69844fe4e34659da7e1f50ea",
          "name": "Shenzhi Wang",
          "hidden": false
        },
        {
          "_id": "69844fe4e34659da7e1f50eb",
          "name": "Chang Gao",
          "hidden": false
        },
        {
          "_id": "69844fe4e34659da7e1f50ec",
          "name": "Chujie Zheng",
          "hidden": false
        },
        {
          "_id": "69844fe4e34659da7e1f50ed",
          "name": "Yichang Zhang",
          "hidden": false
        },
        {
          "_id": "69844fe4e34659da7e1f50ee",
          "name": "Le Yu",
          "hidden": false
        },
        {
          "_id": "69844fe4e34659da7e1f50ef",
          "name": "Shixuan Liu",
          "hidden": false
        },
        {
          "_id": "69844fe4e34659da7e1f50f0",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "69844fe4e34659da7e1f50f1",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "69844fe4e34659da7e1f50f2",
          "name": "Xuanjing Huang",
          "hidden": false
        },
        {
          "_id": "69844fe4e34659da7e1f50f3",
          "name": "Bowen Yu",
          "hidden": false
        },
        {
          "_id": "69844fe4e34659da7e1f50f4",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "69844fe4e34659da7e1f50f5",
          "name": "Junyang Lin",
          "hidden": false
        }
      ],
      "published_at": "2026-02-04T15:24:52",
      "upvotes": 10,
      "date": "2026-02-09"
    },
    {
      "id": "2602.06960",
      "title": "InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning",
      "summary": "Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.",
      "authors": [
        {
          "_id": "69894ae3beecc443208d25b4",
          "name": "Yuchen Yan",
          "user": {
            "_id": "64098738342c26884c792c93",
            "type": "user",
            "user": "yanyc",
            "isPro": false,
            "fullname": "Yuchen Yan",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:29:57.465Z"
        },
        {
          "_id": "69894ae3beecc443208d25b5",
          "name": "Liang Jiang",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25b6",
          "name": "Jin Jiang",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25b7",
          "name": "Shuaicheng Li",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25b8",
          "name": "Zujie Wen",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25b9",
          "name": "Zhiqiang Zhang",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25ba",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25bb",
          "name": "Jian Shao",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25bc",
          "name": "Yueting Zhuang",
          "hidden": false
        },
        {
          "_id": "69894ae3beecc443208d25bd",
          "name": "Yongliang Shen",
          "hidden": false
        }
      ],
      "published_at": "2026-02-06T18:59:27",
      "upvotes": 10,
      "date": "2026-02-09"
    }
  ],
  "2026-02-06": [
    {
      "id": "2601.22027",
      "title": "CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty",
      "summary": "Existing benchmarks for Large Language Model (LLM) agents focus on task completion under idealistic settings but overlook reliability in real-world, user-facing applications. In domains, such as in-car voice assistants, users often issue incomplete or ambiguous requests, creating intrinsic uncertainty that agents must manage through dialogue, tool use, and policy adherence. We introduce CAR-bench, a benchmark for evaluating consistency, uncertainty handling, and capability awareness in multi-turn, tool-using LLM agents in an in-car assistant domain. The environment features an LLM-simulated user, domain policies, and 58 interconnected tools spanning navigation, productivity, charging, and vehicle control. Beyond standard task completion, CAR-bench introduces Hallucination tasks that test agents' limit-awareness under missing tools or information, and Disambiguation tasks that require resolving uncertainty through clarification or internal information gathering. Baseline results reveal large gaps between occasional and consistent success on all task types. Even frontier reasoning LLMs achieve less than 50% consistent pass rate on Disambiguation tasks due to premature actions, and frequently violate policies or fabricate information to satisfy user requests in Hallucination tasks, underscoring the need for more reliable and self-aware LLM agents in real-world settings.",
      "authors": [
        {
          "_id": "698386d9e34659da7e1f4c39",
          "name": "Johannes Kirmayr",
          "user": {
            "_id": "69838556dea8956a5cef3ebd",
            "type": "user",
            "user": "johanneskirmayr",
            "isPro": false,
            "fullname": "Johannes Kirmayr",
            "avatarUrl": "/avatars/1a82518bc4e1a14af9f7874e0d720924.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-04T19:03:13.608Z"
        },
        {
          "_id": "698386d9e34659da7e1f4c3a",
          "name": "Lukas Stappen",
          "user": {
            "_id": "6985bb43ac2abc7a370153d8",
            "type": "user",
            "user": "lstappen",
            "isPro": false,
            "fullname": "Lukas Stappen",
            "avatarUrl": "/avatars/02da4efbe2706fc0a312172c9bfa0cfb.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:55:56.937Z"
        },
        {
          "_id": "698386d9e34659da7e1f4c3b",
          "name": "Elisabeth Andr\u00e9",
          "hidden": false
        }
      ],
      "published_at": "2026-01-29T17:33:42",
      "upvotes": 74,
      "date": "2026-02-06"
    },
    {
      "id": "2602.05386",
      "title": "Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening",
      "summary": "As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S^2Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\\%.",
      "authors": [
        {
          "_id": "698599b14ad556f294b7ecdc",
          "name": "Zhenxiong Yu",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7ecdd",
          "name": "Zhi Yang",
          "user": {
            "_id": "64aa645404e7b379feccc490",
            "type": "user",
            "user": "yangzhi1",
            "isPro": false,
            "fullname": "Zhi Yang",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:50:37.407Z"
        },
        {
          "_id": "698599b14ad556f294b7ecde",
          "name": "Zhiheng Jin",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7ecdf",
          "name": "Shuhe Wang",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7ece0",
          "name": "Heng Zhang",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7ece1",
          "name": "Yanlin Fei",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7ece2",
          "name": "Lingfeng Zeng",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7ece3",
          "name": "Fangqi Lou",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7ece4",
          "name": "Shuo Zhang",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7ece5",
          "name": "Tu Hu",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7ece6",
          "name": "Jingping Liu",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7ece7",
          "name": "Rongze Chen",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7ece8",
          "name": "Xingyu Zhu",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7ece9",
          "name": "Kunyi Wang",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7ecea",
          "name": "Chaofa Yuan",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7eceb",
          "name": "Xin Guo",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7ecec",
          "name": "Zhaowei Liu",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7eced",
          "name": "Feipeng Zhang",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7ecee",
          "name": "Jie Huang",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7ecef",
          "name": "Huacan Wang",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7ecf0",
          "name": "Ronghao Chen",
          "hidden": false
        },
        {
          "_id": "698599b14ad556f294b7ecf1",
          "name": "Liwen Zhang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-05T07:11:05",
      "upvotes": 66,
      "date": "2026-02-06"
    },
    {
      "id": "2602.02474",
      "title": "MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents",
      "summary": "Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present MemSkill, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a controller that learns to select a small set of relevant skills, paired with an LLM-based executor that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a designer that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.",
      "authors": [
        {
          "_id": "6982023e47987be58cdb7d3a",
          "name": "Haozhen Zhang",
          "hidden": false
        },
        {
          "_id": "6982023e47987be58cdb7d3b",
          "name": "Quanyu Long",
          "hidden": false
        },
        {
          "_id": "6982023e47987be58cdb7d3c",
          "name": "Jianzhu Bao",
          "hidden": false
        },
        {
          "_id": "6982023e47987be58cdb7d3d",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "6982023e47987be58cdb7d3e",
          "name": "Weizhi Zhang",
          "hidden": false
        },
        {
          "_id": "6982023e47987be58cdb7d3f",
          "name": "Haodong Yue",
          "hidden": false
        },
        {
          "_id": "6982023e47987be58cdb7d40",
          "name": "Wenya Wang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-02T18:53:28",
      "upvotes": 48,
      "date": "2026-02-06"
    },
    {
      "id": "2602.05261",
      "title": "Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR",
      "summary": "Recent applications of Reinforcement Learning with Verifiable Rewards (RLVR) to Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated significant success in enhancing reasoning capabilities for complex tasks. During RLVR training, an increase in response length is often regarded as a key factor contributing to the growth of reasoning ability. However, the patterns of change in response length vary significantly across different RLVR algorithms during the training process. To provide a fundamental explanation for these variations, this paper conducts an in-depth analysis of the components of mainstream RLVR algorithms. We present a theoretical analysis of the factors influencing response length and validate our theory through extensive experimentation. Building upon these theoretical findings, we propose the Length-Unbiased Sequence Policy Optimization (LUSPO) algorithm. Specifically, we rectify the length bias inherent in Group Sequence Policy Optimization (GSPO), rendering its loss function unbiased with respect to response length and thereby resolving the issue of response length collapse. We conduct extensive experiments across mathematical reasoning benchmarks and multimodal reasoning scenarios, where LUSPO consistently achieves superior performance. Empirical results demonstrate that LUSPO represents a novel, state-of-the-art optimization strategy compared to existing methods such as GRPO and GSPO.",
      "authors": [
        {
          "_id": "69855cbe4ad556f294b7eb29",
          "name": "Fanfan Liu",
          "user": {
            "_id": "6689f7a1683da3ea29b4cee5",
            "type": "user",
            "user": "liufanfanlff",
            "isPro": false,
            "fullname": "Fanfan Liu",
            "avatarUrl": "/avatars/988641cde34a60d183208fd9a2a72392.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:51:59.637Z"
        },
        {
          "_id": "69855cbe4ad556f294b7eb2a",
          "name": "Youyang Yin",
          "hidden": false
        },
        {
          "_id": "69855cbe4ad556f294b7eb2b",
          "name": "Peng Shi",
          "hidden": false
        },
        {
          "_id": "69855cbe4ad556f294b7eb2c",
          "name": "Siqi Yang",
          "hidden": false
        },
        {
          "_id": "69855cbe4ad556f294b7eb2d",
          "name": "Zhixiong Zeng",
          "hidden": false
        },
        {
          "_id": "69855cbe4ad556f294b7eb2e",
          "name": "Haibo Qiu",
          "hidden": false
        }
      ],
      "published_at": "2026-02-05T03:35:38",
      "upvotes": 46,
      "date": "2026-02-06"
    },
    {
      "id": "2602.06036",
      "title": "DFlash: Block Diffusion for Flash Speculative Decoding",
      "summary": "Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM; however, existing methods still rely on autoregressive drafting, which remains sequential and limits practical speedups. Diffusion LLMs offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models. In this paper, we introduce DFlash, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. By generating draft tokens in a single forward pass and conditioning the draft model on context features extracted from the target model, DFlash enables efficient drafting with high-quality outputs and higher acceptance rates. Experiments show that DFlash achieves over 6x lossless acceleration across a range of models and tasks, delivering up to 2.5x higher speedup than the state-of-the-art speculative decoding method EAGLE-3.",
      "authors": [
        {
          "_id": "69855f4e4ad556f294b7eb85",
          "name": "Jian Chen",
          "hidden": false
        },
        {
          "_id": "69855f4e4ad556f294b7eb86",
          "name": "Yesheng Liang",
          "hidden": false
        },
        {
          "_id": "69855f4e4ad556f294b7eb87",
          "name": "Zhijian Liu",
          "hidden": false
        }
      ],
      "published_at": "2026-02-05T18:59:30",
      "upvotes": 33,
      "date": "2026-02-06"
    },
    {
      "id": "2602.06028",
      "title": "Context Forcing: Consistent Autoregressive Video Generation with Long Context",
      "summary": "Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical student-teacher mismatch: the teacher's inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student's context length. To resolve this, we propose Context Forcing, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a Slow-Fast Memory architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.",
      "authors": [
        {
          "_id": "6985603a4ad556f294b7eb92",
          "name": "Shuo Chen",
          "user": {
            "_id": "6556031ab7bad186e86e96e7",
            "type": "user",
            "user": "ShuoChen20",
            "isPro": false,
            "fullname": "ShuoChen",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/_SjKb8h1N8URnQchqu327.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:51:50.030Z"
        },
        {
          "_id": "6985603a4ad556f294b7eb93",
          "name": "Cong Wei",
          "user": {
            "_id": "64f8e358766ff9f3d2b0de84",
            "type": "user",
            "user": "CongWei1230",
            "isPro": true,
            "fullname": "Cong Wei",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f8e358766ff9f3d2b0de84/R2P1YG-mRBh7TU9wkjGGk.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:51:47.676Z"
        },
        {
          "_id": "6985603a4ad556f294b7eb94",
          "name": "Sun Sun",
          "hidden": false
        },
        {
          "_id": "6985603a4ad556f294b7eb95",
          "name": "Ping Nie",
          "hidden": false
        },
        {
          "_id": "6985603a4ad556f294b7eb96",
          "name": "Kai Zhou",
          "hidden": false
        },
        {
          "_id": "6985603a4ad556f294b7eb97",
          "name": "Ge Zhang",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "type": "user",
            "user": "zhangysk",
            "isPro": false,
            "fullname": "Ge Zhang",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:51:45.226Z"
        },
        {
          "_id": "6985603a4ad556f294b7eb98",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        },
        {
          "_id": "6985603a4ad556f294b7eb99",
          "name": "Wenhu Chen",
          "hidden": false
        }
      ],
      "published_at": "2026-02-05T18:58:01",
      "upvotes": 31,
      "date": "2026-02-06"
    },
    {
      "id": "2602.05986",
      "title": "RISE-Video: Can Video Generators Decode Implicit World Rules?",
      "summary": "While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: Reasoning Alignment, Temporal Consistency, Physical Rationality, and Visual Quality. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.",
      "authors": [
        {
          "_id": "69855d7a4ad556f294b7eb31",
          "name": "Mingxin Liu",
          "hidden": false
        },
        {
          "_id": "69855d7a4ad556f294b7eb32",
          "name": "Shuran Ma",
          "hidden": false
        },
        {
          "_id": "69855d7a4ad556f294b7eb33",
          "name": "Shibei Meng",
          "hidden": false
        },
        {
          "_id": "69855d7a4ad556f294b7eb34",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "69855d7a4ad556f294b7eb35",
          "name": "Zicheng Zhang",
          "hidden": false
        },
        {
          "_id": "69855d7a4ad556f294b7eb36",
          "name": "Shaofeng Zhang",
          "hidden": false
        },
        {
          "_id": "69855d7a4ad556f294b7eb37",
          "name": "Zhihang Zhong",
          "hidden": false
        },
        {
          "_id": "69855d7a4ad556f294b7eb38",
          "name": "Peixian Chen",
          "hidden": false
        },
        {
          "_id": "69855d7a4ad556f294b7eb39",
          "name": "Haoyu Cao",
          "hidden": false
        },
        {
          "_id": "69855d7a4ad556f294b7eb3a",
          "name": "Xing Sun",
          "hidden": false
        },
        {
          "_id": "69855d7a4ad556f294b7eb3b",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "69855d7a4ad556f294b7eb3c",
          "name": "Xue Yang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-05T18:36:10",
      "upvotes": 26,
      "date": "2026-02-06"
    },
    {
      "id": "2602.03338",
      "title": "Accurate Failure Prediction in Agents Does Not Imply Effective Failure Prevention",
      "summary": "Proactive interventions by LLM critic models are often assumed to improve reliability, yet their effects at deployment time are poorly understood. We show that a binary LLM critic with strong offline accuracy (AUROC 0.94) can nevertheless cause severe performance degradation, inducing a 26 percentage point (pp) collapse on one model while affecting another by near zero pp. This variability demonstrates that LLM critic accuracy alone is insufficient to determine whether intervention is safe.\n  We identify a disruption-recovery tradeoff: interventions may recover failing trajectories but also disrupt trajectories that would have succeeded. Based on this insight, we propose a pre-deployment test that uses a small pilot of 50 tasks to estimate whether intervention is likely to help or harm, without requiring full deployment. Across benchmarks, the test correctly anticipates outcomes: intervention degrades performance on high-success tasks (0 to -26 pp), while yielding a modest improvement on the high-failure ALFWorld benchmark (+2.8 pp, p=0.014). The primary value of our framework is therefore identifying when not to intervene, preventing severe regressions before deployment.",
      "authors": [
        {
          "_id": "6985a0ce4ad556f294b7ed16",
          "name": "Rakshith Vasudev",
          "hidden": false
        },
        {
          "_id": "6985a0ce4ad556f294b7ed17",
          "name": "Melisa Russak",
          "user": {
            "_id": "60e61b3969bd0df25c9375da",
            "type": "user",
            "user": "melisa",
            "isPro": false,
            "fullname": "Melisa Russak",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625692968400-noauth.jpeg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:50:23.513Z"
        },
        {
          "_id": "6985a0ce4ad556f294b7ed18",
          "name": "Dan Bikel",
          "hidden": false
        },
        {
          "_id": "6985a0ce4ad556f294b7ed19",
          "name": "Waseem Alshikh",
          "hidden": false
        }
      ],
      "published_at": "2026-02-03T10:02:50",
      "upvotes": 25,
      "date": "2026-02-06"
    },
    {
      "id": "2602.05885",
      "title": "Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations",
      "summary": "High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.",
      "authors": [
        {
          "_id": "698580004ad556f294b7ec78",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "698580004ad556f294b7ec79",
          "name": "Jiawei Xu",
          "hidden": false
        },
        {
          "_id": "698580004ad556f294b7ec7a",
          "name": "Yingru Li",
          "user": {
            "_id": "67277d20eebb94a257cd6925",
            "type": "user",
            "user": "R1ch0rd",
            "isPro": false,
            "fullname": "Yingru Li",
            "avatarUrl": "/avatars/c1f714b59fecb53770e28920c0c267cb.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:50:48.867Z"
        },
        {
          "_id": "698580004ad556f294b7ec7b",
          "name": "Longtao Zheng",
          "hidden": false
        },
        {
          "_id": "698580004ad556f294b7ec7c",
          "name": "Tianjian Li",
          "hidden": false
        },
        {
          "_id": "698580004ad556f294b7ec7d",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "698580004ad556f294b7ec7e",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "published_at": "2026-02-05T17:01:09",
      "upvotes": 24,
      "date": "2026-02-06"
    },
    {
      "id": "2602.05327",
      "title": "ProAct: Agentic Lookahead in Interactive Environments",
      "summary": "Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a two-stage training paradigm. First, we introduce Grounded LookAhead Distillation (GLAD), where the agent undergoes supervised fine-tuning on trajectories derived from environment-based search. By compressing complex search trees into concise, causal reasoning chains, the agent learns the logic of foresight without the computational overhead of inference-time search. Second, to further refine decision accuracy, we propose the Monte-Carlo Critic (MC-Critic), a plug-and-play auxiliary value estimator designed to enhance policy-gradient algorithms like PPO and GRPO. By leveraging lightweight environment rollouts to calibrate value estimates, MC-Critic provides a low-variance signal that facilitates stable policy optimization without relying on expensive model-based value approximation. Experiments on both stochastic (e.g., 2048) and deterministic (e.g., Sokoban) environments demonstrate that ProAct significantly improves planning accuracy. Notably, a 4B parameter model trained with ProAct outperforms all open-source baselines and rivals state-of-the-art closed-source models, while demonstrating robust generalization to unseen environments. The codes and models are available at https://github.com/GreatX3/ProAct",
      "authors": [
        {
          "_id": "698557724ad556f294b7eafb",
          "name": "Yangbin Yu",
          "hidden": false
        },
        {
          "_id": "698557724ad556f294b7eafc",
          "name": "Mingyu Yang",
          "hidden": false
        },
        {
          "_id": "698557724ad556f294b7eafd",
          "name": "Junyou Li",
          "hidden": false
        },
        {
          "_id": "698557724ad556f294b7eafe",
          "name": "Yiming Gao",
          "user": {
            "_id": "67cff2a43a9d50150f6d55e6",
            "type": "user",
            "user": "code-yatming",
            "isPro": false,
            "fullname": "Yatming Gor",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/iY05tc7uuocF55aT08Vdx.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:52:08.424Z"
        },
        {
          "_id": "698557724ad556f294b7eaff",
          "name": "Feiyu Liu",
          "hidden": false
        },
        {
          "_id": "698557724ad556f294b7eb00",
          "name": "Yijun Yang",
          "hidden": false
        },
        {
          "_id": "698557724ad556f294b7eb01",
          "name": "Zichuan Lin",
          "hidden": false
        },
        {
          "_id": "698557724ad556f294b7eb02",
          "name": "Jiafei Lyu",
          "hidden": false
        },
        {
          "_id": "698557724ad556f294b7eb03",
          "name": "Yicheng Liu",
          "hidden": false
        },
        {
          "_id": "698557724ad556f294b7eb04",
          "name": "Zhicong Lu",
          "hidden": false
        },
        {
          "_id": "698557724ad556f294b7eb05",
          "name": "Deheng Ye",
          "hidden": false
        },
        {
          "_id": "698557724ad556f294b7eb06",
          "name": "Jie Jiang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-05T05:45:16",
      "upvotes": 23,
      "date": "2026-02-06"
    },
    {
      "id": "2602.04942",
      "title": "Privileged Information Distillation for Language Models",
      "summary": "Training-time privileged information (PI) can enable language models to succeed on tasks they would otherwise fail, making it a powerful tool for reinforcement learning in hard, long-horizon settings. However, transferring capabilities learned with PI to policies that must act without it at inference time remains a fundamental challenge. We study this problem in the context of distilling frontier models for multi-turn agentic environments, where closed-source systems typically hide their internal reasoning and expose only action trajectories. This breaks standard distillation pipelines, since successful behavior is observable but the reasoning process is not. For this, we introduce \u03c0-Distill, a joint teacher-student objective that trains a PI-conditioned teacher and an unconditioned student simultaneously using the same model. Additionally, we also introduce On-Policy Self-Distillation (OPSD), an alternative approach that trains using Reinforcement Learning (RL) with a reverse KL-penalty between the student and the PI-conditioned teacher. We show that both of these algorithms effectively distill frontier agents using action-only PI. Specifically we find that \u03c0-Distill and in some cases OPSD, outperform industry standard practices (Supervised finetuning followed by RL) that assume access to full Chain-of-Thought supervision across multiple agentic benchmarks, models, and forms of PI. We complement our results with extensive analysis that characterizes the factors enabling effective learning with PI, focusing primarily on \u03c0-Distill and characterizing when OPSD is competitive.",
      "authors": [
        {
          "_id": "6985f5fc30e925bc620acf9f",
          "name": "Emiliano Penaloza",
          "hidden": false
        },
        {
          "_id": "6985f5fc30e925bc620acfa0",
          "name": "Dheeraj Vattikonda",
          "user": {
            "_id": "65fb7b1c876d4642e8340e95",
            "type": "user",
            "user": "Dheeraj46329",
            "isPro": false,
            "fullname": "Dheeraj Vattikonda",
            "avatarUrl": "/avatars/c258c4528315243b98c1e60ba398e98b.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:49:44.400Z"
        },
        {
          "_id": "6985f5fc30e925bc620acfa1",
          "name": "Nicolas Gontier",
          "hidden": false
        },
        {
          "_id": "6985f5fc30e925bc620acfa2",
          "name": "Alexandre Lacoste",
          "hidden": false
        },
        {
          "_id": "6985f5fc30e925bc620acfa3",
          "name": "Laurent Charlin",
          "hidden": false
        },
        {
          "_id": "6985f5fc30e925bc620acfa4",
          "name": "Massimo Caccia",
          "user": {
            "_id": "64b04320e5000ae8a57570a7",
            "type": "user",
            "user": "optimass",
            "isPro": false,
            "fullname": "Massimo Caccia",
            "avatarUrl": "/avatars/226f57ea23b9dd861e71b7ba7821adce.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:49:46.321Z"
        }
      ],
      "published_at": "2026-02-04T18:46:17",
      "upvotes": 22,
      "date": "2026-02-06"
    },
    {
      "id": "2602.06035",
      "title": "InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions",
      "summary": "Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.",
      "authors": [
        {
          "_id": "69855ee24ad556f294b7eb7c",
          "name": "Sirui Xu",
          "user": {
            "_id": "64f17c31b4344f592fb2821e",
            "type": "user",
            "user": "xusirui",
            "isPro": false,
            "fullname": "Sirui Xu",
            "avatarUrl": "/avatars/11c8edb0967491822277a8a0d3ff3d31.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:51:52.134Z"
        },
        {
          "_id": "69855ee24ad556f294b7eb7d",
          "name": "Samuel Schulter",
          "hidden": false
        },
        {
          "_id": "69855ee24ad556f294b7eb7e",
          "name": "Morteza Ziyadi",
          "hidden": false
        },
        {
          "_id": "69855ee24ad556f294b7eb7f",
          "name": "Xialin He",
          "hidden": false
        },
        {
          "_id": "69855ee24ad556f294b7eb80",
          "name": "Xiaohan Fei",
          "hidden": false
        },
        {
          "_id": "69855ee24ad556f294b7eb81",
          "name": "Yu-Xiong Wang",
          "hidden": false
        },
        {
          "_id": "69855ee24ad556f294b7eb82",
          "name": "Liangyan Gui",
          "hidden": false
        }
      ],
      "published_at": "2026-02-05T18:59:27",
      "upvotes": 21,
      "date": "2026-02-06"
    },
    {
      "id": "2601.21937",
      "title": "Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities",
      "summary": "Despite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further contaminated by parametric memorization and open-web volatility. We introduce DeR2, a controlled deep-research sandbox that isolates document-grounded reasoning while preserving core difficulties of deep search: multi-step synthesis, denoising, and evidence-based conclusion making. DeR2 decouples evidence access from reasoning via four regimes--Instruction-only, Concepts (gold concepts without documents), Related-only (only relevant documents), and Full-set (relevant documents plus topically related distractors)--yielding interpretable regime gaps that operationalize retrieval loss vs. reasoning loss and enable fine-grained error attribution. To prevent parametric leakage, we apply a two-phase validation that requires parametric failure without evidence while ensuring oracle-concept solvability. To ensure reproducibility, each instance provides a frozen document library (drawn from 2023-2025 theoretical papers) with expert-annotated concepts and validated rationales. Experiments across a diverse set of state-of-the-art foundation models reveal substantial variation and significant headroom: some models exhibit mode-switch fragility, performing worse with the Full-set than with Instruction-only, while others show structural concept misuse, correctly naming concepts but failing to execute them as procedures.",
      "authors": [
        {
          "_id": "6985651d4ad556f294b7ebd9",
          "name": "Shuangshuang Ying",
          "hidden": false
        },
        {
          "_id": "6985651d4ad556f294b7ebda",
          "name": "Zheyu Wang",
          "hidden": false
        },
        {
          "_id": "6985651d4ad556f294b7ebdb",
          "name": "Yunjian Peng",
          "hidden": false
        },
        {
          "_id": "6985651d4ad556f294b7ebdc",
          "name": "Jin Chen",
          "hidden": false
        },
        {
          "_id": "6985651d4ad556f294b7ebdd",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "6985651d4ad556f294b7ebde",
          "name": "Hongbin Lin",
          "hidden": false
        },
        {
          "_id": "6985651d4ad556f294b7ebdf",
          "name": "Dingyu He",
          "hidden": false
        },
        {
          "_id": "6985651d4ad556f294b7ebe0",
          "name": "Siyi Liu",
          "hidden": false
        },
        {
          "_id": "6985651d4ad556f294b7ebe1",
          "name": "Gengchen Yu",
          "hidden": false
        },
        {
          "_id": "6985651d4ad556f294b7ebe2",
          "name": "YinZhu Piao",
          "hidden": false
        },
        {
          "_id": "6985651d4ad556f294b7ebe3",
          "name": "Yuchen Wu",
          "hidden": false
        },
        {
          "_id": "6985651d4ad556f294b7ebe4",
          "name": "Xin Gui",
          "user": {
            "_id": "67dbf07f9d821d38905d145d",
            "type": "user",
            "user": "Ross12",
            "isPro": false,
            "fullname": "guixin",
            "avatarUrl": "/avatars/e806467d2c0f4f642c8d4906b0855817.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:51:40.436Z"
        },
        {
          "_id": "6985651d4ad556f294b7ebe5",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "6985651d4ad556f294b7ebe6",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "6985651d4ad556f294b7ebe7",
          "name": "Xeron Du",
          "hidden": false
        },
        {
          "_id": "6985651d4ad556f294b7ebe8",
          "name": "Libo Qin",
          "hidden": false
        },
        {
          "_id": "6985651d4ad556f294b7ebe9",
          "name": "YiXin Cao",
          "hidden": false
        },
        {
          "_id": "6985651d4ad556f294b7ebea",
          "name": "Ge Zhang",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "type": "user",
            "user": "zhangysk",
            "isPro": false,
            "fullname": "Ge Zhang",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:51:37.889Z"
        },
        {
          "_id": "6985651d4ad556f294b7ebeb",
          "name": "Stephen Huang",
          "hidden": false
        }
      ],
      "published_at": "2026-01-29T16:26:19",
      "upvotes": 19,
      "date": "2026-02-06"
    },
    {
      "id": "2602.04884",
      "title": "Reinforced Attention Learning",
      "summary": "Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.\n  We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.",
      "authors": [
        {
          "_id": "698565d64ad556f294b7ebee",
          "name": "Bangzheng Li",
          "hidden": false
        },
        {
          "_id": "698565d64ad556f294b7ebef",
          "name": "Jianmo Ni",
          "hidden": false
        },
        {
          "_id": "698565d64ad556f294b7ebf0",
          "name": "Chen Qu",
          "hidden": false
        },
        {
          "_id": "698565d64ad556f294b7ebf1",
          "name": "Ian Miao",
          "hidden": false
        },
        {
          "_id": "698565d64ad556f294b7ebf2",
          "name": "Liu Yang",
          "hidden": false
        },
        {
          "_id": "698565d64ad556f294b7ebf3",
          "name": "Xingyu Fu",
          "hidden": false
        },
        {
          "_id": "698565d64ad556f294b7ebf4",
          "name": "Muhao Chen",
          "hidden": false
        },
        {
          "_id": "698565d64ad556f294b7ebf5",
          "name": "Derek Zhiyuan Cheng",
          "hidden": false
        }
      ],
      "published_at": "2026-02-04T18:59:52",
      "upvotes": 18,
      "date": "2026-02-06"
    },
    {
      "id": "2602.05842",
      "title": "Reinforcement World Model Learning for LLM-based Agents",
      "summary": "Large language models (LLMs) have achieved strong performance in language-centric tasks. However, in agentic settings, LLMs often struggle to anticipate action consequences and adapt to environment dynamics, highlighting the need for world-modeling capabilities in LLM-based agents. We propose Reinforcement World Model Learning (RWML), a self-supervised method that learns action-conditioned world models for LLM-based agents on textual states using sim-to-real gap rewards. Our method aligns simulated next states produced by the model with realized next states observed from the environment, encouraging consistency between internal world simulations and actual environment dynamics in a pre-trained embedding space. Unlike next-state token prediction, which prioritizes token-level fidelity (i.e., reproducing exact wording) over semantic equivalence and can lead to model collapse, our method provides a more robust training signal and is empirically less susceptible to reward hacking than LLM-as-a-judge. We evaluate our method on ALFWorld and \u03c4^2 Bench and observe significant gains over the base model, despite being entirely self-supervised. When combined with task-success rewards, our method outperforms direct task-success reward RL by 6.9 and 5.7 points on ALFWorld and \u03c4^2 Bench respectively, while matching the performance of expert-data training.",
      "authors": [
        {
          "_id": "69855bf84ad556f294b7eb1e",
          "name": "Xiao Yu",
          "hidden": false
        },
        {
          "_id": "69855bf84ad556f294b7eb1f",
          "name": "Baolin Peng",
          "hidden": false
        },
        {
          "_id": "69855bf84ad556f294b7eb20",
          "name": "Ruize Xu",
          "hidden": false
        },
        {
          "_id": "69855bf84ad556f294b7eb21",
          "name": "Yelong Shen",
          "hidden": false
        },
        {
          "_id": "69855bf84ad556f294b7eb22",
          "name": "Pengcheng He",
          "hidden": false
        },
        {
          "_id": "69855bf84ad556f294b7eb23",
          "name": "Suman Nath",
          "hidden": false
        },
        {
          "_id": "69855bf84ad556f294b7eb24",
          "name": "Nikhil Singh",
          "hidden": false
        },
        {
          "_id": "69855bf84ad556f294b7eb25",
          "name": "Jiangfeng Gao",
          "hidden": false
        },
        {
          "_id": "69855bf84ad556f294b7eb26",
          "name": "Zhou Yu",
          "hidden": false
        }
      ],
      "published_at": "2026-02-05T16:30:08",
      "upvotes": 18,
      "date": "2026-02-06"
    },
    {
      "id": "2601.21296",
      "title": "Grounding and Enhancing Informativeness and Utility in Dataset Distillation",
      "summary": "Dataset Distillation (DD) seeks to create a compact dataset from a large, real-world dataset. While recent methods often rely on heuristic approaches to balance efficiency and quality, the fundamental relationship between original and synthetic data remains underexplored. This paper revisits knowledge distillation-based dataset distillation within a solid theoretical framework. We introduce the concepts of Informativeness and Utility, capturing crucial information within a sample and essential samples in the training set, respectively. Building on these principles, we define optimal dataset distillation mathematically. We then present InfoUtil, a framework that balances informativeness and utility in synthesizing the distilled dataset. InfoUtil incorporates two key components: (1) game-theoretic informativeness maximization using Shapley Value attribution to extract key information from samples, and (2) principled utility maximization by selecting globally influential samples based on Gradient Norm. These components ensure that the distilled dataset is both informative and utility-optimized. Experiments demonstrate that our method achieves a 6.1\\% performance improvement over the previous state-of-the-art approach on ImageNet-1K dataset using ResNet-18.",
      "authors": [
        {
          "_id": "6985591b4ad556f294b7eb09",
          "name": "Shaobo Wang",
          "user": {
            "_id": "66968099c952e09a4cb29f78",
            "type": "user",
            "user": "Steven-Shaobo",
            "isPro": false,
            "fullname": "Wang",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66968099c952e09a4cb29f78/n90NI2R3E9_RqCyMjDCQF.webp"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:52:05.358Z"
        },
        {
          "_id": "6985591b4ad556f294b7eb0a",
          "name": "Yantai Yang",
          "hidden": false
        },
        {
          "_id": "6985591b4ad556f294b7eb0b",
          "name": "Guo Chen",
          "hidden": false
        },
        {
          "_id": "6985591b4ad556f294b7eb0c",
          "name": "Peiru Li",
          "hidden": false
        },
        {
          "_id": "6985591b4ad556f294b7eb0d",
          "name": "Kaixin Li",
          "user": {
            "_id": "6346be8f7fb9f11870c63998",
            "type": "user",
            "user": "likaixin",
            "isPro": false,
            "fullname": "Kaixin Li",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6346be8f7fb9f11870c63998/tFWawSkXL6bv1zgvzFWQd.png"
          },
          "hidden": false,
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-06T18:52:02.746Z"
        },
        {
          "_id": "6985591b4ad556f294b7eb0e",
          "name": "Yufa Zhou",
          "hidden": false
        },
        {
          "_id": "6985591b4ad556f294b7eb0f",
          "name": "Zhaorun Chen",
          "hidden": false
        },
        {
          "_id": "6985591b4ad556f294b7eb10",
          "name": "Linfeng Zhang",
          "hidden": false
        }
      ],
      "published_at": "2026-01-29T05:49:17",
      "upvotes": 18,
      "date": "2026-02-06"
    },
    {
      "id": "2602.05216",
      "title": "Semantic Search over 9 Million Mathematical Theorems",
      "summary": "Searching for mathematical results remains difficult: most existing tools retrieve entire papers, while mathematicians and theorem-proving agents often seek a specific theorem, lemma, or proposition that answers a query. While semantic search has seen rapid progress, its behavior on large, highly technical corpora such as research-level mathematical theorems remains poorly understood. In this work, we introduce and study semantic theorem retrieval at scale over a unified corpus of 9.2 million theorem statements extracted from arXiv and seven other sources, representing the largest publicly available corpus of human-authored, research-level theorems. We represent each theorem with a short natural-language description as a retrieval representation and systematically analyze how representation context, language model choice, embedding model, and prompting strategy affect retrieval quality. On a curated evaluation set of theorem-search queries written by professional mathematicians, our approach substantially improves both theorem-level and paper-level retrieval compared to existing baselines, demonstrating that semantic theorem search is feasible and effective at web scale. The theorem search tool is available at https://huggingface.co/spaces/uw-math-ai/theorem-search{this link}, and the dataset is available at https://huggingface.co/datasets/uw-math-ai/TheoremSearch{this link}.",
      "authors": [
        {
          "_id": "69854f5b4ad556f294b7eaed",
          "name": "Luke Alexander",
          "hidden": false
        },
        {
          "_id": "69854f5b4ad556f294b7eaee",
          "name": "Eric Leonen",
          "hidden": false
        },
        {
          "_id": "69854f5b4ad556f294b7eaef",
          "name": "Sophie Szeto",
          "hidden": false
        },
        {
          "_id": "69854f5b4ad556f294b7eaf0",
          "name": "Artemii Remizov",
          "hidden": false
        },
        {
          "_id": "69854f5b4ad556f294b7eaf1",
          "name": "Ignacio Tejeda",
          "hidden": false
        },
        {
          "_id": "69854f5b4ad556f294b7eaf2",
          "name": "Giovanni Inchiostro",
          "hidden": false
        },
        {
          "_id": "69854f5b4ad556f294b7eaf3",
          "name": "Vasily Ilin",
          "hidden": false
        }
      ],
      "published_at": "2026-02-05T02:16:20",
      "upvotes": 18,
      "date": "2026-02-06"
    },
    {
      "id": "2602.04210",
      "title": "Steering LLMs via Scalable Interactive Oversight",
      "summary": "As Large Language Models increasingly automate complex, long-horizon tasks such as vibe coding, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight: enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\\% improvement in alignment. Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical pathway for maintaining human control as AI scales.",
      "authors": [
        {
          "_id": "698573404ad556f294b7ec49",
          "name": "Enyu Zhou",
          "hidden": false
        },
        {
          "_id": "698573404ad556f294b7ec4a",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "698573404ad556f294b7ec4b",
          "name": "Long Ma",
          "hidden": false
        },
        {
          "_id": "698573404ad556f294b7ec4c",
          "name": "Zhihao Zhang",
          "hidden": false
        },
        {
          "_id": "698573404ad556f294b7ec4d",
          "name": "Shihan Dou",
          "hidden": false
        },
        {
          "_id": "698573404ad556f294b7ec4e",
          "name": "Zhikai Lei",
          "hidden": false
        },
        {
          "_id": "698573404ad556f294b7ec4f",
          "name": "Guoteng Wang",
          "hidden": false
        },
        {
          "_id": "698573404ad556f294b7ec50",
          "name": "Rui Zheng",
          "hidden": false
        },
        {
          "_id": "698573404ad556f294b7ec51",
          "name": "Hang Yan",
          "hidden": false
        },
        {
          "_id": "698573404ad556f294b7ec52",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "698573404ad556f294b7ec53",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "698573404ad556f294b7ec54",
          "name": "Xuanjing Huang",
          "hidden": false
        }
      ],
      "published_at": "2026-02-04T04:52:00",
      "upvotes": 16,
      "date": "2026-02-06"
    },
    {
      "id": "2602.05115",
      "title": "SocialVeil: Probing Social Intelligence of Language Agents under Communication Barriers",
      "summary": "Large language models (LLMs) are increasingly evaluated in interactive environments to test their social intelligence. However, existing benchmarks often assume idealized communication between agents, limiting our ability to diagnose whether LLMs can maintain and repair interactions in more realistic, imperfect settings. To close this gap, we present SocialVeil, a social learning environment that can simulate social interaction under cognitive-difference-induced communication barriers. Grounded in a systematic literature review of communication challenges in human interaction, SocialVeil introduces three representative types of such disruption, semantic vagueness, sociocultural mismatch, and emotional interference. We also introduce two barrier-aware evaluation metrics, unresolved confusion and mutual understanding, to evaluate interaction quality under impaired communication. Experiments across 720 scenarios and four frontier LLMs show that barriers consistently impair performance, with mutual understanding reduced by over 45\\% on average, and confusion elevated by nearly 50\\%. Human evaluations validate the fidelity of these simulated barriers (ICCapprox0.78, Pearson rapprox0.80). We further demonstrate that adaptation strategies (Repair Instruction and Interactive learning) only have a modest effect far from barrier-free performance. This work takes a step toward bringing social interaction environments closer to real-world communication, opening opportunities for exploring the social intelligence of LLM agents.",
      "authors": [
        {
          "_id": "698619f59c78be977c104bac",
          "name": "Keyang Xuan",
          "hidden": false
        },
        {
          "_id": "698619f59c78be977c104bad",
          "name": "Pengda Wang",
          "hidden": false
        },
        {
          "_id": "698619f59c78be977c104bae",
          "name": "Chongrui Ye",
          "hidden": false
        },
        {
          "_id": "698619f59c78be977c104baf",
          "name": "Haofei Yu",
          "hidden": false
        },
        {
          "_id": "698619f59c78be977c104bb0",
          "name": "Tal August",
          "hidden": false
        },
        {
          "_id": "698619f59c78be977c104bb1",
          "name": "Jiaxuan You",
          "hidden": false
        }
      ],
      "published_at": "2026-02-04T23:04:25",
      "upvotes": 16,
      "date": "2026-02-06"
    },
    {
      "id": "2601.21037",
      "title": "Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning",
      "summary": "Vision-Language Models have excelled at textual reasoning, but they often struggle with fine-grained spatial understanding and continuous action planning, failing to simulate the dynamics required for complex visual reasoning. In this work, we formulate visual reasoning by means of video generation models, positing that generated frames can act as intermediate reasoning steps between initial states and solutions. We evaluate their capacity in two distinct regimes: Maze Navigation for sequential discrete planning with low visual change and Tangram Puzzle for continuous manipulation with high visual change. Our experiments reveal three critical insights: (1) Robust Zero-Shot Generalization: In both tasks, the model demonstrates strong performance on unseen data distributions without specific finetuning. (2) Visual Context: The model effectively uses visual context as explicit control, such as agent icons and tangram shapes, enabling it to maintain high visual consistency and adapt its planning capability robustly to unseen patterns. (3) Visual Test-Time Scaling: We observe a test-time scaling law in sequential planning; increasing the generated video length (visual inference budget) empowers better zero-shot generalization to spatially and temporally complex paths. These findings suggest that video generation is not merely a media tool, but a scalable, generalizable paradigm for visual reasoning.",
      "authors": [
        {
          "_id": "6985af2b4ad556f294b7ed58",
          "name": "Chengzu Li",
          "hidden": false
        },
        {
          "_id": "6985af2b4ad556f294b7ed59",
          "name": "Zanyi Wang",
          "hidden": false
        },
        {
          "_id": "6985af2b4ad556f294b7ed5a",
          "name": "Jiaang Li",
          "hidden": false
        },
        {
          "_id": "6985af2b4ad556f294b7ed5b",
          "name": "Yi Xu",
          "hidden": false
        },
        {
          "_id": "6985af2b4ad556f294b7ed5c",
          "name": "Han Zhou",
          "hidden": false
        },
        {
          "_id": "6985af2b4ad556f294b7ed5d",
          "name": "Huanyu Zhang",
          "hidden": false
        },
        {
          "_id": "6985af2b4ad556f294b7ed5e",
          "name": "Ruichuan An",
          "hidden": false
        },
        {
          "_id": "6985af2b4ad556f294b7ed5f",
          "name": "Dengyang Jiang",
          "hidden": false
        },
        {
          "_id": "6985af2b4ad556f294b7ed60",
          "name": "Zhaochong An",
          "hidden": false
        },
        {
          "_id": "6985af2b4ad556f294b7ed61",
          "name": "Ivan Vuli\u0107",
          "hidden": false
        },
        {
          "_id": "6985af2b4ad556f294b7ed62",
          "name": "Serge Belongie",
          "hidden": false
        },
        {
          "_id": "6985af2b4ad556f294b7ed63",
          "name": "Anna Korhonen",
          "hidden": false
        }
      ],
      "published_at": "2026-01-28T20:57:55",
      "upvotes": 15,
      "date": "2026-02-06"
    }
  ]
}
